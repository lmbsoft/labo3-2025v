{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dfd9ae5-edee-452b-929f-d13eecb983d2",
   "metadata": {},
   "source": [
    "# Modelado tabular con Autgluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba6e856-5142-4c55-9453-e7cd9cb05c58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://nvidia.github.io/libnvidia-container/stable/deb/amd64  InRelease\n",
      "Hit:2 https://download.docker.com/linux/debian bullseye InRelease   \n",
      "Hit:3 https://deb.debian.org/debian bullseye InRelease              \n",
      "Hit:4 https://deb.debian.org/debian-security bullseye-security InRelease\n",
      "Hit:5 https://deb.debian.org/debian bullseye-updates InRelease\n",
      "Hit:6 https://deb.debian.org/debian bullseye-backports InRelease\n",
      "Hit:7 https://packages.cloud.google.com/apt gcsfuse-bullseye InRelease\n",
      "Hit:8 https://packages.cloud.google.com/apt google-compute-engine-bullseye-stable InRelease\n",
      "Hit:9 https://packages.cloud.google.com/apt cloud-sdk-bullseye InRelease\n",
      "Hit:10 https://packages.cloud.google.com/apt google-fast-socket InRelease\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "gcsfuse is already the newest version (3.1.0).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install gcsfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cafae9d5-f167-4844-a54d-837399e5957d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install autogluon.tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714f161-c384-4582-9df4-b2e170562023",
   "metadata": {},
   "source": [
    "# Carga librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a34db802-71e7-4536-8484-73281601d474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80944f5-a7ef-4f3f-a194-ca859daa6053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p /home/jupyter/franco_maestria/gcs_model_dir_con_prophet_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ddf2a47-869b-4059-ab6a-93745c17b19f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!fusermount -u /home/jupyter/franco_maestria/gcs_model_dir_con_prophet_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165665a4-ea68-48fa-ae69-2dc8e0cd0149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"timestamp\":{\"seconds\":1752849064,\"nanos\":712021971},\"severity\":\"INFO\",\"message\":\"Start gcsfuse/3.1.0 (Go version go1.24.0) for app \\\"\\\" using mount point: /home/jupyter/franco_maestria/gcs_model_dir_con_prophet_v3\\n\"}\n",
      "{\"timestamp\":{\"seconds\":1752849064,\"nanos\":712457993},\"severity\":\"INFO\",\"message\":\"GCSFuse config\",\"config\":{\"AppName\":\"\",\"CacheDir\":\"\",\"Debug\":{\"ExitOnInvariantViolation\":false,\"Fuse\":false,\"Gcs\":false,\"LogMutex\":false},\"DisableAutoconfig\":false,\"EnableAtomicRenameObject\":true,\"EnableGoogleLibAuth\":false,\"EnableHns\":true,\"EnableNewReader\":false,\"FileCache\":{\"CacheFileForRangeRead\":false,\"DownloadChunkSizeMb\":200,\"EnableCrc\":false,\"EnableODirect\":false,\"EnableParallelDownloads\":false,\"ExperimentalExcludeRegex\":\"\",\"ExperimentalParallelDownloadsDefaultOn\":true,\"MaxParallelDownloads\":96,\"MaxSizeMb\":-1,\"ParallelDownloadsPerFile\":16,\"WriteBufferSize\":4194304},\"FileSystem\":{\"DirMode\":\"755\",\"DisableParallelDirops\":false,\"ExperimentalEnableDentryCache\":false,\"ExperimentalEnableReaddirplus\":false,\"FileMode\":\"644\",\"FuseOptions\":[],\"Gid\":-1,\"IgnoreInterrupts\":true,\"KernelListCacheTtlSecs\":0,\"PreconditionErrors\":true,\"RenameDirLimit\":0,\"TempDir\":\"\",\"Uid\":-1},\"Foreground\":false,\"GcsAuth\":{\"AnonymousAccess\":false,\"KeyFile\":\"\",\"ReuseTokenFromUrl\":true,\"TokenUrl\":\"\"},\"GcsConnection\":{\"BillingProject\":\"\",\"ClientProtocol\":\"http1\",\"CustomEndpoint\":\"\",\"ExperimentalEnableJsonRead\":false,\"GrpcConnPoolSize\":1,\"HttpClientTimeout\":0,\"LimitBytesPerSec\":-1,\"LimitOpsPerSec\":-1,\"MaxConnsPerHost\":0,\"MaxIdleConnsPerHost\":100,\"SequentialReadSizeMb\":200},\"GcsRetries\":{\"ChunkTransferTimeoutSecs\":10,\"MaxRetryAttempts\":0,\"MaxRetrySleep\":30000000000,\"Multiplier\":2,\"ReadStall\":{\"Enable\":true,\"InitialReqTimeout\":20000000000,\"MaxReqTimeout\":1200000000000,\"MinReqTimeout\":1500000000,\"ReqIncreaseRate\":15,\"ReqTargetPercentile\":0.99}},\"ImplicitDirs\":false,\"List\":{\"EnableEmptyManagedFolders\":false},\"Logging\":{\"FilePath\":\"\",\"Format\":\"json\",\"LogRotate\":{\"BackupFileCount\":10,\"Compress\":true,\"MaxFileSizeMb\":512},\"Severity\":\"INFO\"},\"MachineType\":\"\",\"MetadataCache\":{\"DeprecatedStatCacheCapacity\":20460,\"DeprecatedStatCacheTtl\":60000000000,\"DeprecatedTypeCacheTtl\":60000000000,\"EnableNonexistentTypeCache\":false,\"ExperimentalMetadataPrefetchOnMount\":\"disabled\",\"NegativeTtlSecs\":5,\"StatCacheMaxSizeMb\":33,\"TtlSecs\":60,\"TypeCacheMaxSizeMb\":4},\"Metrics\":{\"CloudMetricsExportIntervalSecs\":0,\"PrometheusPort\":0,\"StackdriverExportInterval\":0,\"UseNewNames\":false},\"Monitoring\":{\"ExperimentalTracingMode\":\"\",\"ExperimentalTracingSamplingRatio\":0},\"OnlyDir\":\"\",\"Profiling\":{\"AllocatedHeap\":true,\"Cpu\":true,\"Enabled\":false,\"Goroutines\":false,\"Heap\":true,\"Label\":\"gcsfuse-0.0.0\",\"Mutex\":false},\"Read\":{\"InactiveStreamTimeout\":10000000000},\"Write\":{\"BlockSizeMb\":33554432,\"CreateEmptyFile\":false,\"EnableStreamingWrites\":true,\"ExperimentalEnableRapidAppends\":false,\"GlobalMaxBlocks\":4,\"MaxBlocksPerFile\":1}}}\n",
      "{\"timestamp\":{\"seconds\":1752849064,\"nanos\":845387574},\"severity\":\"INFO\",\"message\":\"File system has been successfully mounted.\"}\n"
     ]
    }
   ],
   "source": [
    "!gcsfuse forecasting_customer_product /home/jupyter/franco_maestria/gcs_model_dir_con_prophet_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670c745a-acfb-4d86-9e9b-163ced318b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parquet cargado. Shape: (12138186, 194)\n",
      "✅ CSV Prophet cargado. Shape: (7249573, 9)\n",
      "✅ Merge completado. Shape final: (12138186, 200)\n",
      "✅ Conversión de float64 a float32 completada para columnas: ['tn_x', 'inflacion', 'cambio_dolar', 'stock_final', 'clase', 'tn_1', 'diff_tn_1', 'tn_2', 'diff_tn_2', 'tn_3', 'diff_tn_3', 'tn_4', 'diff_tn_4', 'tn_5', 'diff_tn_5', 'tn_6', 'diff_tn_6', 'tn_7', 'diff_tn_7', 'tn_8', 'diff_tn_8', 'tn_9', 'diff_tn_9', 'tn_10', 'diff_tn_10', 'tn_11', 'diff_tn_11', 'tn_12', 'diff_tn_12', 'tn_13', 'diff_tn_13', 'tn_14', 'diff_tn_14', 'tn_15', 'diff_tn_15', 'tn_16', 'diff_tn_16', 'tn_17', 'diff_tn_17', 'tn_18', 'diff_tn_18', 'tn_19', 'diff_tn_19', 'tn_20', 'diff_tn_20', 'tn_21', 'diff_tn_21', 'tn_22', 'diff_tn_22', 'tn_23', 'diff_tn_23', 'tn_24', 'diff_tn_24', 'tn_25', 'diff_tn_25', 'tn_26', 'diff_tn_26', 'tn_27', 'diff_tn_27', 'tn_28', 'diff_tn_28', 'tn_29', 'diff_tn_29', 'tn_30', 'diff_tn_30', 'tn_31', 'diff_tn_31', 'tn_32', 'diff_tn_32', 'tn_33', 'diff_tn_33', 'tn_34', 'diff_tn_34', 'tn_35', 'diff_tn_35', 'tn_36', 'diff_tn_36', 'rollmean_1', 'diff_rollmean_1', 'rollmean_2', 'diff_rollmean_2', 'rollmean_3', 'diff_rollmean_3', 'rollmean_4', 'diff_rollmean_4', 'rollmean_5', 'diff_rollmean_5', 'rollmean_6', 'diff_rollmean_6', 'rollmean_7', 'diff_rollmean_7', 'rollmean_8', 'diff_rollmean_8', 'rollmean_9', 'diff_rollmean_9', 'rollmean_10', 'diff_rollmean_10', 'rollmean_11', 'diff_rollmean_11', 'rollmean_12', 'diff_rollmean_12', 'rollmean_13', 'diff_rollmean_13', 'rollmean_14', 'diff_rollmean_14', 'rollmean_15', 'diff_rollmean_15', 'rollmean_16', 'diff_rollmean_16', 'rollmean_17', 'diff_rollmean_17', 'rollmean_18', 'diff_rollmean_18', 'rollmean_19', 'diff_rollmean_19', 'rollmean_20', 'diff_rollmean_20', 'rollmean_21', 'diff_rollmean_21', 'rollmean_22', 'diff_rollmean_22', 'rollmean_23', 'diff_rollmean_23', 'rollmean_24', 'diff_rollmean_24', 'rollmean_25', 'diff_rollmean_25', 'rollmean_26', 'diff_rollmean_26', 'rollmean_27', 'diff_rollmean_27', 'rollmean_28', 'diff_rollmean_28', 'rollmean_29', 'diff_rollmean_29', 'rollmean_30', 'diff_rollmean_30', 'rollmean_31', 'diff_rollmean_31', 'rollmean_32', 'diff_rollmean_32', 'rollmean_33', 'diff_rollmean_33', 'rollmean_34', 'diff_rollmean_34', 'rollmean_35', 'diff_rollmean_35', 'rollmean_36', 'diff_rollmean_36', 'month_sin', 'month_cos', 'pct_change_1', 'rolling_std_3', 'brand_avg', 'ratio_to_brand_avg', 'cat1_avg', 'ratio_to_cat1_avg', 'cat2_avg', 'ratio_to_cat2_avg', 'cat3_avg', 'ratio_to_cat3_avg', 'product_target_enc', 'customer_target_enc', 'tn_y', 'trend', 'seasonal', 'additive_terms', 'residual', 'slope_trend_3']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12138186 entries, 0 to 12138185\n",
      "Columns: 200 entries, customer_id to slope_trend_3\n",
      "dtypes: datetime64[ns](1), float32(169), int32(10), int64(14), object(6)\n",
      "memory usage: 10.0+ GB\n",
      "None\n",
      "✅ Merge completado. Shape final: (12138186, 200)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>tn_x</th>\n",
       "      <th>IPC</th>\n",
       "      <th>inflacion</th>\n",
       "      <th>cambio_dolar</th>\n",
       "      <th>dias_feriados</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>...</th>\n",
       "      <th>brand_factorized</th>\n",
       "      <th>descripcion_factorized</th>\n",
       "      <th>product_target_enc</th>\n",
       "      <th>customer_target_enc</th>\n",
       "      <th>tn_y</th>\n",
       "      <th>trend</th>\n",
       "      <th>seasonal</th>\n",
       "      <th>additive_terms</th>\n",
       "      <th>residual</th>\n",
       "      <th>slope_trend_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201701</td>\n",
       "      <td>99.438606</td>\n",
       "      <td>102</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>16.080000</td>\n",
       "      <td>1</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.882843</td>\n",
       "      <td>4.146726</td>\n",
       "      <td>99.438606</td>\n",
       "      <td>110.691025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-46.415291</td>\n",
       "      <td>35.162876</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201702</td>\n",
       "      <td>198.843643</td>\n",
       "      <td>104</td>\n",
       "      <td>1.960784</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>2</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.882843</td>\n",
       "      <td>4.146726</td>\n",
       "      <td>198.843643</td>\n",
       "      <td>114.229187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.249763</td>\n",
       "      <td>-7.635300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201703</td>\n",
       "      <td>92.465370</td>\n",
       "      <td>107</td>\n",
       "      <td>2.884615</td>\n",
       "      <td>15.645000</td>\n",
       "      <td>1</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.882843</td>\n",
       "      <td>4.146726</td>\n",
       "      <td>92.465370</td>\n",
       "      <td>117.424942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.674156</td>\n",
       "      <td>-110.633736</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201704</td>\n",
       "      <td>13.297280</td>\n",
       "      <td>109</td>\n",
       "      <td>1.869159</td>\n",
       "      <td>15.490000</td>\n",
       "      <td>1</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.882843</td>\n",
       "      <td>4.146726</td>\n",
       "      <td>13.297280</td>\n",
       "      <td>120.963104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-63.645004</td>\n",
       "      <td>-44.020821</td>\n",
       "      <td>3.424027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201705</td>\n",
       "      <td>101.005630</td>\n",
       "      <td>111</td>\n",
       "      <td>1.834862</td>\n",
       "      <td>16.184999</td>\n",
       "      <td>2</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.882843</td>\n",
       "      <td>4.146726</td>\n",
       "      <td>101.005630</td>\n",
       "      <td>124.387131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-23.848351</td>\n",
       "      <td>0.466848</td>\n",
       "      <td>3.385982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id periodo        tn_x  IPC  inflacion  cambio_dolar  \\\n",
       "0        10001       20001  201701   99.438606  102   2.000000     16.080000   \n",
       "1        10001       20001  201702  198.843643  104   1.960784     15.800000   \n",
       "2        10001       20001  201703   92.465370  107   2.884615     15.645000   \n",
       "3        10001       20001  201704   13.297280  109   1.869159     15.490000   \n",
       "4        10001       20001  201705  101.005630  111   1.834862     16.184999   \n",
       "\n",
       "   dias_feriados cat1         cat2  ... brand_factorized  \\\n",
       "0              1   HC  ROPA LAVADO  ...                0   \n",
       "1              2   HC  ROPA LAVADO  ...                0   \n",
       "2              1   HC  ROPA LAVADO  ...                0   \n",
       "3              1   HC  ROPA LAVADO  ...                0   \n",
       "4              2   HC  ROPA LAVADO  ...                0   \n",
       "\n",
       "  descripcion_factorized  product_target_enc customer_target_enc        tn_y  \\\n",
       "0                      0            2.882843            4.146726   99.438606   \n",
       "1                      0            2.882843            4.146726  198.843643   \n",
       "2                      0            2.882843            4.146726   92.465370   \n",
       "3                      0            2.882843            4.146726   13.297280   \n",
       "4                      0            2.882843            4.146726  101.005630   \n",
       "\n",
       "        trend seasonal  additive_terms    residual  slope_trend_3  \n",
       "0  110.691025      0.0      -46.415291   35.162876            NaN  \n",
       "1  114.229187      0.0       92.249763   -7.635300            NaN  \n",
       "2  117.424942      0.0       85.674156 -110.633736            NaN  \n",
       "3  120.963104      0.0      -63.645004  -44.020821       3.424027  \n",
       "4  124.387131      0.0      -23.848351    0.466848       3.385982  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------\n",
    "# 1) Cargar parquet con FE\n",
    "# ------------------------\n",
    "import pandas as pd\n",
    "\n",
    "parquet_path = \"panel_cliente_producto_fe.parquet\"\n",
    "df_modelo = pd.read_parquet(parquet_path)\n",
    "print(f\"✅ Parquet cargado. Shape: {df_modelo.shape}\")\n",
    "\n",
    "# ------------------------\n",
    "# 2) Cargar Prophet features\n",
    "# ------------------------\n",
    "csv_path = \"prophet_features_customer_product.csv\"\n",
    "df_prophet = pd.read_csv(csv_path)\n",
    "print(f\"✅ CSV Prophet cargado. Shape: {df_prophet.shape}\")\n",
    "\n",
    "# ------------------------\n",
    "# 3) Asegurar consistencia de tipos\n",
    "# ------------------------\n",
    "df_modelo['fecha'] = pd.to_datetime(df_modelo['fecha'])\n",
    "df_prophet['fecha'] = pd.to_datetime(df_prophet['fecha'])\n",
    "\n",
    "# ------------------------\n",
    "# 4) Realizar join por 'product_id' y 'fecha'\n",
    "# ------------------------\n",
    "df_modelo_final = df_modelo.merge(\n",
    "    df_prophet,\n",
    "    on=['customer_id','product_id', 'fecha'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"✅ Merge completado. Shape final: {df_modelo_final.shape}\")\n",
    "\n",
    "# ------------------------\n",
    "# 5) Convertir columnas float64 a float32 para ahorrar memoria\n",
    "# ------------------------\n",
    "float64_cols = df_modelo_final.select_dtypes(include=['float64']).columns.tolist()\n",
    "\n",
    "df_modelo_final[float64_cols] = df_modelo_final[float64_cols].astype('float32')\n",
    "\n",
    "print(f\"✅ Conversión de float64 a float32 completada para columnas: {float64_cols}\")\n",
    "print(df_modelo_final.info())\n",
    "\n",
    "print(f\"✅ Merge completado. Shape final: {df_modelo_final.shape}\")\n",
    "\n",
    "# Verifica el resultado\n",
    "df_modelo_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b1b5eb9-c2c2-48e9-9c0d-60b8815f4319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (10297990, 200) | Test shape: (333840, 200)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# 2) Separar en entrenamiento y test\n",
    "#     👉 Train: periodos <= 201910\n",
    "#     👉 Test:  periodo = 201912  (input para predecir mes+2)\n",
    "# ------------------------\n",
    "\n",
    "train_set = df_modelo_final[(df_modelo_final['fecha'] <= '2019-10-01') & df_modelo_final['clase'].notnull()].copy()\n",
    "test_set = df_modelo_final[(df_modelo_final['fecha'] == '2019-12-01')].copy()\n",
    "\n",
    "print(f\"Train shape: {train_set.shape} | Test shape: {test_set.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4d7e2cd-cf0a-482d-a378-4abf0e76353c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# 5) Crear sample_weight basado en tn_total\n",
    "# ------------------------\n",
    "\n",
    "# Escalar magnitud de toneladas vendidas\n",
    "train_set['sample_weight'] = train_set['tn_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2933159d-5594-4426-ba97-aaa10f90e215",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 197\n",
      "['customer_id', 'product_id', 'tn_x', 'IPC', 'inflacion', 'cambio_dolar', 'dias_feriados', 'cat1', 'cat2', 'cat3']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# 6) Validar features finales\n",
    "# ------------------------\n",
    "\n",
    "features = [col for col in df_modelo_final.columns if col not in [\n",
    "    'periodo', 'clase', 'tn_y'\n",
    "]]\n",
    "\n",
    "print(f\"Total features: {len(features)}\")\n",
    "print(features[:10])  # Algunos de ejemplo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b6b68-96d5-46f8-be16-6fd3d14a3439",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"gcs_model_dir_con_prophet\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Debian 5.10.237-1 (2025-05-19)\n",
      "CPU Count:          48\n",
      "Memory Avail:       264.86 GB / 377.89 GB (70.1%)\n",
      "Disk Space Avail:   1048576.00 GB / 1048576.00 GB (100.0%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=4, num_bag_folds=5, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 3600s of the 14400s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-07-18 14:33:12,188\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/home/jupyter/franco_maestria/gcs_model_dir_con_prophet/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Beginning AutoGluon training ... Time limit = 3597s\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m AutoGluon will save models to \"/home/jupyter/franco_maestria/gcs_model_dir_con_prophet/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Train Data Rows:    9153768\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Train Data Columns: 197\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Label Column:       clase\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tAvailable Memory:                    253927.05 MB\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tTrain Data (Original)  Memory Usage: 10047.60 MB (4.0% of available memory)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\t\tNote: Converting 7 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\tFitting DatetimeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tUseless Original Features (Count: 12): ['tn_34', 'diff_tn_34', 'tn_35', 'diff_tn_35', 'tn_36', 'diff_tn_36', 'rollmean_34', 'diff_rollmean_34', 'rollmean_35', 'diff_rollmean_35', 'rollmean_36', 'diff_rollmean_36']\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tUnused Original Features (Count: 2): ['rollmean_1', 'diff_rollmean_1']\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\t('float', []) : 2 | ['rollmean_1', 'diff_rollmean_1']\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\t('datetime', []) :   1 | ['fecha']\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\t('float', [])    : 153 | ['tn_x', 'inflacion', 'cambio_dolar', 'stock_final', 'tn_1', ...]\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\t('int', [])      :  24 | ['customer_id', 'product_id', 'IPC', 'dias_feriados', 'sku_size', ...]\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\t('object', [])   :   5 | ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\t('category', [])             :   5 | ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\t('float', [])                : 152 | ['tn_x', 'inflacion', 'cambio_dolar', 'stock_final', 'tn_1', ...]\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\t('int', [])                  :  18 | ['customer_id', 'product_id', 'IPC', 'dias_feriados', 'sku_size', ...]\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\t('int', ['bool'])            :   7 | ['is_min_3', 'is_max_3', 'is_min_6', 'is_max_6', 'is_min_12', ...]\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\t('int', ['datetime_as_int']) :   1 | ['fecha']\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t128.8s = Fit runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t183 features in original data used to generate 183 features in processed data.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tTrain Data (Processed) Memory Usage: 6608.40 MB (2.6% of available memory)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Data preprocessing and feature engineering runtime = 144.82s ...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m AutoGluon will fit 5 stack levels (L1 to L5) ...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 920.26s of the 3451.73s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tWarning: Exception caused KNeighborsUnif_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 109, in _fit\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     self.model = self._fit_with_samples(X=X, y=y, model_params=params, time_limit=time_limit - (time.time() - time_start))\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 240, in _fit_with_samples\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     self.model = model_type(**model_params).fit(X_samp, y_samp)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     return fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_regression.py\", line 222, in fit\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     return self._fit(X, y)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_base.py\", line 478, in _fit\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     X, y = validate_data(\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     X, y = check_X_y(X, y, **check_params)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     X = check_array(\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     _assert_all_finite(\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     _assert_all_finite_element_wise(\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     raise ValueError(msg_err)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 860.90s of the 3392.37s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tWarning: Exception caused KNeighborsDist_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 109, in _fit\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     self.model = self._fit_with_samples(X=X, y=y, model_params=params, time_limit=time_limit - (time.time() - time_start))\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 240, in _fit_with_samples\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     self.model = model_type(**model_params).fit(X_samp, y_samp)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     return fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_regression.py\", line 222, in fit\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     return self._fit(X, y)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_base.py\", line 478, in _fit\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     X, y = validate_data(\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     X, y = check_X_y(X, y, **check_params)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     X = check_array(\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     _assert_all_finite(\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     _assert_all_finite_element_wise(\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m     raise ValueError(msg_err)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 804.19s of the 3335.66s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.58% memory usage per fold, 70.31%/80.00% total).\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=17.58%)\n",
      "\u001b[36m(_ray_fit pid=1660159)\u001b[0m \tRan out of time, early stopping on iteration 757. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1660159)\u001b[0m \t[749]\tvalid_set's l1: 0.0821743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1661149)\u001b[0m [1000]\tvalid_set's l1: 0.0832147\n",
      "\u001b[36m(_ray_fit pid=1661149)\u001b[0m [2000]\tvalid_set's l1: 0.0822347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1661149)\u001b[0m \tRan out of time, early stopping on iteration 2035. Best iteration is:\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1661149)\u001b[0m \t[2032]\tvalid_set's l1: 0.0822004\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t-0.0828\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t633.5s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t68.44s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 100.09s of the 2631.55s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.14% memory usage per fold, 72.54%/80.00% total).\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=18.14%)\n",
      "\u001b[36m(_ray_fit pid=1668223)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1668223)\u001b[0m \t[1]\tvalid_set's l1: 0.176111\n",
      "\u001b[36m(_ray_fit pid=1668658)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1668658)\u001b[0m \t[1]\tvalid_set's l1: 0.178244\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t-0.1774\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t91.05s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t6.17s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 2424.78s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t-0.0828\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t1.27s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t0.18s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 806.50s of the 2416.49s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.87% memory usage per fold, 75.48%/80.00% total).\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=18.87%)\n",
      "\u001b[36m(_ray_fit pid=1669268)\u001b[0m \tRan out of time, early stopping on iteration 786. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1669268)\u001b[0m \t[784]\tvalid_set's l1: 0.0823423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1670366)\u001b[0m [1000]\tvalid_set's l1: 0.0829005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1670366)\u001b[0m \tRan out of time, early stopping on iteration 1977. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1670366)\u001b[0m \t[1977]\tvalid_set's l1: 0.0817508\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t-0.0827\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t636.05s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t66.01s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 87.56s of the 1697.55s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.88% memory usage per fold, 75.52%/80.00% total).\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=18.88%)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
      "\u001b[36m(_ray_fit pid=1671339)\u001b[0m \tWarning: Model has no time left to train, skipping model... (Time Left = -0.7s)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1.10s of the 1611.09s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tTime limit exceeded... Skipping RandomForestMSE_BAG_L2.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tWarning: Model has no time left to train, skipping model... (Time Left = -19.6s)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 1377.05s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 1.0}\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t-0.0827\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t0.44s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t0.18s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting 106 L3 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: LightGBMXT_BAG_L3 ... Training model for up to 605.22s of the 1358.66s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.83% memory usage per fold, 75.32%/80.00% total).\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=18.83%)\n",
      "\u001b[36m(_ray_fit pid=1672574)\u001b[0m \tRan out of time, early stopping on iteration 540. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1672574)\u001b[0m \t[536]\tvalid_set's l1: 0.0827915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1673453)\u001b[0m [1000]\tvalid_set's l1: 0.0838709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=1673453)\u001b[0m \tRan out of time, early stopping on iteration 1401. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1673453)\u001b[0m \t[1401]\tvalid_set's l1: 0.0833923\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t-0.0833\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t474.6s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t40.18s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: LightGBM_BAG_L3 ... Training model for up to 45.11s of the 798.55s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.83% memory usage per fold, 75.33%/80.00% total).\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=18.83%)\n",
      "\u001b[36m(_ray_fit pid=1680371)\u001b[0m \tWarning: Model has no time left to train, skipping model... (Time Left = -15.9s)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tTime limit exceeded... Skipping LightGBM_BAG_L3.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: WeightedEnsemble_L4 ... Training model for up to 360.00s of the 640.04s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L3': 1.0}\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t-0.0833\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t0.42s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t0.15s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting 106 L4 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: LightGBMXT_BAG_L4 ... Training model for up to 424.61s of the 633.61s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.86% memory usage per fold, 75.44%/80.00% total).\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=18.86%)\n",
      "\u001b[36m(_ray_fit pid=1681273)\u001b[0m \tRan out of time, early stopping on iteration 311. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1681273)\u001b[0m \t[305]\tvalid_set's l1: 0.0834827\n",
      "\u001b[36m(_ray_fit pid=1682020)\u001b[0m \tRan out of time, early stopping on iteration 883. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1682020)\u001b[0m \t[883]\tvalid_set's l1: 0.0825521\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t-0.084\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t322.38s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t20.85s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: LightGBM_BAG_L4 ... Training model for up to 35.87s of the 244.87s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 19.46% memory usage per fold, 77.84%/80.00% total).\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=19.46%)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tTime limit exceeded... Skipping LightGBM_BAG_L4.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: WeightedEnsemble_L5 ... Training model for up to 360.00s of the 123.11s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L4': 1.0}\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t-0.084\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t0.45s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t0.16s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting 106 L5 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: LightGBMXT_BAG_L5 ... Training model for up to 119.67s of the 115.84s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 19.46% memory usage per fold, 77.83%/80.00% total).\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=19.46%)\n",
      "\u001b[36m(_ray_fit pid=1682998)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1682998)\u001b[0m \t[1]\tvalid_set's l1: 0.178145\n",
      "\u001b[36m(_ray_fit pid=1682999)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1682999)\u001b[0m \t[1]\tvalid_set's l1: 0.177707\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1683492)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1683492)\u001b[0m \t[1]\tvalid_set's l1: 0.176864\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t-0.1779\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t92.13s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t6.19s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Fitting model: WeightedEnsemble_L6 ... Training model for up to 360.00s of the -106.95s of remaining time.\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.524, 'LightGBMXT_BAG_L1': 0.333, 'LightGBMXT_BAG_L3': 0.143}\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t-0.0821\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t2.24s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m \t0.14s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m AutoGluon training complete, total runtime = 3859.85s ... Best model: WeightedEnsemble_L6 | Estimated inference throughput: 10124.5 rows/s (1830754 batch size)\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/jupyter/franco_maestria/gcs_model_dir_con_prophet/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=1655574)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                  model  score_holdout  score_val          eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     LightGBMXT_BAG_L2      -0.084171  -0.082705  mean_absolute_error       44.436077     140.618725  1360.596916                20.258455               66.010633         636.046419            2       True          4\n",
      "1   WeightedEnsemble_L3      -0.084171  -0.082705  mean_absolute_error       44.640222     140.799144  1361.037839                 0.204145                0.180419           0.440923            3       True          5\n",
      "2   WeightedEnsemble_L6      -0.084305  -0.082138  mean_absolute_error       59.530549     180.933764  1837.437294                 0.171740                0.137439           2.239348            6       True         11\n",
      "3     LightGBMXT_BAG_L1      -0.084664  -0.082803  mean_absolute_error       19.948547      68.438797   633.499627                19.948547               68.438797         633.499627            1       True          1\n",
      "4   WeightedEnsemble_L2      -0.084664  -0.082803  mean_absolute_error       20.074280      68.614215   634.773357                 0.125733                0.175418           1.273730            2       True          3\n",
      "5     LightGBMXT_BAG_L3      -0.085386  -0.083274  mean_absolute_error       59.358809     180.796326  1835.197946                14.922732               40.177601         474.601030            3       True          6\n",
      "6   WeightedEnsemble_L4      -0.085386  -0.083274  mean_absolute_error       59.489979     180.949471  1835.622711                 0.131171                0.153146           0.424765            4       True          7\n",
      "7     LightGBMXT_BAG_L4      -0.086346  -0.084049  mean_absolute_error       69.323099     201.651255  2157.574920                 9.964291               20.854930         322.376974            4       True          8\n",
      "8   WeightedEnsemble_L5      -0.086346  -0.084049  mean_absolute_error       69.468081     201.808714  2158.022012                 0.144982                0.157459           0.447092            5       True          9\n",
      "9       LightGBM_BAG_L1      -0.180783  -0.177409  mean_absolute_error        4.229075       6.169296    91.050870                 4.229075                6.169296          91.050870            1       True          2\n",
      "10    LightGBMXT_BAG_L5      -0.181223  -0.177871  mean_absolute_error       74.018960     207.841181  2249.702845                 4.695860                6.189926          92.127924            5       True         10\n",
      "\t4\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t4008s\t = DyStack   runtime |\t10392s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=4.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=4)`\n",
      "Beginning AutoGluon training ... Time limit = 10392s\n",
      "AutoGluon will save models to \"/home/jupyter/franco_maestria/gcs_model_dir_con_prophet\"\n",
      "Train Data Rows:    10297990\n",
      "Train Data Columns: 197\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    249316.43 MB\n",
      "\tTrain Data (Original)  Memory Usage: 11310.10 MB (4.5% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 7 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 12): ['tn_34', 'diff_tn_34', 'tn_35', 'diff_tn_35', 'tn_36', 'diff_tn_36', 'rollmean_34', 'diff_rollmean_34', 'rollmean_35', 'diff_rollmean_35', 'rollmean_36', 'diff_rollmean_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['rollmean_1', 'diff_rollmean_1']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['rollmean_1', 'diff_rollmean_1']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) :   1 | ['fecha']\n",
      "\t\t('float', [])    : 153 | ['tn_x', 'inflacion', 'cambio_dolar', 'stock_final', 'tn_1', ...]\n",
      "\t\t('int', [])      :  24 | ['customer_id', 'product_id', 'IPC', 'dias_feriados', 'sku_size', ...]\n",
      "\t\t('object', [])   :   5 | ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             :   5 | ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
      "\t\t('float', [])                : 152 | ['tn_x', 'inflacion', 'cambio_dolar', 'stock_final', 'tn_1', ...]\n",
      "\t\t('int', [])                  :  18 | ['customer_id', 'product_id', 'IPC', 'dias_feriados', 'sku_size', ...]\n",
      "\t\t('int', ['bool'])            :   7 | ['is_min_3', 'is_max_3', 'is_min_6', 'is_max_6', 'is_min_12', ...]\n",
      "\t\t('int', ['datetime_as_int']) :   1 | ['fecha']\n",
      "\t124.6s = Fit runtime\n",
      "\t183 features in original data used to generate 183 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7434.45 MB (3.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 138.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 5 stack levels (L1 to L5) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 2733.42s of the 10252.88s of remaining time.\n",
      "\tWarning: Exception caused KNeighborsUnif_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "    self._fit_single(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "    model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 109, in _fit\n",
      "    self.model = self._fit_with_samples(X=X, y=y, model_params=params, time_limit=time_limit - (time.time() - time_start))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 240, in _fit_with_samples\n",
      "    self.model = model_type(**model_params).fit(X_samp, y_samp)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_regression.py\", line 222, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_base.py\", line 478, in _fit\n",
      "    X, y = validate_data(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 2679.46s of the 10198.92s of remaining time.\n",
      "\tWarning: Exception caused KNeighborsDist_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "    self._fit_single(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "    model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 109, in _fit\n",
      "    self.model = self._fit_with_samples(X=X, y=y, model_params=params, time_limit=time_limit - (time.time() - time_start))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 240, in _fit_with_samples\n",
      "    self.model = model_type(**model_params).fit(X_samp, y_samp)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_regression.py\", line 222, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_base.py\", line 478, in _fit\n",
      "    X, y = validate_data(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2628.74s of the 10148.20s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.32% memory usage per fold, 40.64%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=24, gpus=0, memory=20.32%)\n",
      "\t-0.0815\t = Validation score   (-mean_absolute_error)\n",
      "\t2137.46s\t = Training   runtime\n",
      "\t119.4s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 410.88s of the 7930.34s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 19.74% memory usage per fold, 78.97%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=19.74%)\n",
      "\t-0.084\t = Validation score   (-mean_absolute_error)\n",
      "\t316.82s\t = Training   runtime\n",
      "\t21.86s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 43.58s of the 7563.04s of remaining time.\n",
      "\tWarning: Exception caused RandomForestMSE_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "    self._fit_single(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "    model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 219, in _fit\n",
      "    model = model.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 375, in fit\n",
      "    estimator._compute_missing_values_in_feature_mask(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 222, in _compute_missing_values_in_feature_mask\n",
      "    _assert_all_finite_element_wise(X, xp=np, allow_nan=True, **common_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 7428.17s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.833, 'LightGBM_BAG_L1': 0.167}\n",
      "\t-0.0813\t = Validation score   (-mean_absolute_error)\n",
      "\t1.19s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2473.84s of the 7419.37s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.26% memory usage per fold, 42.52%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=24, gpus=0, memory=21.26%)\n",
      "\t-0.0815\t = Validation score   (-mean_absolute_error)\n",
      "\t2009.52s\t = Training   runtime\n",
      "\t108.32s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 386.83s of the 5332.36s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.26% memory usage per fold, 42.52%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=24, gpus=0, memory=21.26%)\n",
      "\t-0.0829\t = Validation score   (-mean_absolute_error)\n",
      "\t304.85s\t = Training   runtime\n",
      "\t11.19s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 32.91s of the 4978.44s of remaining time.\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -5.2s)\n",
      "\tTime limit exceeded... Skipping RandomForestMSE_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 4853.54s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.714, 'LightGBM_BAG_L2': 0.286}\n",
      "\t-0.0812\t = Validation score   (-mean_absolute_error)\n",
      "\t1.09s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting 106 L3 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L3 ... Training model for up to 2154.32s of the 4844.32s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.29% memory usage per fold, 42.59%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=24, gpus=0, memory=21.29%)\n",
      "\t-0.0813\t = Validation score   (-mean_absolute_error)\n",
      "\t1750.54s\t = Training   runtime\n",
      "\t98.18s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L3 ... Training model for up to 330.28s of the 3020.28s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.60% memory usage per fold, 41.20%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=24, gpus=0, memory=20.60%)\n",
      "\t-0.0829\t = Validation score   (-mean_absolute_error)\n",
      "\t259.75s\t = Training   runtime\n",
      "\t9.84s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L3 ... Training model for up to 23.33s of the 2713.33s of remaining time.\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -14.1s)\n",
      "\tTime limit exceeded... Skipping RandomForestMSE_BAG_L3.\n",
      "Fitting model: WeightedEnsemble_L4 ... Training model for up to 360.00s of the 2597.87s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L3': 0.75, 'LightGBM_BAG_L3': 0.25}\n",
      "\t-0.0812\t = Validation score   (-mean_absolute_error)\n",
      "\t1.04s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting 106 L4 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L4 ... Training model for up to 1728.33s of the 2589.11s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.29% memory usage per fold, 42.59%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=24, gpus=0, memory=21.29%)\n",
      "\t-0.0815\t = Validation score   (-mean_absolute_error)\n",
      "\t1401.79s\t = Training   runtime\n",
      "\t75.85s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L4 ... Training model for up to 258.85s of the 1119.63s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.30% memory usage per fold, 42.59%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=24, gpus=0, memory=21.30%)\n",
      "\t-0.0861\t = Validation score   (-mean_absolute_error)\n",
      "\t202.21s\t = Training   runtime\n",
      "\t8.43s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L4 ... Training model for up to 9.60s of the 870.37s of remaining time.\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -9.2s)\n",
      "\tTime limit exceeded... Skipping RandomForestMSE_BAG_L4.\n",
      "Fitting model: WeightedEnsemble_L5 ... Training model for up to 360.00s of the 777.49s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L4': 1.0}\n",
      "\t-0.0815\t = Validation score   (-mean_absolute_error)\n",
      "\t1.02s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting 106 L5 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L5 ... Training model for up to 772.76s of the 768.73s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.30% memory usage per fold, 42.59%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=24, gpus=0, memory=21.30%)\n",
      "\t-0.0822\t = Validation score   (-mean_absolute_error)\n",
      "\t617.64s\t = Training   runtime\n",
      "\t22.67s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L5 ... Training model for up to 101.67s of the 97.63s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.30% memory usage per fold, 42.59%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=24, gpus=0, memory=21.30%)\n",
      "\t-0.1777\t = Validation score   (-mean_absolute_error)\n",
      "\t128.71s\t = Training   runtime\n",
      "\t6.24s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L6 ... Training model for up to 360.00s of the -156.86s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.375, 'LightGBMXT_BAG_L3': 0.25, 'LightGBMXT_BAG_L1': 0.188, 'LightGBMXT_BAG_L4': 0.125, 'LightGBM_BAG_L2': 0.062}\n",
      "\t-0.0806\t = Validation score   (-mean_absolute_error)\n",
      "\t3.19s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10714.21s ... Best model: WeightedEnsemble_L6 | Estimated inference throughput: 4631.6 rows/s (2059598 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/jupyter/franco_maestria/gcs_model_dir_con_prophet\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Entrenamiento finalizado.\n",
      "   product_id           tn\n",
      "0       20001  1331.555054\n",
      "1       20002  1066.198364\n",
      "2       20003   758.457703\n",
      "3       20004   493.570862\n",
      "4       20005   487.704651\n",
      "✅ Archivo generado: forecast_customer_producto_agregado_prophet_202002_v3.csv\n",
      "Productos únicos: 780\n",
      "Total TN predichas (sum): 26,054.17\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 4) Entrenar Autogluon Tabular\n",
    "# -------------------------------\n",
    "predictor = TabularPredictor(\n",
    "    label='clase',\n",
    "    problem_type='regression',\n",
    "    eval_metric='mae',\n",
    "    path='gcs_model_dir_con_prophet'\n",
    ")\n",
    "\n",
    "predictor.fit(\n",
    "    train_data=train_set[features + ['clase']],\n",
    "    time_limit=14400,\n",
    "     ag_args_fit={'sample_weight': 'sample_weight'},\n",
    "    presets='best_quality',\n",
    "    num_bag_folds=5,\n",
    "    num_stack_levels=4\n",
    ")\n",
    "\n",
    "print(\"✅ Entrenamiento finalizado.\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Predicciones sobre el test set\n",
    "# ---------------------------\n",
    "\n",
    "test_set['tn_pred'] = predictor.predict(test_set[features])\n",
    "test_set['tn_pred'] = test_set['tn_pred'].clip(lower=0)\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Agregar predicciones por producto\n",
    "# ---------------------------\n",
    "\n",
    "df_final = (\n",
    "    test_set.groupby('product_id', as_index=False)['tn_pred']\n",
    "    .sum()\n",
    "    .rename(columns={'tn_pred': 'tn'})\n",
    ")\n",
    "\n",
    "print(df_final.head())\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Exportar CSV final\n",
    "# ---------------------------\n",
    "\n",
    "df_final.to_csv('forecast_customer_producto_agregado_prophet_202002_v3.csv', index=False)\n",
    "\n",
    "print(\"✅ Archivo generado: forecast_customer_producto_agregado_prophet_202002_v3.csv\")\n",
    "print(f\"Productos únicos: {df_final['product_id'].nunique()}\")\n",
    "print(f\"Total TN predichas (sum): {df_final['tn'].sum():,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64420f0c-0e15-4dd6-9849-4532377f9afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Leaderboard:\n",
      "                  model  score_val          eval_metric  pred_time_val  \\\n",
      "0   WeightedEnsemble_L6  -0.080583  mean_absolute_error     444.839525   \n",
      "1   WeightedEnsemble_L4  -0.081157  mean_absolute_error     368.973023   \n",
      "2   WeightedEnsemble_L3  -0.081158  mean_absolute_error     260.967582   \n",
      "3     LightGBMXT_BAG_L3  -0.081286  mean_absolute_error     358.945425   \n",
      "4   WeightedEnsemble_L2  -0.081294  mean_absolute_error     141.455906   \n",
      "5     LightGBMXT_BAG_L2  -0.081450  mean_absolute_error     249.581079   \n",
      "6     LightGBMXT_BAG_L1  -0.081487  mean_absolute_error     119.396747   \n",
      "7     LightGBMXT_BAG_L4  -0.081520  mean_absolute_error     444.643486   \n",
      "8   WeightedEnsemble_L5  -0.081520  mean_absolute_error     444.829876   \n",
      "9     LightGBMXT_BAG_L5  -0.082191  mean_absolute_error     475.750342   \n",
      "10      LightGBM_BAG_L3  -0.082867  mean_absolute_error     270.613960   \n",
      "11      LightGBM_BAG_L2  -0.082898  mean_absolute_error     152.445604   \n",
      "12      LightGBM_BAG_L1  -0.083960  mean_absolute_error      21.860726   \n",
      "13      LightGBM_BAG_L4  -0.086116  mean_absolute_error     377.222751   \n",
      "14      LightGBM_BAG_L5  -0.177706  mean_absolute_error     459.317216   \n",
      "\n",
      "       fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n",
      "0   8183.924561                0.196039           3.191956            6   \n",
      "1   6779.986097                0.182848           1.044092            4   \n",
      "2   4769.732622                0.198372           1.089568            3   \n",
      "3   6519.187726               98.176215        1750.544672            3   \n",
      "4   2455.467558                0.198433           1.190562            2   \n",
      "5   4463.795350              108.323606        2009.518354            2   \n",
      "6   2137.456056              119.396747        2137.456056            1   \n",
      "7   8180.732605               75.853310        1401.790600            4   \n",
      "8   8181.755900                0.186390           1.023296            5   \n",
      "9   9000.577783               22.674281         617.637458            5   \n",
      "10  5028.397332                9.844750         259.754278            3   \n",
      "11  2759.124700               11.188131         304.847704            2   \n",
      "12   316.820940               21.860726         316.820940            1   \n",
      "13  6981.149724                8.432576         202.207720            4   \n",
      "14  8511.652830                6.241155         128.712505            5   \n",
      "\n",
      "    can_infer  fit_order  \n",
      "0        True         15  \n",
      "1        True          9  \n",
      "2        True          6  \n",
      "3        True          7  \n",
      "4        True          3  \n",
      "5        True          4  \n",
      "6        True          1  \n",
      "7        True         10  \n",
      "8        True         12  \n",
      "9        True         13  \n",
      "10       True          8  \n",
      "11       True          5  \n",
      "12       True          2  \n",
      "13       True         11  \n",
      "14       True         14  \n",
      "\n",
      "🔍 Importancia de Features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "These features in provided data are not utilized by the predictor and will be ignored: ['tn_34', 'diff_tn_34', 'tn_35', 'diff_tn_35', 'tn_36', 'diff_tn_36', 'rollmean_1', 'diff_rollmean_1', 'rollmean_34', 'diff_rollmean_34', 'rollmean_35', 'diff_rollmean_35', 'rollmean_36', 'diff_rollmean_36']\n",
      "Computing feature importance via permutation shuffling for 183 features using 5000 rows with 5 shuffle sets...\n",
      "\t3658.42s\t= Expected runtime (731.68s per shuffle set)\n",
      "\t843.08s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            feature  importance    stddev   p_value  n  p99_high   p99_low\n",
      "0             trend    0.035034  0.007000  0.000182  5  0.049447  0.020620\n",
      "1              tn_x    0.013414  0.004916  0.001825  5  0.023536  0.003292\n",
      "2    additive_terms    0.011871  0.003216  0.000588  5  0.018493  0.005248\n",
      "3              tn_1    0.008539  0.002767  0.001156  5  0.014236  0.002842\n",
      "4     slope_trend_3    0.007285  0.001732  0.000357  5  0.010852  0.003718\n",
      "..              ...         ...       ...       ... ..       ...       ...\n",
      "95            tn_17    0.000507  0.000553  0.054913  5  0.001646 -0.000632\n",
      "96            tn_27    0.000496  0.000223  0.003802  5  0.000955  0.000037\n",
      "97       diff_tn_25    0.000495  0.000666  0.086037  5  0.001867 -0.000877\n",
      "98        inflacion    0.000492  0.000293  0.009901  5  0.001095 -0.000111\n",
      "99  cluster_x_month    0.000484  0.000538  0.057166  5  0.001591 -0.000623\n",
      "\n",
      "[100 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 9) Leaderboard (performance interna)\n",
    "# -------------------------------\n",
    "print(\"\\n🔍 Leaderboard:\")\n",
    "lb = predictor.leaderboard(silent=True)\n",
    "print(lb)\n",
    "\n",
    "# -------------------------------\n",
    "# 10) Importancia de features\n",
    "# -------------------------------\n",
    "print(\"\\n🔍 Importancia de Features:\")\n",
    "fi = predictor.feature_importance(train_set[features + ['clase']])\n",
    "fi = fi.reset_index().rename(columns={'index': 'feature'})\n",
    "print(fi.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa87373-ea4f-4616-b2d9-51a9ed7bc962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29df506a-a4a2-4d0c-9433-fb31b33d3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, para guardar 'fi' en un archivo CSV:\n",
    "fi.to_csv('importancia_feature_modelo_customer_product_18-07-25_Autogluon_Tabular.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6aea9-739c-4b2d-bc2c-0de33163f922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
