{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dfd9ae5-edee-452b-929f-d13eecb983d2",
   "metadata": {},
   "source": [
    "# Modelado tabular con Autgluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba6e856-5142-4c55-9453-e7cd9cb05c58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://nvidia.github.io/libnvidia-container/stable/deb/amd64  InRelease\n",
      "Hit:2 https://deb.debian.org/debian bullseye InRelease              \n",
      "Hit:3 https://download.docker.com/linux/debian bullseye InRelease   \n",
      "Get:4 https://deb.debian.org/debian-security bullseye-security InRelease [27.2 kB]\n",
      "Hit:5 https://deb.debian.org/debian bullseye-updates InRelease\n",
      "Hit:6 https://deb.debian.org/debian bullseye-backports InRelease\n",
      "Hit:7 https://packages.cloud.google.com/apt gcsfuse-bullseye InRelease\n",
      "Hit:8 https://packages.cloud.google.com/apt google-compute-engine-bullseye-stable InRelease\n",
      "Hit:9 https://packages.cloud.google.com/apt cloud-sdk-bullseye InRelease\n",
      "Hit:10 https://packages.cloud.google.com/apt google-fast-socket InRelease\n",
      "Fetched 27.2 kB in 1s (32.7 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "gcsfuse is already the newest version (3.1.0).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install gcsfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cafae9d5-f167-4844-a54d-837399e5957d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install autogluon.timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714f161-c384-4582-9df4-b2e170562023",
   "metadata": {},
   "source": [
    "# Carga librerÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a34db802-71e7-4536-8484-73281601d474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from autogluon.timeseries import TimeSeriesPredictor\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80944f5-a7ef-4f3f-a194-ca859daa6053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p /home/jupyter/franco_maestria/gcs_model_dir_fullpower_hibrido_top150_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ddf2a47-869b-4059-ab6a-93745c17b19f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusermount: entry for /home/jupyter/franco_maestria/gcs_model_dir_fullpower_hibrido_top150_v3 not found in /etc/mtab\n"
     ]
    }
   ],
   "source": [
    "!fusermount -u /home/jupyter/franco_maestria/gcs_model_dir_fullpower_hibrido_top150_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165665a4-ea68-48fa-ae69-2dc8e0cd0149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"timestamp\":{\"seconds\":1752957183,\"nanos\":833161408},\"severity\":\"INFO\",\"message\":\"Start gcsfuse/3.1.0 (Go version go1.24.0) for app \\\"\\\" using mount point: /home/jupyter/franco_maestria/gcs_model_dir_fullpower_hibrido_top150_v3\\n\"}\n",
      "{\"timestamp\":{\"seconds\":1752957183,\"nanos\":833208216},\"severity\":\"INFO\",\"message\":\"GCSFuse config\",\"config\":{\"AppName\":\"\",\"CacheDir\":\"\",\"Debug\":{\"ExitOnInvariantViolation\":false,\"Fuse\":false,\"Gcs\":false,\"LogMutex\":false},\"DisableAutoconfig\":false,\"EnableAtomicRenameObject\":true,\"EnableGoogleLibAuth\":false,\"EnableHns\":true,\"EnableNewReader\":false,\"FileCache\":{\"CacheFileForRangeRead\":false,\"DownloadChunkSizeMb\":200,\"EnableCrc\":false,\"EnableODirect\":false,\"EnableParallelDownloads\":false,\"ExperimentalExcludeRegex\":\"\",\"ExperimentalParallelDownloadsDefaultOn\":true,\"MaxParallelDownloads\":96,\"MaxSizeMb\":-1,\"ParallelDownloadsPerFile\":16,\"WriteBufferSize\":4194304},\"FileSystem\":{\"DirMode\":\"755\",\"DisableParallelDirops\":false,\"ExperimentalEnableDentryCache\":false,\"ExperimentalEnableReaddirplus\":false,\"FileMode\":\"644\",\"FuseOptions\":[],\"Gid\":-1,\"IgnoreInterrupts\":true,\"KernelListCacheTtlSecs\":0,\"PreconditionErrors\":true,\"RenameDirLimit\":0,\"TempDir\":\"\",\"Uid\":-1},\"Foreground\":false,\"GcsAuth\":{\"AnonymousAccess\":false,\"KeyFile\":\"\",\"ReuseTokenFromUrl\":true,\"TokenUrl\":\"\"},\"GcsConnection\":{\"BillingProject\":\"\",\"ClientProtocol\":\"http1\",\"CustomEndpoint\":\"\",\"ExperimentalEnableJsonRead\":false,\"GrpcConnPoolSize\":1,\"HttpClientTimeout\":0,\"LimitBytesPerSec\":-1,\"LimitOpsPerSec\":-1,\"MaxConnsPerHost\":0,\"MaxIdleConnsPerHost\":100,\"SequentialReadSizeMb\":200},\"GcsRetries\":{\"ChunkTransferTimeoutSecs\":10,\"MaxRetryAttempts\":0,\"MaxRetrySleep\":30000000000,\"Multiplier\":2,\"ReadStall\":{\"Enable\":true,\"InitialReqTimeout\":20000000000,\"MaxReqTimeout\":1200000000000,\"MinReqTimeout\":1500000000,\"ReqIncreaseRate\":15,\"ReqTargetPercentile\":0.99}},\"ImplicitDirs\":false,\"List\":{\"EnableEmptyManagedFolders\":false},\"Logging\":{\"FilePath\":\"\",\"Format\":\"json\",\"LogRotate\":{\"BackupFileCount\":10,\"Compress\":true,\"MaxFileSizeMb\":512},\"Severity\":\"INFO\"},\"MachineType\":\"\",\"MetadataCache\":{\"DeprecatedStatCacheCapacity\":20460,\"DeprecatedStatCacheTtl\":60000000000,\"DeprecatedTypeCacheTtl\":60000000000,\"EnableNonexistentTypeCache\":false,\"ExperimentalMetadataPrefetchOnMount\":\"disabled\",\"NegativeTtlSecs\":5,\"StatCacheMaxSizeMb\":33,\"TtlSecs\":60,\"TypeCacheMaxSizeMb\":4},\"Metrics\":{\"CloudMetricsExportIntervalSecs\":0,\"PrometheusPort\":0,\"StackdriverExportInterval\":0,\"UseNewNames\":false},\"Monitoring\":{\"ExperimentalTracingMode\":\"\",\"ExperimentalTracingSamplingRatio\":0},\"OnlyDir\":\"\",\"Profiling\":{\"AllocatedHeap\":true,\"Cpu\":true,\"Enabled\":false,\"Goroutines\":false,\"Heap\":true,\"Label\":\"gcsfuse-0.0.0\",\"Mutex\":false},\"Read\":{\"InactiveStreamTimeout\":10000000000},\"Write\":{\"BlockSizeMb\":33554432,\"CreateEmptyFile\":false,\"EnableStreamingWrites\":true,\"ExperimentalEnableRapidAppends\":false,\"GlobalMaxBlocks\":4,\"MaxBlocksPerFile\":1}}}\n",
      "{\"timestamp\":{\"seconds\":1752957184,\"nanos\":66936810},\"severity\":\"INFO\",\"message\":\"File system has been successfully mounted.\"}\n"
     ]
    }
   ],
   "source": [
    "!gcsfuse forecasting_customer_product /home/jupyter/franco_maestria/gcs_model_dir_fullpower_hibrido_top150_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "670c745a-acfb-4d86-9e9b-163ced318b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CSV Prophet cargado. Shape: (7249573, 9)\n",
      "âœ… Merge completado. Shape final: (12138186, 200)\n",
      "âœ… ConversiÃ³n de float64 a float32 completada para columnas: ['tn_x', 'inflacion', 'cambio_dolar', 'stock_final', 'clase', 'tn_1', 'diff_tn_1', 'tn_2', 'diff_tn_2', 'tn_3', 'diff_tn_3', 'tn_4', 'diff_tn_4', 'tn_5', 'diff_tn_5', 'tn_6', 'diff_tn_6', 'tn_7', 'diff_tn_7', 'tn_8', 'diff_tn_8', 'tn_9', 'diff_tn_9', 'tn_10', 'diff_tn_10', 'tn_11', 'diff_tn_11', 'tn_12', 'diff_tn_12', 'tn_13', 'diff_tn_13', 'tn_14', 'diff_tn_14', 'tn_15', 'diff_tn_15', 'tn_16', 'diff_tn_16', 'tn_17', 'diff_tn_17', 'tn_18', 'diff_tn_18', 'tn_19', 'diff_tn_19', 'tn_20', 'diff_tn_20', 'tn_21', 'diff_tn_21', 'tn_22', 'diff_tn_22', 'tn_23', 'diff_tn_23', 'tn_24', 'diff_tn_24', 'tn_25', 'diff_tn_25', 'tn_26', 'diff_tn_26', 'tn_27', 'diff_tn_27', 'tn_28', 'diff_tn_28', 'tn_29', 'diff_tn_29', 'tn_30', 'diff_tn_30', 'tn_31', 'diff_tn_31', 'tn_32', 'diff_tn_32', 'tn_33', 'diff_tn_33', 'tn_34', 'diff_tn_34', 'tn_35', 'diff_tn_35', 'tn_36', 'diff_tn_36', 'rollmean_1', 'diff_rollmean_1', 'rollmean_2', 'diff_rollmean_2', 'rollmean_3', 'diff_rollmean_3', 'rollmean_4', 'diff_rollmean_4', 'rollmean_5', 'diff_rollmean_5', 'rollmean_6', 'diff_rollmean_6', 'rollmean_7', 'diff_rollmean_7', 'rollmean_8', 'diff_rollmean_8', 'rollmean_9', 'diff_rollmean_9', 'rollmean_10', 'diff_rollmean_10', 'rollmean_11', 'diff_rollmean_11', 'rollmean_12', 'diff_rollmean_12', 'rollmean_13', 'diff_rollmean_13', 'rollmean_14', 'diff_rollmean_14', 'rollmean_15', 'diff_rollmean_15', 'rollmean_16', 'diff_rollmean_16', 'rollmean_17', 'diff_rollmean_17', 'rollmean_18', 'diff_rollmean_18', 'rollmean_19', 'diff_rollmean_19', 'rollmean_20', 'diff_rollmean_20', 'rollmean_21', 'diff_rollmean_21', 'rollmean_22', 'diff_rollmean_22', 'rollmean_23', 'diff_rollmean_23', 'rollmean_24', 'diff_rollmean_24', 'rollmean_25', 'diff_rollmean_25', 'rollmean_26', 'diff_rollmean_26', 'rollmean_27', 'diff_rollmean_27', 'rollmean_28', 'diff_rollmean_28', 'rollmean_29', 'diff_rollmean_29', 'rollmean_30', 'diff_rollmean_30', 'rollmean_31', 'diff_rollmean_31', 'rollmean_32', 'diff_rollmean_32', 'rollmean_33', 'diff_rollmean_33', 'rollmean_34', 'diff_rollmean_34', 'rollmean_35', 'diff_rollmean_35', 'rollmean_36', 'diff_rollmean_36', 'month_sin', 'month_cos', 'pct_change_1', 'rolling_std_3', 'brand_avg', 'ratio_to_brand_avg', 'cat1_avg', 'ratio_to_cat1_avg', 'cat2_avg', 'ratio_to_cat2_avg', 'cat3_avg', 'ratio_to_cat3_avg', 'product_target_enc', 'customer_target_enc', 'tn_y', 'trend', 'seasonal', 'additive_terms', 'residual', 'slope_trend_3']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12138186 entries, 0 to 12138185\n",
      "Columns: 200 entries, customer_id to slope_trend_3\n",
      "dtypes: datetime64[ns](1), float32(169), int32(10), int64(14), object(6)\n",
      "memory usage: 10.0+ GB\n",
      "None\n",
      "âœ… Merge completado. Shape final: (12138186, 200)\n",
      "âœ… Parquet cargado. Shape: (12138186, 200)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# 1) Cargar parquet con FE\n",
    "# ------------------------\n",
    "\n",
    "parquet_path = \"panel_cliente_producto_fe.parquet\"\n",
    "df_modelo = pd.read_parquet(parquet_path)\n",
    "\n",
    "# ------------------------\n",
    "# 2) Cargar Prophet features\n",
    "# ------------------------\n",
    "csv_path = \"prophet_features_customer_product.csv\"\n",
    "df_prophet = pd.read_csv(csv_path)\n",
    "print(f\"âœ… CSV Prophet cargado. Shape: {df_prophet.shape}\")\n",
    "\n",
    "# ------------------------\n",
    "# 3) Asegurar consistencia de tipos\n",
    "# ------------------------\n",
    "df_modelo['fecha'] = pd.to_datetime(df_modelo['fecha'])\n",
    "df_prophet['fecha'] = pd.to_datetime(df_prophet['fecha'])\n",
    "\n",
    "# ------------------------\n",
    "# 4) Realizar join por 'product_id' y 'fecha'\n",
    "# ------------------------\n",
    "df_modelo_final = df_modelo.merge(\n",
    "    df_prophet,\n",
    "    on=['customer_id','product_id', 'fecha'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"âœ… Merge completado. Shape final: {df_modelo_final.shape}\")\n",
    "\n",
    "# ------------------------\n",
    "# 5) Convertir columnas float64 a float32 para ahorrar memoria\n",
    "# ------------------------\n",
    "float64_cols = df_modelo_final.select_dtypes(include=['float64']).columns.tolist()\n",
    "\n",
    "df_modelo_final[float64_cols] = df_modelo_final[float64_cols].astype('float32')\n",
    "\n",
    "print(f\"âœ… ConversiÃ³n de float64 a float32 completada para columnas: {float64_cols}\")\n",
    "print(df_modelo_final.info())\n",
    "\n",
    "print(f\"âœ… Merge completado. Shape final: {df_modelo_final.shape}\")\n",
    "\n",
    "# Verifica el resultado\n",
    "df_modelo_final.head()\n",
    "\n",
    "\n",
    "print(f\"âœ… Parquet cargado. Shape: {df_modelo_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b1b5eb9-c2c2-48e9-9c0d-60b8815f4319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 40 clientes:\n",
      "[10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008, 10009, 10010, 10011, 10012, 10013, 10014, 10016, 10015, 10017, 10018, 10019, 10020, 10023, 10022, 10021, 10024, 10025, 10039, 10026, 10028, 10030, 10033, 10027, 10034, 10032, 10031, 10038, 10035, 10037, 10036, 10042, 10041, 10082, 10029, 10045, 10044, 10043, 10040, 10047, 10046, 10074, 10051, 10048, 10050, 10057, 10049, 10053, 10055, 10054, 10127, 10052, 10060, 10056, 10064, 10061, 10062, 10059, 10063, 10058, 10066, 10065, 10067, 10069, 10072, 10071, 10068, 10076, 10073, 10070, 10075, 10081, 10080, 10084, 10086, 10088, 10077, 10078, 10090, 10089, 10079, 10091, 10083, 10085, 10097, 10096, 10087, 10094, 10092, 10093, 10101, 10099, 10098, 10095, 10102, 10104, 10105, 10109, 10113, 10103, 10107, 10108, 10110, 10112, 10106, 10136, 10111, 10115, 10118, 10117, 10121, 10100, 10114, 10116, 10130, 10119, 10122, 10125, 10120, 10124, 10123, 10129, 10126, 10133, 10131, 10139, 10138, 10143, 10142, 10137, 10135, 10132, 10134, 10147, 10141, 10128, 10229, 10140, 10152, 10144, 10153, 10146, 10150]\n",
      "TOP40 shape: (3308748, 200) | Otros shape: (8829438, 200)\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# ðŸ“¦ BLOQUE 1 â€” Filtrar TOP 70 clientes y resto\n",
    "# =============================================\n",
    "\n",
    "# 2) Calcular compra promedio por cliente\n",
    "cliente_avg = (\n",
    "    df_modelo_final.groupby('customer_id')['tn_x']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'tn_x': 'avg_tn'})\n",
    "    .sort_values('avg_tn', ascending=False)\n",
    ")\n",
    "\n",
    "# 3) Identificar TOP 20 clientes\n",
    "top_40_customers = cliente_avg.head(150)['customer_id'].tolist()\n",
    "print(f\"TOP 40 clientes:\\n{top_40_customers}\")\n",
    "\n",
    "# 4) Crear datasets\n",
    "df_top40 = df_modelo_final[df_modelo_final['customer_id'].isin(top_40_customers)].copy()\n",
    "df_otros = df_modelo_final[~df_modelo_final['customer_id'].isin(top_40_customers)].copy()\n",
    "\n",
    "print(f\"TOP40 shape: {df_top40.shape} | Otros shape: {df_otros.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "393ade4f-4930-476e-b4f8-cfaade2aa35a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"gcs_model_dir_fullpower_hibrido_top150_v3\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Debian 5.10.237-1 (2025-05-19)\n",
      "CPU Count:          48\n",
      "Memory Avail:       310.53 GB / 377.89 GB (82.2%)\n",
      "Disk Space Avail:   1048576.00 GB / 1048576.00 GB (100.0%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 2700s of the 10800s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-07-19 13:45:00,358\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/home/jupyter/franco_maestria/gcs_model_dir_fullpower_hibrido_top150_v3/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Beginning AutoGluon training ... Time limit = 2696s\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m AutoGluon will save models to \"/home/jupyter/franco_maestria/gcs_model_dir_fullpower_hibrido_top150_v3/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Train Data Rows:    2719472\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Train Data Columns: 196\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Label Column:       clase\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tAvailable Memory:                    311335.17 MB\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tTrain Data (Original)  Memory Usage: 2974.58 MB (1.0% of available memory)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\t\tNote: Converting 6 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tFitting DatetimeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tUseless Original Features (Count: 12): ['tn_34', 'diff_tn_34', 'tn_35', 'diff_tn_35', 'tn_36', 'diff_tn_36', 'rollmean_34', 'diff_rollmean_34', 'rollmean_35', 'diff_rollmean_35', 'rollmean_36', 'diff_rollmean_36']\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tUnused Original Features (Count: 2): ['rollmean_1', 'diff_rollmean_1']\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\t('float', []) : 2 | ['rollmean_1', 'diff_rollmean_1']\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\t('datetime', []) :   1 | ['fecha']\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\t('float', [])    : 152 | ['tn_x', 'inflacion', 'cambio_dolar', 'stock_final', 'tn_1', ...]\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\t('int', [])      :  24 | ['customer_id', 'product_id', 'IPC', 'dias_feriados', 'sku_size', ...]\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\t('object', [])   :   5 | ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\t('category', [])             :   5 | ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\t('float', [])                : 152 | ['tn_x', 'inflacion', 'cambio_dolar', 'stock_final', 'tn_1', ...]\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\t('int', [])                  :  18 | ['customer_id', 'product_id', 'IPC', 'dias_feriados', 'sku_size', ...]\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\t('int', ['bool'])            :   6 | ['is_min_3', 'is_max_3', 'is_min_6', 'is_max_6', 'is_min_12', ...]\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\t('int', ['datetime_as_int']) :   1 | ['fecha']\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t35.4s = Fit runtime\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t182 features in original data used to generate 182 features in processed data.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tTrain Data (Processed) Memory Usage: 1960.68 MB (0.6% of available memory)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Data preprocessing and feature engineering runtime = 39.86s ...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1770.48s of the 2656.38s of remaining time.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tWarning: Exception caused KNeighborsUnif_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 109, in _fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     self.model = self._fit_with_samples(X=X, y=y, model_params=params, time_limit=time_limit - (time.time() - time_start))\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 240, in _fit_with_samples\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     self.model = model_type(**model_params).fit(X_samp, y_samp)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     return fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_regression.py\", line 222, in fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     return self._fit(X, y)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_base.py\", line 478, in _fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     X, y = validate_data(\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     X, y = check_X_y(X, y, **check_params)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     X = check_array(\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     _assert_all_finite(\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     _assert_all_finite_element_wise(\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     raise ValueError(msg_err)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1749.68s of the 2635.57s of remaining time.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tWarning: Exception caused KNeighborsDist_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 109, in _fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     self.model = self._fit_with_samples(X=X, y=y, model_params=params, time_limit=time_limit - (time.time() - time_start))\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 240, in _fit_with_samples\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     self.model = model_type(**model_params).fit(X_samp, y_samp)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     return fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_regression.py\", line 222, in fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     return self._fit(X, y)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_base.py\", line 478, in _fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     X, y = validate_data(\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     X, y = check_X_y(X, y, **check_params)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     X = check_array(\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     _assert_all_finite(\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     _assert_all_finite_element_wise(\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     raise ValueError(msg_err)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1732.25s of the 2618.15s of remaining time.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=4.06%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=19655)\u001b[0m [1000]\tvalid_set's l1: 0.244261\n",
      "\u001b[36m(_ray_fit pid=19658)\u001b[0m [1000]\tvalid_set's l1: 0.241968\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19652)\u001b[0m [1000]\tvalid_set's l1: 0.243346\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19653)\u001b[0m [2000]\tvalid_set's l1: 0.239813\n",
      "\u001b[36m(_ray_fit pid=19655)\u001b[0m [2000]\tvalid_set's l1: 0.241575\n",
      "\u001b[36m(_ray_fit pid=19658)\u001b[0m [2000]\tvalid_set's l1: 0.239635\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19654)\u001b[0m [2000]\tvalid_set's l1: 0.245749\n",
      "\u001b[36m(_ray_fit pid=19656)\u001b[0m [2000]\tvalid_set's l1: 0.240813\n",
      "\u001b[36m(_ray_fit pid=19653)\u001b[0m [3000]\tvalid_set's l1: 0.238454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19659)\u001b[0m [3000]\tvalid_set's l1: 0.236942\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19658)\u001b[0m [3000]\tvalid_set's l1: 0.238537\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19654)\u001b[0m [3000]\tvalid_set's l1: 0.244268\n",
      "\u001b[36m(_ray_fit pid=19656)\u001b[0m [3000]\tvalid_set's l1: 0.239662\n",
      "\u001b[36m(_ray_fit pid=19653)\u001b[0m [4000]\tvalid_set's l1: 0.237867\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19655)\u001b[0m [4000]\tvalid_set's l1: 0.239181\n",
      "\u001b[36m(_ray_fit pid=19659)\u001b[0m [4000]\tvalid_set's l1: 0.235876\n",
      "\u001b[36m(_ray_fit pid=19658)\u001b[0m [4000]\tvalid_set's l1: 0.237869\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19654)\u001b[0m [4000]\tvalid_set's l1: 0.243504\n",
      "\u001b[36m(_ray_fit pid=19656)\u001b[0m [4000]\tvalid_set's l1: 0.238841\n",
      "\u001b[36m(_ray_fit pid=19652)\u001b[0m [4000]\tvalid_set's l1: 0.238203\n",
      "\u001b[36m(_ray_fit pid=19653)\u001b[0m [5000]\tvalid_set's l1: 0.237291\n",
      "\u001b[36m(_ray_fit pid=19659)\u001b[0m [5000]\tvalid_set's l1: 0.235241\n",
      "\u001b[36m(_ray_fit pid=19655)\u001b[0m [5000]\tvalid_set's l1: 0.238579\n",
      "\u001b[36m(_ray_fit pid=19658)\u001b[0m [5000]\tvalid_set's l1: 0.237278\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19654)\u001b[0m [5000]\tvalid_set's l1: 0.242766\n",
      "\u001b[36m(_ray_fit pid=19656)\u001b[0m [5000]\tvalid_set's l1: 0.238159\n",
      "\u001b[36m(_ray_fit pid=19652)\u001b[0m [5000]\tvalid_set's l1: 0.237419\n",
      "\u001b[36m(_ray_fit pid=19653)\u001b[0m [6000]\tvalid_set's l1: 0.236723\n",
      "\u001b[36m(_ray_fit pid=19659)\u001b[0m [6000]\tvalid_set's l1: 0.23452\n",
      "\u001b[36m(_ray_fit pid=19655)\u001b[0m [6000]\tvalid_set's l1: 0.238018\n",
      "\u001b[36m(_ray_fit pid=19658)\u001b[0m [6000]\tvalid_set's l1: 0.236754\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19654)\u001b[0m [6000]\tvalid_set's l1: 0.242193\n",
      "\u001b[36m(_ray_fit pid=19656)\u001b[0m [6000]\tvalid_set's l1: 0.237647\n",
      "\u001b[36m(_ray_fit pid=19652)\u001b[0m [6000]\tvalid_set's l1: 0.236932\n",
      "\u001b[36m(_ray_fit pid=19653)\u001b[0m [7000]\tvalid_set's l1: 0.236309\n",
      "\u001b[36m(_ray_fit pid=19659)\u001b[0m [7000]\tvalid_set's l1: 0.233973\n",
      "\u001b[36m(_ray_fit pid=19655)\u001b[0m [7000]\tvalid_set's l1: 0.237389\n",
      "\u001b[36m(_ray_fit pid=19658)\u001b[0m [7000]\tvalid_set's l1: 0.23635\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19654)\u001b[0m [7000]\tvalid_set's l1: 0.241681\n",
      "\u001b[36m(_ray_fit pid=19656)\u001b[0m [7000]\tvalid_set's l1: 0.23708\n",
      "\u001b[36m(_ray_fit pid=19652)\u001b[0m [7000]\tvalid_set's l1: 0.236414\n",
      "\u001b[36m(_ray_fit pid=19653)\u001b[0m [8000]\tvalid_set's l1: 0.236026\n",
      "\u001b[36m(_ray_fit pid=19655)\u001b[0m [8000]\tvalid_set's l1: 0.237032\n",
      "\u001b[36m(_ray_fit pid=19659)\u001b[0m [8000]\tvalid_set's l1: 0.233539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=19656)\u001b[0m \tRan out of time, early stopping on iteration 7429. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=19656)\u001b[0m \t[7390]\tvalid_set's l1: 0.236889\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t-0.237\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t1378.17s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t819.21s\t = Validation runtime\n",
      "\u001b[36m(_ray_fit pid=19658)\u001b[0m \tRan out of time, early stopping on iteration 7592. Best iteration is:\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19658)\u001b[0m \t[7574]\tvalid_set's l1: 0.235997\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 229.40s of the 1115.29s of remaining time.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=4.09%)\n",
      "\u001b[36m(_ray_fit pid=30676)\u001b[0m \tRan out of time, early stopping on iteration 886. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=30676)\u001b[0m \t[878]\tvalid_set's l1: 0.242282\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t-0.2429\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t175.59s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t40.72s\t = Validation runtime\n",
      "\u001b[36m(_ray_fit pid=30674)\u001b[0m \tRan out of time, early stopping on iteration 866. Best iteration is:\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=30674)\u001b[0m \t[866]\tvalid_set's l1: 0.247732\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 31.22s of the 917.12s of remaining time.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tWarning: Exception caused RandomForestMSE_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 219, in _fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     model = model.fit(X, y, sample_weight=sample_weight)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     return fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 375, in fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     estimator._compute_missing_values_in_feature_mask(\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 222, in _compute_missing_values_in_feature_mask\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     _assert_all_finite_element_wise(X, xp=np, allow_nan=True, **common_kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     raise ValueError(msg_err)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 12.06s of the 897.96s of remaining time.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=4.26%)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tTime limit exceeded... Skipping CatBoost_BAG_L1.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 815.19s of remaining time.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.722, 'LightGBM_BAG_L1': 0.278}\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t-0.2357\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t0.73s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 812.88s of the 811.93s of remaining time.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=4.14%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=32238)\u001b[0m [1000]\tvalid_set's l1: 0.23777\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=32239)\u001b[0m [1000]\tvalid_set's l1: 0.233799\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=32240)\u001b[0m [1000]\tvalid_set's l1: 0.241759\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=32238)\u001b[0m [2000]\tvalid_set's l1: 0.236605\n",
      "\u001b[36m(_ray_fit pid=32242)\u001b[0m [2000]\tvalid_set's l1: 0.241639\n",
      "\u001b[36m(_ray_fit pid=32245)\u001b[0m [2000]\tvalid_set's l1: 0.23335\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=32239)\u001b[0m [2000]\tvalid_set's l1: 0.232524\n",
      "\u001b[36m(_ray_fit pid=32241)\u001b[0m [2000]\tvalid_set's l1: 0.237499\n",
      "\u001b[36m(_ray_fit pid=32238)\u001b[0m [3000]\tvalid_set's l1: 0.236186\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=32241)\u001b[0m [3000]\tvalid_set's l1: 0.237096\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=32240)\u001b[0m [3000]\tvalid_set's l1: 0.239741\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=32240)\u001b[0m \tRan out of time, early stopping on iteration 3376. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=32240)\u001b[0m \t[3263]\tvalid_set's l1: 0.23959\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t-0.2372\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t641.65s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t106.15s\t = Validation runtime\n",
      "\u001b[36m(_ray_fit pid=32241)\u001b[0m \tRan out of time, early stopping on iteration 3454. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=32241)\u001b[0m \t[3022]\tvalid_set's l1: 0.237073\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 136.26s of the 135.31s of remaining time.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=4.17%)\n",
      "\u001b[36m(_ray_fit pid=41460)\u001b[0m \tRan out of time, early stopping on iteration 411. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=41460)\u001b[0m \t[404]\tvalid_set's l1: 0.236201\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t-0.2369\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t100.56s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t9.93s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 16.08s of the 15.13s of remaining time.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tWarning: Exception caused RandomForestMSE_BAG_L2 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 219, in _fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     model = model.fit(X, y, sample_weight=sample_weight)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     return fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 375, in fit\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     estimator._compute_missing_values_in_feature_mask(\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 222, in _compute_missing_values_in_feature_mask\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     _assert_all_finite_element_wise(X, xp=np, allow_nan=True, **common_kwargs)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m     raise ValueError(msg_err)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_ray_fit pid=41464)\u001b[0m \tRan out of time, early stopping on iteration 413. Best iteration is:\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=41464)\u001b[0m \t[406]\tvalid_set's l1: 0.239606\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -73.81s of remaining time.\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.417, 'LightGBM_BAG_L2': 0.333, 'LightGBMXT_BAG_L2': 0.25}\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t-0.2341\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t1.12s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m AutoGluon training complete, total runtime = 2804.3s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 348.3 rows/s (339934 batch size)\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/jupyter/franco_maestria/gcs_model_dir_fullpower_hibrido_top150_v3/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=15633)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val          eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3      -0.233493  -0.234103  mean_absolute_error       94.291226     976.028800  2297.095484                 0.116129                0.029302           1.124429            3       True          6\n",
      "1    LightGBMXT_BAG_L1      -0.234530  -0.237046  mean_absolute_error       56.034818     819.206730  1378.165382                56.034818              819.206730        1378.165382            1       True          1\n",
      "2  WeightedEnsemble_L2      -0.234977  -0.235652  mean_absolute_error       64.675184     859.967778  1554.487376                 0.131225                0.043486           0.727403            2       True          3\n",
      "3      LightGBM_BAG_L2      -0.235186  -0.236853  mean_absolute_error       69.875146     869.852605  1654.320970                 5.331187                9.928313         100.560997            2       True          5\n",
      "4    LightGBMXT_BAG_L2      -0.235656  -0.237159  mean_absolute_error       88.843910     966.071185  2195.410058                24.299951              106.146893         641.650084            2       True          4\n",
      "5      LightGBM_BAG_L1      -0.243434  -0.242908  mean_absolute_error        8.509141      40.717563   175.594591                 8.509141               40.717563         175.594591            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t2940s\t = DyStack   runtime |\t7860s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 7860s\n",
      "AutoGluon will save models to \"/home/jupyter/franco_maestria/gcs_model_dir_fullpower_hibrido_top150_v3\"\n",
      "Train Data Rows:    3059407\n",
      "Train Data Columns: 196\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    309855.59 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3348.49 MB (1.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 6 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 12): ['tn_34', 'diff_tn_34', 'tn_35', 'diff_tn_35', 'tn_36', 'diff_tn_36', 'rollmean_34', 'diff_rollmean_34', 'rollmean_35', 'diff_rollmean_35', 'rollmean_36', 'diff_rollmean_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['rollmean_1', 'diff_rollmean_1']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['rollmean_1', 'diff_rollmean_1']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) :   1 | ['fecha']\n",
      "\t\t('float', [])    : 152 | ['tn_x', 'inflacion', 'cambio_dolar', 'stock_final', 'tn_1', ...]\n",
      "\t\t('int', [])      :  24 | ['customer_id', 'product_id', 'IPC', 'dias_feriados', 'sku_size', ...]\n",
      "\t\t('object', [])   :   5 | ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             :   5 | ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
      "\t\t('float', [])                : 152 | ['tn_x', 'inflacion', 'cambio_dolar', 'stock_final', 'tn_1', ...]\n",
      "\t\t('int', [])                  :  18 | ['customer_id', 'product_id', 'IPC', 'dias_feriados', 'sku_size', ...]\n",
      "\t\t('int', ['bool'])            :   6 | ['is_min_3', 'is_max_3', 'is_min_6', 'is_max_6', 'is_min_12', ...]\n",
      "\t\t('int', ['datetime_as_int']) :   1 | ['fecha']\n",
      "\t34.3s = Fit runtime\n",
      "\t182 features in original data used to generate 182 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2205.77 MB (0.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 37.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 5213.74s of the 7822.56s of remaining time.\n",
      "\tWarning: Exception caused KNeighborsUnif_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "    self._fit_single(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "    model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 109, in _fit\n",
      "    self.model = self._fit_with_samples(X=X, y=y, model_params=params, time_limit=time_limit - (time.time() - time_start))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 240, in _fit_with_samples\n",
      "    self.model = model_type(**model_params).fit(X_samp, y_samp)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_regression.py\", line 222, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_base.py\", line 478, in _fit\n",
      "    X, y = validate_data(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 5195.79s of the 7804.61s of remaining time.\n",
      "\tWarning: Exception caused KNeighborsDist_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "    self._fit_single(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "    model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 109, in _fit\n",
      "    self.model = self._fit_with_samples(X=X, y=y, model_params=params, time_limit=time_limit - (time.time() - time_start))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 240, in _fit_with_samples\n",
      "    self.model = model_type(**model_params).fit(X_samp, y_samp)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_regression.py\", line 222, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_base.py\", line 478, in _fit\n",
      "    X, y = validate_data(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5179.63s of the 7788.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=4.60%)\n",
      "\t-0.235\t = Validation score   (-mean_absolute_error)\n",
      "\t1822.36s\t = Training   runtime\n",
      "\t1281.1s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 3186.89s of the 5795.70s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=4.60%)\n",
      "\t-0.232\t = Validation score   (-mean_absolute_error)\n",
      "\t1967.36s\t = Training   runtime\n",
      "\t657.21s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 1131.38s of the 3740.20s of remaining time.\n",
      "\tWarning: Exception caused RandomForestMSE_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "    self._fit_single(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "    model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 219, in _fit\n",
      "    model = model.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 375, in fit\n",
      "    estimator._compute_missing_values_in_feature_mask(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 222, in _compute_missing_values_in_feature_mask\n",
      "    _assert_all_finite_element_wise(X, xp=np, allow_nan=True, **common_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1112.24s of the 3721.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=4.77%)\n",
      "\t-0.2494\t = Validation score   (-mean_absolute_error)\n",
      "\t881.65s\t = Training   runtime\n",
      "\t2.23s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 213.56s of the 2822.38s of remaining time.\n",
      "\tWarning: Exception caused ExtraTreesMSE_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "    self._fit_single(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "    model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 219, in _fit\n",
      "    model = model.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 375, in fit\n",
      "    estimator._compute_missing_values_in_feature_mask(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 222, in _compute_missing_values_in_feature_mask\n",
      "    _assert_all_finite_element_wise(X, xp=np, allow_nan=True, **common_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 194.36s of the 2803.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=8.07%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=79005, ip=10.70.80.80)\n",
      "ModuleNotFoundError: No module named 'fastai'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=79005, ip=10.70.80.80)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 230, in _fit\n",
      "    try_import_fastai()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/common/utils/try_import.py\", line 141, in try_import_fastai\n",
      "    raise ImportError(\n",
      "ImportError: Import fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`.\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 172.38s of the 2781.19s of remaining time.\n",
      "2025-07-19 15:59:28,559\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 15:59:28,561\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 15:59:28,561\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 15:59:28,563\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 15:59:28,563\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 15:59:28,564\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 15:59:28,565\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=6.04%)\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=79961, ip=10.70.80.80)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1222, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py\", line 628, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1137, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 1614, in __init__\n",
      "    self._init(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 1680, in _init\n",
      "    _check_call(ret)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [16:00:40] /workspace/src/data/gradient_index.h:100: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n",
      "Stack trace:\n",
      "  [bt] (0) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2a6ecc) [0x7f37ad60aecc]\n",
      "  [bt] (1) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5e9ec4) [0x7f37ad94dec4]\n",
      "  [bt] (2) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5ea949) [0x7f37ad94e949]\n",
      "  [bt] (3) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x57b541) [0x7f37ad8df541]\n",
      "  [bt] (4) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGQuantileDMatrixCreateFromCallback+0x178) [0x7f37ad51d3c8]\n",
      "  [bt] (5) /opt/conda/lib/python3.10/lib-dynload/../../libffi.so.8(+0x6d8a) [0x7f525df54d8a]\n",
      "  [bt] (6) /opt/conda/lib/python3.10/lib-dynload/../../libffi.so.8(+0x61cd) [0x7f525df541cd]\n",
      "  [bt] (7) /opt/conda/lib/python3.10/lib-dynload/../../libffi.so.8(ffi_call+0xcd) [0x7f525df5491d]\n",
      "  [bt] (8) /opt/conda/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x931f) [0x7f525dc9b31f]\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(XGBoostError): \u001b[36mray::_ray_fit()\u001b[39m (pid=79961, ip=10.70.80.80)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1222, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py\", line 628, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1137, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 1614, in __init__\n",
      "    self._init(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 1680, in _init\n",
      "    _check_call(ret)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [16:00:40] /workspace/src/data/gradient_index.h:100: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n",
      "Stack trace:\n",
      "  [bt] (0) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2a6ecc) [0x7f37ad60aecc]\n",
      "  [bt] (1) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5e9ec4) [0x7f37ad94dec4]\n",
      "  [bt] (2) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5ea949) [0x7f37ad94e949]\n",
      "  [bt] (3) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x57b541) [0x7f37ad8df541]\n",
      "  [bt] (4) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGQuantileDMatrixCreateFromCallback+0x178) [0x7f37ad51d3c8]\n",
      "  [bt] (5) /opt/conda/lib/python3.10/lib-dynload/../../libffi.so.8(+0x6d8a) [0x7f525df54d8a]\n",
      "  [bt] (6) /opt/conda/lib/python3.10/lib-dynload/../../libffi.so.8(+0x61cd) [0x7f525df541cd]\n",
      "  [bt] (7) /opt/conda/lib/python3.10/lib-dynload/../../libffi.so.8(ffi_call+0xcd) [0x7f525df5491d]\n",
      "  [bt] (8) /opt/conda/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x931f) [0x7f525dc9b31f]\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 94.62s of the 2703.44s of remaining time.\n",
      "2025-07-19 16:00:45,589\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:00:45,590\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:00:45,591\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:00:45,592\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:00:45,592\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:00:45,593\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:00:45,594\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=4.38%)\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=80999, ip=10.70.80.80)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 759, in _process_train_data\n",
      "    df = self.processor.fit_transform(df)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 1001, in fit_transform\n",
      "    result = self._call_func_on_transformers(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 910, in _call_func_on_transformers\n",
      "    return Parallel(n_jobs=self.n_jobs)(jobs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 1986, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 1914, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 718, in fit_transform\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 588, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 918, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py\", line 434, in fit\n",
      "    X = self._validate_input(X, in_fit=True)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py\", line 363, in _validate_input\n",
      "    raise ve\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py\", line 344, in _validate_input\n",
      "    X = validate_data(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2944, in validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float64').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::_ray_fit()\u001b[39m (pid=80999, ip=10.70.80.80)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 759, in _process_train_data\n",
      "    df = self.processor.fit_transform(df)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 1001, in fit_transform\n",
      "    result = self._call_func_on_transformers(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 910, in _call_func_on_transformers\n",
      "    return Parallel(n_jobs=self.n_jobs)(jobs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 1986, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 1914, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 718, in fit_transform\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 588, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 918, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py\", line 434, in fit\n",
      "    X = self._validate_input(X, in_fit=True)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py\", line 363, in _validate_input\n",
      "    raise ve\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py\", line 344, in _validate_input\n",
      "    X = validate_data(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2944, in validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float64').\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 56.34s of the 2665.15s of remaining time.\n",
      "2025-07-19 16:01:24,603\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:01:24,605\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:01:24,605\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:01:24,606\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:01:24,607\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:01:24,607\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:01:24,608\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=4.61%)\n",
      "\t-0.2757\t = Validation score   (-mean_absolute_error)\n",
      "\t38.13s\t = Training   runtime\n",
      "\t4.05s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 1.25s of the 2610.06s of remaining time.\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -4.6s)\n",
      "\tTime limit exceeded... Skipping CatBoost_r177_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 521.37s of the 2539.47s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.625, 'LightGBMXT_BAG_L1': 0.375}\n",
      "\t-0.2303\t = Validation score   (-mean_absolute_error)\n",
      "\t1.16s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2536.25s of the 2535.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=4.70%)\n",
      "\t-0.234\t = Validation score   (-mean_absolute_error)\n",
      "\t827.52s\t = Training   runtime\n",
      "\t90.02s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1674.46s of the 1673.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=4.70%)\n",
      "\t-0.2321\t = Validation score   (-mean_absolute_error)\n",
      "\t284.75s\t = Training   runtime\n",
      "\t22.76s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1367.22s of the 1366.11s of remaining time.\n",
      "\tWarning: Exception caused RandomForestMSE_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "    self._fit_single(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "    model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 219, in _fit\n",
      "    model = model.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 375, in fit\n",
      "    estimator._compute_missing_values_in_feature_mask(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 222, in _compute_missing_values_in_feature_mask\n",
      "    _assert_all_finite_element_wise(X, xp=np, allow_nan=True, **common_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 1345.08s of the 1343.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=4.90%)\n",
      "\t-0.2338\t = Validation score   (-mean_absolute_error)\n",
      "\t1067.22s\t = Training   runtime\n",
      "\t4.28s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 259.31s of the 258.20s of remaining time.\n",
      "\tWarning: Exception caused ExtraTreesMSE_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "    self._fit_single(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "    model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 219, in _fit\n",
      "    model = model.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 375, in fit\n",
      "    estimator._compute_missing_values_in_feature_mask(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 222, in _compute_missing_values_in_feature_mask\n",
      "    _assert_all_finite_element_wise(X, xp=np, allow_nan=True, **common_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 236.86s of the 235.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=8.31%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=98763, ip=10.70.80.80)\n",
      "ModuleNotFoundError: No module named 'fastai'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=98763, ip=10.70.80.80)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 230, in _fit\n",
      "    try_import_fastai()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/common/utils/try_import.py\", line 141, in try_import_fastai\n",
      "    raise ImportError(\n",
      "ImportError: Import fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`.\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 212.90s of the 211.79s of remaining time.\n",
      "2025-07-19 16:42:18,074\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:42:18,075\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:42:18,076\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:42:18,076\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:42:18,077\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:42:18,078\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:42:18,079\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=6.21%)\n",
      "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=99719, ip=10.70.80.80)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1222, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py\", line 628, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1137, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 1614, in __init__\n",
      "    self._init(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 1680, in _init\n",
      "    _check_call(ret)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [16:43:34] /workspace/src/data/gradient_index.h:100: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n",
      "Stack trace:\n",
      "  [bt] (0) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2a6ecc) [0x7f030eb65ecc]\n",
      "  [bt] (1) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5e9ec4) [0x7f030eea8ec4]\n",
      "  [bt] (2) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5ea949) [0x7f030eea9949]\n",
      "  [bt] (3) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x57b541) [0x7f030ee3a541]\n",
      "  [bt] (4) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGQuantileDMatrixCreateFromCallback+0x178) [0x7f030ea783c8]\n",
      "  [bt] (5) /opt/conda/lib/python3.10/lib-dynload/../../libffi.so.8(+0x6d8a) [0x7f1c4e559d8a]\n",
      "  [bt] (6) /opt/conda/lib/python3.10/lib-dynload/../../libffi.so.8(+0x61cd) [0x7f1c4e5591cd]\n",
      "  [bt] (7) /opt/conda/lib/python3.10/lib-dynload/../../libffi.so.8(ffi_call+0xcd) [0x7f1c4e55991d]\n",
      "  [bt] (8) /opt/conda/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x931f) [0x7f1c4e2a031f]\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(XGBoostError): \u001b[36mray::_ray_fit()\u001b[39m (pid=99719, ip=10.70.80.80)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1222, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py\", line 628, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1137, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 1614, in __init__\n",
      "    self._init(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 1680, in _init\n",
      "    _check_call(ret)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [16:43:34] /workspace/src/data/gradient_index.h:100: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n",
      "Stack trace:\n",
      "  [bt] (0) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2a6ecc) [0x7f030eb65ecc]\n",
      "  [bt] (1) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5e9ec4) [0x7f030eea8ec4]\n",
      "  [bt] (2) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5ea949) [0x7f030eea9949]\n",
      "  [bt] (3) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x57b541) [0x7f030ee3a541]\n",
      "  [bt] (4) /opt/conda/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGQuantileDMatrixCreateFromCallback+0x178) [0x7f030ea783c8]\n",
      "  [bt] (5) /opt/conda/lib/python3.10/lib-dynload/../../libffi.so.8(+0x6d8a) [0x7f1c4e559d8a]\n",
      "  [bt] (6) /opt/conda/lib/python3.10/lib-dynload/../../libffi.so.8(+0x61cd) [0x7f1c4e5591cd]\n",
      "  [bt] (7) /opt/conda/lib/python3.10/lib-dynload/../../libffi.so.8(ffi_call+0xcd) [0x7f1c4e55991d]\n",
      "  [bt] (8) /opt/conda/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x931f) [0x7f1c4e2a031f]\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 130.04s of the 128.94s of remaining time.\n",
      "2025-07-19 16:43:41,107\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:43:41,108\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:43:41,109\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:43:41,110\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:43:41,110\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:43:41,111\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:43:41,112\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=4.50%)\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=100726, ip=10.70.80.80)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 759, in _process_train_data\n",
      "    df = self.processor.fit_transform(df)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 1001, in fit_transform\n",
      "    result = self._call_func_on_transformers(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 910, in _call_func_on_transformers\n",
      "    return Parallel(n_jobs=self.n_jobs)(jobs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 1986, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 1914, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 718, in fit_transform\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 588, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 918, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py\", line 434, in fit\n",
      "    X = self._validate_input(X, in_fit=True)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py\", line 363, in _validate_input\n",
      "    raise ve\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py\", line 344, in _validate_input\n",
      "    X = validate_data(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2944, in validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float64').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::_ray_fit()\u001b[39m (pid=100726, ip=10.70.80.80)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 759, in _process_train_data\n",
      "    df = self.processor.fit_transform(df)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 1001, in fit_transform\n",
      "    result = self._call_func_on_transformers(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 910, in _call_func_on_transformers\n",
      "    return Parallel(n_jobs=self.n_jobs)(jobs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 1986, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 1914, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 718, in fit_transform\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 588, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 918, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py\", line 434, in fit\n",
      "    X = self._validate_input(X, in_fit=True)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py\", line 363, in _validate_input\n",
      "    raise ve\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py\", line 344, in _validate_input\n",
      "    X = validate_data(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2944, in validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float64').\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 88.93s of the 87.83s of remaining time.\n",
      "2025-07-19 16:44:22,123\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:44:22,125\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:44:22,126\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:44:22,127\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:44:22,128\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:44:22,129\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-07-19 16:44:22,129\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=6, gpus=0, memory=4.74%)\n",
      "\t-0.2356\t = Validation score   (-mean_absolute_error)\n",
      "\t63.27s\t = Training   runtime\n",
      "\t5.47s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -78.80s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.368, 'LightGBM_BAG_L2': 0.263, 'LightGBMXT_BAG_L1': 0.158, 'LightGBMXT_BAG_L2': 0.158, 'CatBoost_BAG_L2': 0.053}\n",
      "\t-0.2293\t = Validation score   (-mean_absolute_error)\n",
      "\t2.14s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 7989.29s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 185.5 rows/s (382426 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/jupyter/franco_maestria/gcs_model_dir_fullpower_hibrido_top150_v3\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_id           tn\n",
      "0       20001  1301.626465\n",
      "1       20002  1091.926025\n",
      "2       20003   730.439636\n",
      "3       20004   449.942505\n",
      "4       20005   406.428040\n",
      "âœ… Forecast TOP100 guardado.\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# ðŸš€ BLOQUE 2 â€” Modelo TABULAR para TOP 40 clientes\n",
    "# ====================================================\n",
    "\n",
    "# âš™ï¸ Separar Train/Test\n",
    "df_top40['fecha'] = pd.to_datetime(df_top40['fecha'])\n",
    "train_top40 = df_top40[(df_top40['fecha'] <= '2019-10-01') & df_top40['clase'].notnull()].copy()\n",
    "test_top40 = df_top40[df_top40['fecha'] == '2019-12-01'].copy()\n",
    "\n",
    "# Escalar magnitud de toneladas vendidas\n",
    "train_top40['sample_weight'] = train_top40['tn_x']\n",
    "\n",
    "features_top40 = [col for col in df_top40.columns if col not in ['periodo', 'clase', 'tn_y','seasonal']]\n",
    "\n",
    "# âš™ï¸ Entrenar predictor\n",
    "predictor_top40 = TabularPredictor(label='clase', problem_type='regression', eval_metric='mae',\n",
    "    path='gcs_model_dir_fullpower_hibrido_top150_v3')\n",
    "predictor_top40.fit(\n",
    "    train_data=train_top40[features_top40 + ['clase']],\n",
    "    presets='best_quality',\n",
    "    time_limit=10800,\n",
    "    ag_args_fit={'sample_weight': 'sample_weight'}\n",
    ")\n",
    "\n",
    "# âš™ï¸ PredicciÃ³n y agregado por producto\n",
    "test_top40['tn_pred'] = predictor_top40.predict(test_top40[features_top40])\n",
    "df_top40_pred = (\n",
    "    test_top40.groupby('product_id')['tn_pred']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={'tn_pred': 'tn'})\n",
    ")\n",
    "print(df_top40_pred.head())\n",
    "\n",
    "# âš™ï¸ Guardar CSV parcial\n",
    "df_top40_pred.to_csv('forecast_top150_202002_v3.csv', index=False)\n",
    "print(\"âœ… Forecast TOP100 guardado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d3a5ddd-aa41-4daf-8a83-eafb30083dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"gcs_model_dir_fullpower_hibrido_top150_v3\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Debian 5.10.237-1 (2025-05-19)\n",
      "CPU Count:          48\n",
      "Memory Avail:       303.24 GB / 377.89 GB (80.2%)\n",
      "Disk Space Avail:   1048576.00 GB / 1048576.00 GB (100.0%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 2700s of the 10800s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-07-19 20:35:18,010\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/home/jupyter/franco_maestria/gcs_model_dir_fullpower_hibrido_top150_v3/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Beginning AutoGluon training ... Time limit = 2695s\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m AutoGluon will save models to \"/home/jupyter/franco_maestria/gcs_model_dir_fullpower_hibrido_top150_v3/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Train Data Rows:    6434296\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Train Data Columns: 196\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Label Column:       clase\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tAvailable Memory:                    297825.57 MB\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tTrain Data (Original)  Memory Usage: 7037.98 MB (2.4% of available memory)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\t\tNote: Converting 6 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tFitting DatetimeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tUseless Original Features (Count: 12): ['tn_34', 'diff_tn_34', 'tn_35', 'diff_tn_35', 'tn_36', 'diff_tn_36', 'rollmean_34', 'diff_rollmean_34', 'rollmean_35', 'diff_rollmean_35', 'rollmean_36', 'diff_rollmean_36']\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tUnused Original Features (Count: 2): ['rollmean_1', 'diff_rollmean_1']\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\t('float', []) : 2 | ['rollmean_1', 'diff_rollmean_1']\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\t('datetime', []) :   1 | ['fecha']\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\t('float', [])    : 152 | ['tn_x', 'inflacion', 'cambio_dolar', 'stock_final', 'tn_1', ...]\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\t('int', [])      :  24 | ['customer_id', 'product_id', 'IPC', 'dias_feriados', 'sku_size', ...]\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\t('object', [])   :   5 | ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\t('category', [])             :   5 | ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\t('float', [])                : 152 | ['tn_x', 'inflacion', 'cambio_dolar', 'stock_final', 'tn_1', ...]\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\t('int', [])                  :  18 | ['customer_id', 'product_id', 'IPC', 'dias_feriados', 'sku_size', ...]\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\t('int', ['bool'])            :   6 | ['is_min_3', 'is_max_3', 'is_min_6', 'is_max_6', 'is_min_12', ...]\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\t('int', ['datetime_as_int']) :   1 | ['fecha']\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t80.1s = Fit runtime\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t182 features in original data used to generate 182 features in processed data.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tTrain Data (Processed) Memory Usage: 4638.99 MB (1.6% of available memory)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Data preprocessing and feature engineering runtime = 90.64s ...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1736.11s of the 2604.80s of remaining time.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tWarning: Exception caused KNeighborsUnif_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 109, in _fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     self.model = self._fit_with_samples(X=X, y=y, model_params=params, time_limit=time_limit - (time.time() - time_start))\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 240, in _fit_with_samples\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     self.model = model_type(**model_params).fit(X_samp, y_samp)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     return fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_regression.py\", line 222, in fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     return self._fit(X, y)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_base.py\", line 478, in _fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     X, y = validate_data(\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     X, y = check_X_y(X, y, **check_params)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     X = check_array(\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     _assert_all_finite(\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     _assert_all_finite_element_wise(\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     raise ValueError(msg_err)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1696.09s of the 2564.79s of remaining time.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tWarning: Exception caused KNeighborsDist_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 109, in _fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     self.model = self._fit_with_samples(X=X, y=y, model_params=params, time_limit=time_limit - (time.time() - time_start))\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 240, in _fit_with_samples\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     self.model = model_type(**model_params).fit(X_samp, y_samp)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     return fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_regression.py\", line 222, in fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     return self._fit(X, y)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_base.py\", line 478, in _fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     X, y = validate_data(\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     X, y = check_X_y(X, y, **check_params)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     X = check_array(\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     _assert_all_finite(\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     _assert_all_finite_element_wise(\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     raise ValueError(msg_err)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1659.35s of the 2528.04s of remaining time.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.26% memory usage per fold, 41.05%/80.00% total).\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=10.26%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=19117)\u001b[0m [1000]\tvalid_set's l1: 0.0126862\n",
      "\u001b[36m(_ray_fit pid=19119)\u001b[0m [1000]\tvalid_set's l1: 0.0127624\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19118)\u001b[0m [1000]\tvalid_set's l1: 0.0127661\n",
      "\u001b[36m(_ray_fit pid=19117)\u001b[0m [2000]\tvalid_set's l1: 0.0125771\n",
      "\u001b[36m(_ray_fit pid=19119)\u001b[0m [2000]\tvalid_set's l1: 0.0126589\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19118)\u001b[0m [2000]\tvalid_set's l1: 0.0126649\n",
      "\u001b[36m(_ray_fit pid=19117)\u001b[0m [3000]\tvalid_set's l1: 0.0125199\n",
      "\u001b[36m(_ray_fit pid=19120)\u001b[0m [3000]\tvalid_set's l1: 0.0124774\n",
      "\u001b[36m(_ray_fit pid=19119)\u001b[0m [3000]\tvalid_set's l1: 0.0126119\n",
      "\u001b[36m(_ray_fit pid=19118)\u001b[0m [3000]\tvalid_set's l1: 0.012599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=19117)\u001b[0m \tRan out of time, early stopping on iteration 3494. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=19117)\u001b[0m \t[3494]\tvalid_set's l1: 0.0124968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=28308)\u001b[0m [1000]\tvalid_set's l1: 0.0126449\n",
      "\u001b[36m(_ray_fit pid=28308)\u001b[0m [2000]\tvalid_set's l1: 0.0125595\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=28308)\u001b[0m [3000]\tvalid_set's l1: 0.0124964\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=28461)\u001b[0m [3000]\tvalid_set's l1: 0.0124802\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=28308)\u001b[0m \tRan out of time, early stopping on iteration 3329. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=28308)\u001b[0m \t[3321]\tvalid_set's l1: 0.0124889\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t-0.0126\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t1340.51s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t182.63s\t = Validation runtime\n",
      "\u001b[36m(_ray_fit pid=28453)\u001b[0m \tRan out of time, early stopping on iteration 3320. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=28453)\u001b[0m \t[3311]\tvalid_set's l1: 0.0125775\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 263.53s of the 1132.23s of remaining time.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.46% memory usage per fold, 41.83%/80.00% total).\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=10.46%)\n",
      "\u001b[36m(_ray_fit pid=30292)\u001b[0m \tRan out of time, early stopping on iteration 319. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=30292)\u001b[0m \t[319]\tvalid_set's l1: 0.0129666\n",
      "\u001b[36m(_ray_fit pid=30896)\u001b[0m \tRan out of time, early stopping on iteration 342. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=30896)\u001b[0m \t[340]\tvalid_set's l1: 0.0130855\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t-0.0129\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t202.3s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t13.9s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 28.06s of the 896.76s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=30981)\u001b[0m \tRan out of time, early stopping on iteration 372. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=30981)\u001b[0m \t[372]\tvalid_set's l1: 0.0127717\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tWarning: Exception caused RandomForestMSE_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 219, in _fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     model = model.fit(X, y, sample_weight=sample_weight)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     return fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 375, in fit\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     estimator._compute_missing_values_in_feature_mask(\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 222, in _compute_missing_values_in_feature_mask\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     _assert_all_finite_element_wise(X, xp=np, allow_nan=True, **common_kwargs)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m     raise ValueError(msg_err)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 782.74s of remaining time.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.857, 'LightGBM_BAG_L1': 0.143}\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t-0.0125\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t0.78s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 780.01s of the 777.68s of remaining time.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.75% memory usage per fold, 43.02%/80.00% total).\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=10.75%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=32014)\u001b[0m [1000]\tvalid_set's l1: 0.0126652\n",
      "\u001b[36m(_ray_fit pid=32016)\u001b[0m [1000]\tvalid_set's l1: 0.0123921\n",
      "\u001b[36m(_ray_fit pid=32013)\u001b[0m [1000]\tvalid_set's l1: 0.0125863\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=32015)\u001b[0m \tRan out of time, early stopping on iteration 1401. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=32015)\u001b[0m \t[1399]\tvalid_set's l1: 0.0124151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=33104)\u001b[0m [1000]\tvalid_set's l1: 0.0126837\n",
      "\u001b[36m(_ray_fit pid=33032)\u001b[0m [1000]\tvalid_set's l1: 0.0124809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=33018)\u001b[0m \tRan out of time, early stopping on iteration 1350. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=33018)\u001b[0m \t[1350]\tvalid_set's l1: 0.0126457\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t-0.0125\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t622.57s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t77.37s\t = Validation runtime\n",
      "\u001b[36m(_ray_fit pid=33104)\u001b[0m \tRan out of time, early stopping on iteration 1402. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=33104)\u001b[0m \t[1397]\tvalid_set's l1: 0.012656\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 114.72s of the 112.39s of remaining time.\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.76% memory usage per fold, 43.03%/80.00% total).\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=10.76%)\n",
      "\u001b[36m(_ray_fit pid=41784)\u001b[0m \tRan out of time, early stopping on iteration 17. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=41784)\u001b[0m \t[17]\tvalid_set's l1: 0.0145853\n",
      "\u001b[36m(_ray_fit pid=42277)\u001b[0m \tRan out of time, early stopping on iteration 26. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=42277)\u001b[0m \t[26]\tvalid_set's l1: 0.0139088\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t-0.0138\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t80.61s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t4.8s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -73.86s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=42375)\u001b[0m \tRan out of time, early stopping on iteration 27. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=42375)\u001b[0m \t[27]\tvalid_set's l1: 0.0138824\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.625, 'LightGBMXT_BAG_L1': 0.375}\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t-0.0125\t = Validation score   (-mean_absolute_error)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t1.35s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m AutoGluon training complete, total runtime = 2842.02s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 2936.4 rows/s (804287 batch size)\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/jupyter/franco_maestria/gcs_model_dir_fullpower_hibrido_top150_v3/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=14846)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val          eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3      -0.012331  -0.012460  mean_absolute_error       93.616611     273.987359  2166.731556                 0.113917                0.093891           1.353813            3       True          6\n",
      "1    LightGBMXT_BAG_L2      -0.012357  -0.012508  mean_absolute_error       93.502695     273.893467  2165.377742                24.314471               77.366080         622.570405            2       True          4\n",
      "2    LightGBMXT_BAG_L1      -0.012369  -0.012552  mean_absolute_error       59.502794     182.625637  1340.511050                59.502794              182.625637        1340.511050            1       True          1\n",
      "3  WeightedEnsemble_L2      -0.012389  -0.012546  mean_absolute_error       69.296278     196.619559  1543.585482                 0.108054                0.092171           0.778144            2       True          3\n",
      "4      LightGBM_BAG_L1      -0.012797  -0.012910  mean_absolute_error        9.685431      13.901751   202.296288                 9.685431               13.901751         202.296288            1       True          2\n",
      "5      LightGBM_BAG_L2      -0.013696  -0.013807  mean_absolute_error       74.498469     201.322916  1623.419582                 5.310245                4.795529          80.612245            2       True          5\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t2997s\t = DyStack   runtime |\t7803s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 7803s\n",
      "AutoGluon will save models to \"/home/jupyter/franco_maestria/gcs_model_dir_fullpower_hibrido_top150_v3\"\n",
      "Train Data Rows:    7238583\n",
      "Train Data Columns: 196\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    299338.39 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7922.37 MB (2.6% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 6 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 12): ['tn_34', 'diff_tn_34', 'tn_35', 'diff_tn_35', 'tn_36', 'diff_tn_36', 'rollmean_34', 'diff_rollmean_34', 'rollmean_35', 'diff_rollmean_35', 'rollmean_36', 'diff_rollmean_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['rollmean_1', 'diff_rollmean_1']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['rollmean_1', 'diff_rollmean_1']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) :   1 | ['fecha']\n",
      "\t\t('float', [])    : 152 | ['tn_x', 'inflacion', 'cambio_dolar', 'stock_final', 'tn_1', ...]\n",
      "\t\t('int', [])      :  24 | ['customer_id', 'product_id', 'IPC', 'dias_feriados', 'sku_size', ...]\n",
      "\t\t('object', [])   :   5 | ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             :   5 | ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
      "\t\t('float', [])                : 152 | ['tn_x', 'inflacion', 'cambio_dolar', 'stock_final', 'tn_1', ...]\n",
      "\t\t('int', [])                  :  18 | ['customer_id', 'product_id', 'IPC', 'dias_feriados', 'sku_size', ...]\n",
      "\t\t('int', ['bool'])            :   6 | ['is_min_3', 'is_max_3', 'is_min_6', 'is_max_6', 'is_min_12', ...]\n",
      "\t\t('int', ['datetime_as_int']) :   1 | ['fecha']\n",
      "\t75.4s = Fit runtime\n",
      "\t182 features in original data used to generate 182 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5218.86 MB (1.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 83.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 5145.32s of the 7719.90s of remaining time.\n",
      "\tWarning: Exception caused KNeighborsUnif_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "    self._fit_single(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "    model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 109, in _fit\n",
      "    self.model = self._fit_with_samples(X=X, y=y, model_params=params, time_limit=time_limit - (time.time() - time_start))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 240, in _fit_with_samples\n",
      "    self.model = model_type(**model_params).fit(X_samp, y_samp)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_regression.py\", line 222, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_base.py\", line 478, in _fit\n",
      "    X, y = validate_data(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 5110.93s of the 7685.51s of remaining time.\n",
      "\tWarning: Exception caused KNeighborsDist_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "    self._fit_single(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "    model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 109, in _fit\n",
      "    self.model = self._fit_with_samples(X=X, y=y, model_params=params, time_limit=time_limit - (time.time() - time_start))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py\", line 240, in _fit_with_samples\n",
      "    self.model = model_type(**model_params).fit(X_samp, y_samp)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_regression.py\", line 222, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_base.py\", line 478, in _fit\n",
      "    X, y = validate_data(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5078.09s of the 7652.67s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.53% memory usage per fold, 46.13%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=11.53%)\n",
      "\t-0.0123\t = Validation score   (-mean_absolute_error)\n",
      "\t3951.51s\t = Training   runtime\n",
      "\t741.77s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1056.85s of the 3631.42s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.34% memory usage per fold, 45.38%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=11.34%)\n",
      "\t-0.0126\t = Validation score   (-mean_absolute_error)\n",
      "\t856.11s\t = Training   runtime\n",
      "\t129.68s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 155.23s of the 2729.81s of remaining time.\n",
      "\tWarning: Exception caused RandomForestMSE_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "    self._fit_single(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "    model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 219, in _fit\n",
      "    model = model.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 375, in fit\n",
      "    estimator._compute_missing_values_in_feature_mask(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 222, in _compute_missing_values_in_feature_mask\n",
      "    _assert_all_finite_element_wise(X, xp=np, allow_nan=True, **common_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 116.20s of the 2690.78s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.72% memory usage per fold, 46.89%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=11.72%)\n",
      "\t-0.0168\t = Validation score   (-mean_absolute_error)\n",
      "\t80.2s\t = Training   runtime\n",
      "\t3.2s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 6.11s of the 2580.69s of remaining time.\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -5.4s)\n",
      "\tTime limit exceeded... Skipping ExtraTreesMSE_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 514.53s of the 2497.47s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.8, 'LightGBM_BAG_L1': 0.2}\n",
      "\t-0.0123\t = Validation score   (-mean_absolute_error)\n",
      "\t1.08s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2493.67s of the 2491.17s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.74% memory usage per fold, 46.95%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=11.74%)\n",
      "\t-0.0122\t = Validation score   (-mean_absolute_error)\n",
      "\t2022.03s\t = Training   runtime\n",
      "\t259.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 405.36s of the 402.86s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.74% memory usage per fold, 46.95%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=12, gpus=0, memory=11.74%)\n",
      "\t-0.0122\t = Validation score   (-mean_absolute_error)\n",
      "\t318.15s\t = Training   runtime\n",
      "\t20.97s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 52.59s of the 50.09s of remaining time.\n",
      "\tWarning: Exception caused RandomForestMSE_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tInput X contains infinity or a value too large for dtype('float32').\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "    self._fit_single(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "    model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 219, in _fit\n",
      "    model = model.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 375, in fit\n",
      "    estimator._compute_missing_values_in_feature_mask(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 222, in _compute_missing_values_in_feature_mask\n",
      "    _assert_all_finite_element_wise(X, xp=np, allow_nan=True, **common_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -71.89s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.333, 'LightGBMXT_BAG_L2': 0.333, 'LightGBM_BAG_L2': 0.333}\n",
      "\t-0.0121\t = Validation score   (-mean_absolute_error)\n",
      "\t1.53s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 7971.12s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 783.6 rows/s (904823 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/jupyter/franco_maestria/gcs_model_dir_fullpower_hibrido_top150_v3\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_id         tn\n",
      "0       20001  36.689022\n",
      "1       20002  30.132318\n",
      "2       20003  37.661961\n",
      "3       20004  53.916893\n",
      "4       20005  69.847389\n",
      "âœ… Forecast TOP100 guardado.\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# ðŸš€ BLOQUE 3 â€” Modelo TABULAR para resto clientes\n",
    "# ====================================================\n",
    "\n",
    "# âš™ï¸ Separar Train/Test\n",
    "df_otros['fecha'] = pd.to_datetime(df_otros['fecha'])\n",
    "train_resto = df_otros[(df_otros['fecha'] <= '2019-10-01') & df_otros['clase'].notnull()].copy()\n",
    "test_resto = df_otros[df_otros['fecha'] == '2019-12-01'].copy()\n",
    "\n",
    "# Escalar magnitud de toneladas vendidas\n",
    "train_resto['sample_weight'] = train_resto['tn_x']\n",
    "\n",
    "features_resto = [col for col in df_otros.columns if col not in ['periodo', 'clase', 'tn_y','seasonal']]\n",
    "\n",
    "# âš™ï¸ Entrenar predictor\n",
    "predictor_resto = TabularPredictor(label='clase', problem_type='regression', eval_metric='mae',\n",
    "    path='gcs_model_dir_fullpower_hibrido_top150_v3')\n",
    "predictor_resto.fit(\n",
    "    train_data=train_resto[features_resto + ['clase']],\n",
    "    presets='best_quality',\n",
    "    time_limit=10800,\n",
    "    ag_args_fit={'sample_weight': 'sample_weight'}\n",
    ")\n",
    "\n",
    "# âš™ï¸ PredicciÃ³n y agregado por producto\n",
    "test_resto['tn_pred'] = predictor_resto.predict(test_resto[features_resto])\n",
    "df_resto_pred = (\n",
    "    test_resto.groupby('product_id')['tn_pred']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={'tn_pred': 'tn'})\n",
    ")\n",
    "print(df_resto_pred.head())\n",
    "\n",
    "# âš™ï¸ Guardar CSV parcial\n",
    "df_resto_pred.to_csv('forecast_resto_top150_202002_v3.csv', index=False)\n",
    "print(\"âœ… Forecast TOP100 guardado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "988bb357-00d6-4241-b5ee-424ab350fa7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_id           tn\n",
      "0       20001  1338.315522\n",
      "1       20002  1122.058318\n",
      "2       20003   768.101600\n",
      "3       20004   503.859393\n",
      "4       20005   476.275430\n",
      "âœ… Forecast combinado guardado: forecast_total_202002.csv\n",
      "Productos Ãºnicos: 780 | TN totales: 26,793.76\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# ðŸ—ƒï¸ BLOQUE 4 â€” Merge forecasts y salida final\n",
    "# ================================================\n",
    "\n",
    "df_top40_pred = pd.read_csv('forecast_top150_202002_v3.csv')\n",
    "df_resto_pred = pd.read_csv('forecast_resto_top150_202002_v3.csv')\n",
    "\n",
    "# âš™ï¸ Unir y sumar por producto\n",
    "df_final = (\n",
    "    pd.concat([df_top40_pred, df_resto_pred], axis=0)\n",
    "    .groupby('product_id', as_index=False)\n",
    "    .agg({'tn': 'sum'})\n",
    ")\n",
    "\n",
    "print(df_final.head())\n",
    "\n",
    "# âš™ï¸ Guardar archivo final\n",
    "df_final.to_csv('forecast_total_top_150_202002_v3.csv', index=False)\n",
    "print(\"âœ… Forecast combinado guardado: forecast_total_202002.csv\")\n",
    "print(f\"Productos Ãºnicos: {df_final['product_id'].nunique()} | TN totales: {df_final['tn'].sum():,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a5afb-df69-4948-99d4-90b94f8bbbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1311409e-077e-4fa2-ab00-f685458658dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ab466-eddc-41d8-9594-f51f9608c6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
