{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e28ee4e",
   "metadata": {},
   "source": [
    "# 🧩 1. Setup inicial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69c28b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Cargar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# 1.2 Paths de entrada y salida\n",
    "ruta_parquet = \"C:/Developer/Laboratorio_III/data/dataset_product_periodo_con_clusters.parquet\"\n",
    "output_dir = \"output_forecasts_by_cluster_tabular\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1.3 Archivo de log\n",
    "log_file = open(\"log_forecast_clusters.txt\", \"w\")\n",
    "def log(msg):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    line = f\"[{timestamp}] {msg}\\\\n\"\n",
    "    log_file.write(line)\n",
    "    log_file.flush()\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a1239",
   "metadata": {},
   "source": [
    "# 📥 2. Cargar dataset y clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6f42358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:13:41] Clusters únicos: [ 1  0 31  4 39  7 43 48  9 41 24 45 35 10 20 18 32 12 15 21 44 19 30 47\n",
      " 46 37 17 25  3 26 38 22  5  2 42 14 34  8 27 13 23 36 49 29 33 28 16 40\n",
      " 11  6]\\n\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Cargar dataset con clusters\n",
    "df = pd.read_parquet(ruta_parquet)\n",
    "# Normalizar fecha\n",
    "df['fecha'] = pd.to_datetime(df['fecha']).dt.normalize()\n",
    "\n",
    "# 2.2 Crear campo clase (tn_total desplazado +2)\n",
    "df['clase'] = df.groupby('product_id')['tn_total'].shift(-2)\n",
    "\n",
    "# 2.3 Revisar clusters únicos\n",
    "clusters_unicos = df['cluster_dtw'].dropna().unique()\n",
    "log(f\"Clusters únicos: {clusters_unicos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d65e04",
   "metadata": {},
   "source": [
    "# 🔄 3. Loop por cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b63b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_001345\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.65 GB / 15.31 GB (17.3%)\n",
      "Disk Space Avail:   216.76 GB / 475.95 GB (45.5%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_001345\"\n",
      "Train Data Rows:    563\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2711.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.37 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:13:45] ===> Iniciando procesamiento Cluster 1\\n\n",
      "[2025-07-06 21:13:45] Features OK para Cluster 1\\n\n",
      "[2025-07-06 21:13:45] Train shape: (563, 100) | Test shape: (17, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.34 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 450, Val Rows: 113\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.83s of the 299.83s of remaining time.\n",
      "\t-38.1916\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.81s of the 299.81s of remaining time.\n",
      "\t-38.0957\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.79s of the 299.79s of remaining time.\n",
      "\t-17.2699\t = Validation score   (-mean_absolute_error)\n",
      "\t1.51s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.26s of the 298.25s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 16.2201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-16.2058\t = Validation score   (-mean_absolute_error)\n",
      "\t3.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 295.09s of the 295.09s of remaining time.\n",
      "\t-19.144\t = Validation score   (-mean_absolute_error)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 294.41s of the 294.41s of remaining time.\n",
      "\t-18.9134\t = Validation score   (-mean_absolute_error)\n",
      "\t30.36s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 264.03s of the 264.03s of remaining time.\n",
      "\t-20.978\t = Validation score   (-mean_absolute_error)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 263.49s of the 263.49s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 263.28s of the 263.27s of remaining time.\n",
      "\t-20.7029\t = Validation score   (-mean_absolute_error)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 261.83s of the 261.83s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 261.60s of the 261.59s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 19.2999\n",
      "[2000]\tvalid_set's l1: 19.2996\n",
      "[3000]\tvalid_set's l1: 19.2996\n",
      "[4000]\tvalid_set's l1: 19.2996\n",
      "[5000]\tvalid_set's l1: 19.2996\n",
      "[6000]\tvalid_set's l1: 19.2996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-19.2996\t = Validation score   (-mean_absolute_error)\n",
      "\t47.86s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.83s of the 213.04s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.65, 'CatBoost': 0.25, 'RandomForestMSE': 0.05, 'XGBoost': 0.05}\n",
      "\t-15.3082\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 87.04s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2544.5 rows/s (113 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_001345\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_001512\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.38 GB / 15.31 GB (15.6%)\n",
      "Disk Space Avail:   216.67 GB / 475.95 GB (45.5%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_001512\"\n",
      "Train Data Rows:    748\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2437.92 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.50 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:15:12] ✅ Cluster 1: Predicciones guardadas | Productos: 17\\n\n",
      "[2025-07-06 21:15:12] ===> Cluster 1 completado en 1.46 min.\\n\n",
      "[2025-07-06 21:15:12] ===> Iniciando procesamiento Cluster 0\\n\n",
      "[2025-07-06 21:15:12] Features OK para Cluster 0\\n\n",
      "[2025-07-06 21:15:12] Train shape: (748, 100) | Test shape: (22, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.46 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 598, Val Rows: 150\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.84s of the 299.84s of remaining time.\n",
      "\t-14.2216\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.83s of remaining time.\n",
      "\t-13.8264\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.81s of the 299.81s of remaining time.\n",
      "\t-10.7186\t = Validation score   (-mean_absolute_error)\n",
      "\t2.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.41s of the 297.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 10.8393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-8.701\t = Validation score   (-mean_absolute_error)\n",
      "\t2.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 295.31s of the 295.31s of remaining time.\n",
      "\t-9.0777\t = Validation score   (-mean_absolute_error)\n",
      "\t0.76s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 294.47s of the 294.47s of remaining time.\n",
      "\t-9.193\t = Validation score   (-mean_absolute_error)\n",
      "\t111.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 183.08s of the 183.08s of remaining time.\n",
      "\t-7.3284\t = Validation score   (-mean_absolute_error)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 182.55s of the 182.55s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 182.32s of the 182.32s of remaining time.\n",
      "\t-10.5861\t = Validation score   (-mean_absolute_error)\n",
      "\t1.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 180.83s of the 180.83s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 180.59s of the 180.58s of remaining time.\n",
      "\t-10.4654\t = Validation score   (-mean_absolute_error)\n",
      "\t3.41s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.84s of the 177.06s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE': 0.5, 'CatBoost': 0.3, 'LightGBM': 0.2}\n",
      "\t-5.8777\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 123.02s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3562.1 rows/s (150 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_001512\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_001715\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.74 GB / 15.31 GB (17.9%)\n",
      "Disk Space Avail:   216.62 GB / 475.95 GB (45.5%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_001715\"\n",
      "Train Data Rows:    1054\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2800.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.70 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:17:15] ✅ Cluster 0: Predicciones guardadas | Productos: 22\\n\n",
      "[2025-07-06 21:17:15] ===> Cluster 0 completado en 2.05 min.\\n\n",
      "[2025-07-06 21:17:15] ===> Iniciando procesamiento Cluster 31\\n\n",
      "[2025-07-06 21:17:15] Features OK para Cluster 31\\n\n",
      "[2025-07-06 21:17:15] Train shape: (1054, 100) | Test shape: (31, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.64 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.19s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 843, Val Rows: 211\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.81s of the 299.81s of remaining time.\n",
      "\t-17.2235\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.79s of the 299.79s of remaining time.\n",
      "\t-15.6031\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.78s of the 299.78s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.36141\n",
      "[2000]\tvalid_set's l1: 5.11799\n",
      "[3000]\tvalid_set's l1: 5.12425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.095\t = Validation score   (-mean_absolute_error)\n",
      "\t8.9s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 290.72s of the 290.72s of remaining time.\n",
      "\t-7.2297\t = Validation score   (-mean_absolute_error)\n",
      "\t1.74s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 288.96s of the 288.96s of remaining time.\n",
      "\t-9.1619\t = Validation score   (-mean_absolute_error)\n",
      "\t1.13s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 287.75s of the 287.74s of remaining time.\n",
      "\t-8.2269\t = Validation score   (-mean_absolute_error)\n",
      "\t39.7s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 248.03s of the 248.03s of remaining time.\n",
      "\t-7.7918\t = Validation score   (-mean_absolute_error)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 247.40s of the 247.40s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 247.16s of the 247.16s of remaining time.\n",
      "\t-8.6189\t = Validation score   (-mean_absolute_error)\n",
      "\t4.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 242.92s of the 242.92s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 242.68s of the 242.68s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 10.0936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-10.0936\t = Validation score   (-mean_absolute_error)\n",
      "\t14.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.81s of the 227.96s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.917, 'ExtraTreesMSE': 0.083}\n",
      "\t-4.9571\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 72.11s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3237.6 rows/s (211 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_001715\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_001828\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.39 GB / 15.31 GB (15.6%)\n",
      "Disk Space Avail:   216.47 GB / 475.95 GB (45.5%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_001828\"\n",
      "Train Data Rows:    544\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2450.37 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:18:28] ✅ Cluster 31: Predicciones guardadas | Productos: 31\\n\n",
      "[2025-07-06 21:18:28] ===> Cluster 31 completado en 1.21 min.\\n\n",
      "[2025-07-06 21:18:28] ===> Iniciando procesamiento Cluster 4\\n\n",
      "[2025-07-06 21:18:28] Features OK para Cluster 4\\n\n",
      "[2025-07-06 21:18:28] Train shape: (544, 100) | Test shape: (16, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 435, Val Rows: 109\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.85s of the 299.84s of remaining time.\n",
      "\t-19.3413\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.82s of the 299.82s of remaining time.\n",
      "\t-18.1314\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.80s of the 299.79s of remaining time.\n",
      "\t-11.8244\t = Validation score   (-mean_absolute_error)\n",
      "\t1.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.02s of the 298.02s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 15.2604\n",
      "[2000]\tvalid_set's l1: 15.0133\n",
      "[3000]\tvalid_set's l1: 14.9573\n",
      "[4000]\tvalid_set's l1: 14.9212\n",
      "[5000]\tvalid_set's l1: 14.9185\n",
      "[6000]\tvalid_set's l1: 14.9178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-14.9171\t = Validation score   (-mean_absolute_error)\n",
      "\t15.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 282.53s of the 282.53s of remaining time.\n",
      "\t-16.2653\t = Validation score   (-mean_absolute_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 281.90s of the 281.90s of remaining time.\n",
      "\t-13.0487\t = Validation score   (-mean_absolute_error)\n",
      "\t16.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 265.55s of the 265.54s of remaining time.\n",
      "\t-14.1535\t = Validation score   (-mean_absolute_error)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 265.01s of the 265.01s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 264.79s of the 264.78s of remaining time.\n",
      "\t-17.0051\t = Validation score   (-mean_absolute_error)\n",
      "\t2.64s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 262.12s of the 262.12s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 261.87s of the 261.86s of remaining time.\n",
      "\t-16.7417\t = Validation score   (-mean_absolute_error)\n",
      "\t5.26s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.85s of the 256.45s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.579, 'CatBoost': 0.211, 'KNeighborsDist': 0.105, 'ExtraTreesMSE': 0.105}\n",
      "\t-11.0476\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 43.64s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2591.3 rows/s (109 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_001828\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_001912\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.32 GB / 15.31 GB (15.1%)\n",
      "Disk Space Avail:   216.42 GB / 475.95 GB (45.5%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_001912\"\n",
      "Train Data Rows:    748\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2374.92 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.50 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:19:12] ✅ Cluster 4: Predicciones guardadas | Productos: 16\\n\n",
      "[2025-07-06 21:19:12] ===> Cluster 4 completado en 0.73 min.\\n\n",
      "[2025-07-06 21:19:12] ===> Iniciando procesamiento Cluster 39\\n\n",
      "[2025-07-06 21:19:12] Features OK para Cluster 39\\n\n",
      "[2025-07-06 21:19:12] Train shape: (748, 100) | Test shape: (22, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.45 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 598, Val Rows: 150\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.83s of the 299.82s of remaining time.\n",
      "\t-15.9923\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.81s of the 299.80s of remaining time.\n",
      "\t-15.202\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.79s of the 299.79s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 10.4164\n",
      "[2000]\tvalid_set's l1: 10.1482\n",
      "[3000]\tvalid_set's l1: 10.1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-10.0802\t = Validation score   (-mean_absolute_error)\n",
      "\t7.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 291.98s of the 291.98s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 12.212\n",
      "[2000]\tvalid_set's l1: 12.1239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-12.119\t = Validation score   (-mean_absolute_error)\n",
      "\t6.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 285.46s of the 285.46s of remaining time.\n",
      "\t-11.7647\t = Validation score   (-mean_absolute_error)\n",
      "\t0.74s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 284.65s of the 284.64s of remaining time.\n",
      "\t-9.936\t = Validation score   (-mean_absolute_error)\n",
      "\t106.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 178.50s of the 178.49s of remaining time.\n",
      "\t-11.0776\t = Validation score   (-mean_absolute_error)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 177.90s of the 177.90s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 177.68s of the 177.68s of remaining time.\n",
      "\t-11.5088\t = Validation score   (-mean_absolute_error)\n",
      "\t1.74s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 175.92s of the 175.91s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 175.67s of the 175.67s of remaining time.\n",
      "\t-14.6348\t = Validation score   (-mean_absolute_error)\n",
      "\t5.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.83s of the 169.65s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 0.444, 'LightGBMXT': 0.278, 'XGBoost': 0.167, 'ExtraTreesMSE': 0.111}\n",
      "\t-8.9178\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 130.44s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2939.9 rows/s (150 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_001912\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_002122\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.26 GB / 15.31 GB (14.8%)\n",
      "Disk Space Avail:   216.36 GB / 475.95 GB (45.5%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_002122\"\n",
      "Train Data Rows:    612\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2317.69 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.41 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:21:22] ✅ Cluster 39: Predicciones guardadas | Productos: 22\\n\n",
      "[2025-07-06 21:21:22] ===> Cluster 39 completado en 2.18 min.\\n\n",
      "[2025-07-06 21:21:22] ===> Iniciando procesamiento Cluster 7\\n\n",
      "[2025-07-06 21:21:22] Features OK para Cluster 7\\n\n",
      "[2025-07-06 21:21:22] Train shape: (612, 100) | Test shape: (18, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.37 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 489, Val Rows: 123\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.87s of the 299.87s of remaining time.\n",
      "\t-30.3171\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.85s of the 299.85s of remaining time.\n",
      "\t-28.4522\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.82s of the 299.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 13.1816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-13.0333\t = Validation score   (-mean_absolute_error)\n",
      "\t3.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 296.36s of the 296.36s of remaining time.\n",
      "\t-18.9578\t = Validation score   (-mean_absolute_error)\n",
      "\t1.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 294.84s of the 294.84s of remaining time.\n",
      "\t-25.6773\t = Validation score   (-mean_absolute_error)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 294.14s of the 294.14s of remaining time.\n",
      "\t-18.2492\t = Validation score   (-mean_absolute_error)\n",
      "\t63.83s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 230.29s of the 230.29s of remaining time.\n",
      "\t-21.2832\t = Validation score   (-mean_absolute_error)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 229.76s of the 229.76s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 229.55s of the 229.55s of remaining time.\n",
      "\t-24.016\t = Validation score   (-mean_absolute_error)\n",
      "\t2.77s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 226.75s of the 226.75s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 226.51s of the 226.51s of remaining time.\n",
      "\t-24.4708\t = Validation score   (-mean_absolute_error)\n",
      "\t3.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.87s of the 222.81s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-13.0333\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 77.26s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 19942.8 rows/s (123 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_002122\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_002240\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.12 GB / 15.31 GB (13.9%)\n",
      "Disk Space Avail:   216.32 GB / 475.95 GB (45.5%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_002240\"\n",
      "Train Data Rows:    306\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2175.26 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.20 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:22:40] ✅ Cluster 7: Predicciones guardadas | Productos: 18\\n\n",
      "[2025-07-06 21:22:40] ===> Cluster 7 completado en 1.29 min.\\n\n",
      "[2025-07-06 21:22:40] ===> Iniciando procesamiento Cluster 43\\n\n",
      "[2025-07-06 21:22:40] Features OK para Cluster 43\\n\n",
      "[2025-07-06 21:22:40] Train shape: (306, 100) | Test shape: (9, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.2s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.18 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.26s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 244, Val Rows: 62\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.74s of the 299.74s of remaining time.\n",
      "\t-26.257\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.72s of the 299.72s of remaining time.\n",
      "\t-24.8292\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.70s of the 299.69s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 10.8892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-10.7605\t = Validation score   (-mean_absolute_error)\n",
      "\t1.93s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.72s of the 297.72s of remaining time.\n",
      "\t-12.4022\t = Validation score   (-mean_absolute_error)\n",
      "\t1.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 296.07s of the 296.06s of remaining time.\n",
      "\t-17.6873\t = Validation score   (-mean_absolute_error)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 295.47s of the 295.47s of remaining time.\n",
      "\t-17.6866\t = Validation score   (-mean_absolute_error)\n",
      "\t29.8s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 265.66s of the 265.65s of remaining time.\n",
      "\t-15.4641\t = Validation score   (-mean_absolute_error)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 265.14s of the 265.14s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 264.94s of the 264.93s of remaining time.\n",
      "\t-14.7883\t = Validation score   (-mean_absolute_error)\n",
      "\t1.6s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 263.30s of the 263.30s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 263.05s of the 263.05s of remaining time.\n",
      "\t-15.5526\t = Validation score   (-mean_absolute_error)\n",
      "\t3.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.74s of the 259.78s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.625, 'ExtraTreesMSE': 0.25, 'XGBoost': 0.125}\n",
      "\t-10.1385\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 40.29s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1582.9 rows/s (62 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_002240\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_002320\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.33 GB / 15.31 GB (15.2%)\n",
      "Disk Space Avail:   216.30 GB / 475.95 GB (45.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_002320\"\n",
      "Train Data Rows:    470\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2382.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:23:20] ✅ Cluster 43: Predicciones guardadas | Productos: 9\\n\n",
      "[2025-07-06 21:23:20] ===> Cluster 43 completado en 0.68 min.\\n\n",
      "[2025-07-06 21:23:20] ===> Iniciando procesamiento Cluster 48\\n\n",
      "[2025-07-06 21:23:20] Features OK para Cluster 48\\n\n",
      "[2025-07-06 21:23:20] Train shape: (470, 100) | Test shape: (14, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.29 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 376, Val Rows: 94\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.87s of the 299.87s of remaining time.\n",
      "\t-15.1379\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.85s of the 299.85s of remaining time.\n",
      "\t-14.4086\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.84s of the 299.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.47153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-6.4126\t = Validation score   (-mean_absolute_error)\n",
      "\t3.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 296.58s of the 296.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 7.59047\n",
      "[2000]\tvalid_set's l1: 7.45967\n",
      "[3000]\tvalid_set's l1: 7.42179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-7.4211\t = Validation score   (-mean_absolute_error)\n",
      "\t7.44s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 288.91s of the 288.91s of remaining time.\n",
      "\t-7.9583\t = Validation score   (-mean_absolute_error)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 288.28s of the 288.27s of remaining time.\n",
      "\t-9.2216\t = Validation score   (-mean_absolute_error)\n",
      "\t106.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 181.44s of the 181.43s of remaining time.\n",
      "\t-7.4541\t = Validation score   (-mean_absolute_error)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 180.88s of the 180.88s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 180.65s of the 180.65s of remaining time.\n",
      "\t-7.8686\t = Validation score   (-mean_absolute_error)\n",
      "\t2.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 178.58s of the 178.57s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 178.33s of the 178.33s of remaining time.\n",
      "\t-9.8229\t = Validation score   (-mean_absolute_error)\n",
      "\t3.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.87s of the 174.90s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.5, 'ExtraTreesMSE': 0.389, 'XGBoost': 0.111}\n",
      "\t-5.6308\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 125.18s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1709.9 rows/s (94 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_002320\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_002526\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.23 GB / 15.31 GB (14.5%)\n",
      "Disk Space Avail:   216.26 GB / 475.95 GB (45.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_002526\"\n",
      "Train Data Rows:    816\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2285.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.54 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:25:26] ✅ Cluster 48: Predicciones guardadas | Productos: 14\\n\n",
      "[2025-07-06 21:25:26] ===> Cluster 48 completado en 2.09 min.\\n\n",
      "[2025-07-06 21:25:26] ===> Iniciando procesamiento Cluster 9\\n\n",
      "[2025-07-06 21:25:26] Features OK para Cluster 9\\n\n",
      "[2025-07-06 21:25:26] Train shape: (816, 100) | Test shape: (24, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.50 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 652, Val Rows: 164\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.85s of the 299.85s of remaining time.\n",
      "\t-17.603\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.82s of remaining time.\n",
      "\t-15.9824\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.81s of the 299.81s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 9.72759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-9.6923\t = Validation score   (-mean_absolute_error)\n",
      "\t3.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 296.48s of the 296.48s of remaining time.\n",
      "\t-8.1164\t = Validation score   (-mean_absolute_error)\n",
      "\t2.4s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 294.04s of the 294.04s of remaining time.\n",
      "\t-12.944\t = Validation score   (-mean_absolute_error)\n",
      "\t0.9s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 293.08s of the 293.07s of remaining time.\n",
      "\t-8.4463\t = Validation score   (-mean_absolute_error)\n",
      "\t103.41s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 189.63s of the 189.63s of remaining time.\n",
      "\t-10.6336\t = Validation score   (-mean_absolute_error)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 189.05s of the 189.05s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 188.83s of the 188.82s of remaining time.\n",
      "\t-11.6368\t = Validation score   (-mean_absolute_error)\n",
      "\t3.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 185.27s of the 185.27s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 185.02s of the 185.02s of remaining time.\n",
      "\t-14.2785\t = Validation score   (-mean_absolute_error)\n",
      "\t5.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.85s of the 179.66s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.458, 'CatBoost': 0.333, 'KNeighborsDist': 0.083, 'XGBoost': 0.083, 'ExtraTreesMSE': 0.042}\n",
      "\t-6.9086\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 120.43s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3673.5 rows/s (164 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_002526\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_002727\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.45 GB / 15.31 GB (16.0%)\n",
      "Disk Space Avail:   216.20 GB / 475.95 GB (45.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_002727\"\n",
      "Train Data Rows:    238\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2493.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:27:27] ✅ Cluster 9: Predicciones guardadas | Productos: 24\\n\n",
      "[2025-07-06 21:27:27] ===> Cluster 9 completado en 2.01 min.\\n\n",
      "[2025-07-06 21:27:27] ===> Iniciando procesamiento Cluster 41\\n\n",
      "[2025-07-06 21:27:27] Features OK para Cluster 41\\n\n",
      "[2025-07-06 21:27:27] Train shape: (238, 100) | Test shape: (7, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.2s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.14 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.24s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 190, Val Rows: 48\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.76s of the 299.75s of remaining time.\n",
      "\t-9.6585\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.73s of the 299.73s of remaining time.\n",
      "\t-9.0568\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.72s of the 299.71s of remaining time.\n",
      "\t-8.7243\t = Validation score   (-mean_absolute_error)\n",
      "\t1.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.19s of the 298.19s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 8.83743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-8.6635\t = Validation score   (-mean_absolute_error)\n",
      "\t2.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 295.23s of the 295.23s of remaining time.\n",
      "\t-8.299\t = Validation score   (-mean_absolute_error)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 294.65s of the 294.64s of remaining time.\n",
      "\t-10.3708\t = Validation score   (-mean_absolute_error)\n",
      "\t32.48s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 262.15s of the 262.14s of remaining time.\n",
      "\t-7.5515\t = Validation score   (-mean_absolute_error)\n",
      "\t0.41s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 261.69s of the 261.69s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 261.48s of the 261.47s of remaining time.\n",
      "\t-9.0787\t = Validation score   (-mean_absolute_error)\n",
      "\t1.17s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 260.28s of the 260.28s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 260.05s of the 260.04s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 9.93769\n",
      "[2000]\tvalid_set's l1: 9.93625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-9.9362\t = Validation score   (-mean_absolute_error)\n",
      "\t8.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.76s of the 251.62s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE': 0.773, 'XGBoost': 0.136, 'LightGBM': 0.091}\n",
      "\t-7.4135\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 48.46s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1168.2 rows/s (48 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_002727\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_002815\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.51 GB / 15.31 GB (16.4%)\n",
      "Disk Space Avail:   216.17 GB / 475.95 GB (45.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_002815\"\n",
      "Train Data Rows:    272\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2573.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.18 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:28:15] ✅ Cluster 41: Predicciones guardadas | Productos: 7\\n\n",
      "[2025-07-06 21:28:15] ===> Cluster 41 completado en 0.81 min.\\n\n",
      "[2025-07-06 21:28:15] ===> Iniciando procesamiento Cluster 24\\n\n",
      "[2025-07-06 21:28:15] Features OK para Cluster 24\\n\n",
      "[2025-07-06 21:28:15] Train shape: (272, 100) | Test shape: (8, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['cat3_factorized', 'descripcion_factorized', 'mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 3 | ['cat3_factorized', 'descripcion_factorized', 'mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  6 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'brand_factorized', 'quarter_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  6 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'brand_factorized', 'quarter_factorized', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t78 features in original data used to generate 78 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.26s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 217, Val Rows: 55\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.74s of the 299.74s of remaining time.\n",
      "\t-11.4934\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.72s of the 299.72s of remaining time.\n",
      "\t-11.1328\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.71s of the 299.70s of remaining time.\n",
      "\t-5.2681\t = Validation score   (-mean_absolute_error)\n",
      "\t1.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.39s of the 298.39s of remaining time.\n",
      "\t-7.2603\t = Validation score   (-mean_absolute_error)\n",
      "\t1.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 297.18s of the 297.18s of remaining time.\n",
      "\t-8.0441\t = Validation score   (-mean_absolute_error)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 296.65s of the 296.65s of remaining time.\n",
      "\t-8.1004\t = Validation score   (-mean_absolute_error)\n",
      "\t4.69s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 291.95s of the 291.95s of remaining time.\n",
      "\t-6.7693\t = Validation score   (-mean_absolute_error)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 291.47s of the 291.47s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 291.26s of the 291.26s of remaining time.\n",
      "\t-7.5383\t = Validation score   (-mean_absolute_error)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 290.10s of the 290.10s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 289.85s of the 289.85s of remaining time.\n",
      "\t-8.9613\t = Validation score   (-mean_absolute_error)\n",
      "\t2.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.74s of the 287.60s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.667, 'ExtraTreesMSE': 0.167, 'KNeighborsDist': 0.111, 'XGBoost': 0.056}\n",
      "\t-4.7333\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 12.47s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1342.5 rows/s (55 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_002815\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_002828\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.51 GB / 15.31 GB (16.4%)\n",
      "Disk Space Avail:   216.16 GB / 475.95 GB (45.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_002828\"\n",
      "Train Data Rows:    306\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2568.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.20 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:28:28] ✅ Cluster 24: Predicciones guardadas | Productos: 8\\n\n",
      "[2025-07-06 21:28:28] ===> Cluster 24 completado en 0.21 min.\\n\n",
      "[2025-07-06 21:28:28] ===> Iniciando procesamiento Cluster 45\\n\n",
      "[2025-07-06 21:28:28] Features OK para Cluster 45\\n\n",
      "[2025-07-06 21:28:28] Train shape: (306, 100) | Test shape: (9, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.19 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.29s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 244, Val Rows: 62\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.71s of the 299.71s of remaining time.\n",
      "\t-15.9913\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.70s of the 299.69s of remaining time.\n",
      "\t-15.4633\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.68s of the 299.68s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.75441\n",
      "[2000]\tvalid_set's l1: 6.5376\n",
      "[3000]\tvalid_set's l1: 6.38888\n",
      "[4000]\tvalid_set's l1: 6.34861\n",
      "[5000]\tvalid_set's l1: 6.33783\n",
      "[6000]\tvalid_set's l1: 6.33382\n",
      "[7000]\tvalid_set's l1: 6.3331\n",
      "[8000]\tvalid_set's l1: 6.33312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-6.3331\t = Validation score   (-mean_absolute_error)\n",
      "\t13.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 286.13s of the 286.12s of remaining time.\n",
      "\t-8.1818\t = Validation score   (-mean_absolute_error)\n",
      "\t2.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 283.61s of the 283.61s of remaining time.\n",
      "\t-10.2799\t = Validation score   (-mean_absolute_error)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 282.90s of the 282.89s of remaining time.\n",
      "\t-10.4736\t = Validation score   (-mean_absolute_error)\n",
      "\t95.96s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 186.90s of the 186.90s of remaining time.\n",
      "\t-8.3483\t = Validation score   (-mean_absolute_error)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 186.34s of the 186.34s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 186.10s of the 186.10s of remaining time.\n",
      "\t-10.9304\t = Validation score   (-mean_absolute_error)\n",
      "\t1.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 184.76s of the 184.75s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 184.49s of the 184.49s of remaining time.\n",
      "\t-7.2492\t = Validation score   (-mean_absolute_error)\n",
      "\t3.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.71s of the 180.62s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-6.3331\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 119.47s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 7077.5 rows/s (62 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_002828\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_003028\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.57 GB / 15.31 GB (16.8%)\n",
      "Disk Space Avail:   216.12 GB / 475.95 GB (45.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003028\"\n",
      "Train Data Rows:    476\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2630.56 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:30:28] ✅ Cluster 45: Predicciones guardadas | Productos: 9\\n\n",
      "[2025-07-06 21:30:28] ===> Cluster 45 completado en 1.99 min.\\n\n",
      "[2025-07-06 21:30:28] ===> Iniciando procesamiento Cluster 35\\n\n",
      "[2025-07-06 21:30:28] Features OK para Cluster 35\\n\n",
      "[2025-07-06 21:30:28] Train shape: (476, 100) | Test shape: (14, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.29 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 380, Val Rows: 96\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.50s of the 299.50s of remaining time.\n",
      "\t-18.3008\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.48s of the 299.48s of remaining time.\n",
      "\t-17.8136\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.46s of the 299.46s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.8311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.8124\t = Validation score   (-mean_absolute_error)\n",
      "\t2.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 296.44s of the 296.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 7.70368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-7.6988\t = Validation score   (-mean_absolute_error)\n",
      "\t2.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 294.05s of the 294.04s of remaining time.\n",
      "\t-11.3647\t = Validation score   (-mean_absolute_error)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 293.43s of the 293.43s of remaining time.\n",
      "\t-9.6678\t = Validation score   (-mean_absolute_error)\n",
      "\t33.87s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 259.53s of the 259.53s of remaining time.\n",
      "\t-10.2091\t = Validation score   (-mean_absolute_error)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 258.98s of the 258.98s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 258.75s of the 258.74s of remaining time.\n",
      "\t-9.9298\t = Validation score   (-mean_absolute_error)\n",
      "\t2.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 256.58s of the 256.58s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 256.33s of the 256.33s of remaining time.\n",
      "\t-10.2863\t = Validation score   (-mean_absolute_error)\n",
      "\t3.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.50s of the 252.64s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.96, 'XGBoost': 0.04}\n",
      "\t-5.7967\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 47.43s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 8711.9 rows/s (96 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003028\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_003116\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.58 GB / 15.31 GB (16.8%)\n",
      "Disk Space Avail:   216.09 GB / 475.95 GB (45.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003116\"\n",
      "Train Data Rows:    291\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2636.90 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.19 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:31:15] ✅ Cluster 35: Predicciones guardadas | Productos: 14\\n\n",
      "[2025-07-06 21:31:15] ===> Cluster 35 completado en 0.79 min.\\n\n",
      "[2025-07-06 21:31:15] ===> Iniciando procesamiento Cluster 10\\n\n",
      "[2025-07-06 21:31:16] Features OK para Cluster 10\\n\n",
      "[2025-07-06 21:31:16] Train shape: (291, 100) | Test shape: (34, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.18 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.18s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 232, Val Rows: 59\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.82s of the 299.82s of remaining time.\n",
      "\t-9.3786\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.80s of the 299.80s of remaining time.\n",
      "\t-9.111\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.77s of the 299.77s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 22.3907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-22.1826\t = Validation score   (-mean_absolute_error)\n",
      "\t1.92s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.81s of the 297.81s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 17.9097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-17.8936\t = Validation score   (-mean_absolute_error)\n",
      "\t2.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 295.64s of the 295.64s of remaining time.\n",
      "\t-8.655\t = Validation score   (-mean_absolute_error)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 295.10s of the 295.10s of remaining time.\n",
      "\t-8.8983\t = Validation score   (-mean_absolute_error)\n",
      "\t2.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 292.28s of the 292.28s of remaining time.\n",
      "\t-8.4721\t = Validation score   (-mean_absolute_error)\n",
      "\t0.44s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 291.79s of the 291.79s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 291.57s of the 291.57s of remaining time.\n",
      "\t-9.3325\t = Validation score   (-mean_absolute_error)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 290.84s of the 290.84s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 290.57s of the 290.57s of remaining time.\n",
      "\t-9.5778\t = Validation score   (-mean_absolute_error)\n",
      "\t3.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.82s of the 287.37s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 0.48, 'RandomForestMSE': 0.32, 'KNeighborsDist': 0.16, 'ExtraTreesMSE': 0.04}\n",
      "\t-6.9097\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 12.72s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 861.4 rows/s (59 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003116\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_003128\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.66 GB / 15.31 GB (17.4%)\n",
      "Disk Space Avail:   216.07 GB / 475.95 GB (45.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003128\"\n",
      "Train Data Rows:    578\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2727.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.38 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:31:28] ✅ Cluster 10: Predicciones guardadas | Productos: 34\\n\n",
      "[2025-07-06 21:31:28] ===> Cluster 10 completado en 0.22 min.\\n\n",
      "[2025-07-06 21:31:28] ===> Iniciando procesamiento Cluster 20\\n\n",
      "[2025-07-06 21:31:28] Features OK para Cluster 20\\n\n",
      "[2025-07-06 21:31:28] Train shape: (578, 100) | Test shape: (17, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.35 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 462, Val Rows: 116\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.86s of the 299.86s of remaining time.\n",
      "\t-11.2361\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.84s of the 299.84s of remaining time.\n",
      "\t-10.8881\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.83s of the 299.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.86566\n",
      "[2000]\tvalid_set's l1: 5.80133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.7875\t = Validation score   (-mean_absolute_error)\n",
      "\t4.91s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 294.80s of the 294.79s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.63994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-6.6286\t = Validation score   (-mean_absolute_error)\n",
      "\t3.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 291.31s of the 291.31s of remaining time.\n",
      "\t-7.9673\t = Validation score   (-mean_absolute_error)\n",
      "\t0.67s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 290.58s of the 290.58s of remaining time.\n",
      "\t-6.876\t = Validation score   (-mean_absolute_error)\n",
      "\t13.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 277.45s of the 277.45s of remaining time.\n",
      "\t-7.5649\t = Validation score   (-mean_absolute_error)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 276.85s of the 276.85s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 276.62s of the 276.62s of remaining time.\n",
      "\t-5.9111\t = Validation score   (-mean_absolute_error)\n",
      "\t2.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 274.53s of the 274.53s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 274.28s of the 274.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.49479\n",
      "[2000]\tvalid_set's l1: 5.49429\n",
      "[3000]\tvalid_set's l1: 5.49428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.4943\t = Validation score   (-mean_absolute_error)\n",
      "\t24.6s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.86s of the 249.17s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMLarge': 0.545, 'XGBoost': 0.455}\n",
      "\t-5.4292\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 50.91s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4461.9 rows/s (116 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003128\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_003220\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.69 GB / 15.31 GB (17.6%)\n",
      "Disk Space Avail:   216.01 GB / 475.95 GB (45.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003220\"\n",
      "Train Data Rows:    850\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2754.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.56 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:32:20] ✅ Cluster 20: Predicciones guardadas | Productos: 17\\n\n",
      "[2025-07-06 21:32:20] ===> Cluster 20 completado en 0.85 min.\\n\n",
      "[2025-07-06 21:32:20] ===> Iniciando procesamiento Cluster 18\\n\n",
      "[2025-07-06 21:32:20] Features OK para Cluster 18\\n\n",
      "[2025-07-06 21:32:20] Train shape: (850, 100) | Test shape: (25, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.52 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 680, Val Rows: 170\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.84s of the 299.84s of remaining time.\n",
      "\t-5.0153\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.81s of the 299.81s of remaining time.\n",
      "\t-4.9395\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.78s of the 299.78s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 2.33363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-2.3232\t = Validation score   (-mean_absolute_error)\n",
      "\t2.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 296.93s of the 296.93s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 2.40628\n",
      "[2000]\tvalid_set's l1: 2.37989\n",
      "[3000]\tvalid_set's l1: 2.37948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-2.3771\t = Validation score   (-mean_absolute_error)\n",
      "\t9.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 287.05s of the 287.05s of remaining time.\n",
      "\t-3.1724\t = Validation score   (-mean_absolute_error)\n",
      "\t0.91s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 286.06s of the 286.06s of remaining time.\n",
      "\t-2.3953\t = Validation score   (-mean_absolute_error)\n",
      "\t84.42s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 201.61s of the 201.61s of remaining time.\n",
      "\t-2.7769\t = Validation score   (-mean_absolute_error)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 201.06s of the 201.06s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 200.81s of the 200.81s of remaining time.\n",
      "\t-2.534\t = Validation score   (-mean_absolute_error)\n",
      "\t2.94s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 197.84s of the 197.84s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 197.59s of the 197.59s of remaining time.\n",
      "\t-2.6839\t = Validation score   (-mean_absolute_error)\n",
      "\t6.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.84s of the 191.08s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.444, 'ExtraTreesMSE': 0.222, 'CatBoost': 0.167, 'XGBoost': 0.167}\n",
      "\t-1.8908\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 109.01s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3684.0 rows/s (170 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003220\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_003409\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       1.89 GB / 15.31 GB (12.3%)\n",
      "Disk Space Avail:   215.87 GB / 475.95 GB (45.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003409\"\n",
      "Train Data Rows:    136\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1930.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.09 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:34:09] ✅ Cluster 18: Predicciones guardadas | Productos: 25\\n\n",
      "[2025-07-06 21:34:09] ===> Cluster 18 completado en 1.83 min.\\n\n",
      "[2025-07-06 21:34:09] ===> Iniciando procesamiento Cluster 32\\n\n",
      "[2025-07-06 21:34:09] Features OK para Cluster 32\\n\n",
      "[2025-07-06 21:34:09] Train shape: (136, 100) | Test shape: (4, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['cat2_factorized', 'descripcion_factorized', 'mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 3 | ['cat2_factorized', 'descripcion_factorized', 'mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  6 | ['periodo_factorized', 'cat1_factorized', 'cat3_factorized', 'brand_factorized', 'quarter_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  5 | ['periodo_factorized', 'cat3_factorized', 'brand_factorized', 'quarter_factorized', 'product_id']\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.3s = Fit runtime\n",
      "\t78 features in original data used to generate 78 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.08 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.37s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 108, Val Rows: 28\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.63s of the 299.62s of remaining time.\n",
      "\t-14.4306\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.60s of the 299.59s of remaining time.\n",
      "\t-13.6246\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.57s of the 299.57s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 7.47832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-7.4423\t = Validation score   (-mean_absolute_error)\n",
      "\t2.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.08s of the 297.07s of remaining time.\n",
      "\t-8.9108\t = Validation score   (-mean_absolute_error)\n",
      "\t1.5s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 295.54s of the 295.53s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 9.08597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-8.8498\t = Validation score   (-mean_absolute_error)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 294.98s of the 294.98s of remaining time.\n",
      "\t-9.6031\t = Validation score   (-mean_absolute_error)\n",
      "\t2.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 292.73s of the 292.73s of remaining time.\n",
      "\t-7.0573\t = Validation score   (-mean_absolute_error)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 292.19s of the 292.19s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 291.95s of the 291.95s of remaining time.\n",
      "\t-7.7283\t = Validation score   (-mean_absolute_error)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 291.30s of the 291.29s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 291.05s of the 291.05s of remaining time.\n",
      "\t-9.1933\t = Validation score   (-mean_absolute_error)\n",
      "\t1.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.63s of the 289.84s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.385, 'CatBoost': 0.385, 'ExtraTreesMSE': 0.231}\n",
      "\t-5.0747\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10.24s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 536.9 rows/s (28 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003409\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_003420\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       1.82 GB / 15.31 GB (11.9%)\n",
      "Disk Space Avail:   215.94 GB / 475.95 GB (45.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003420\"\n",
      "Train Data Rows:    736\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1867.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.49 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:34:20] ✅ Cluster 32: Predicciones guardadas | Productos: 4\\n\n",
      "[2025-07-06 21:34:20] ===> Cluster 32 completado en 0.18 min.\\n\n",
      "[2025-07-06 21:34:20] ===> Iniciando procesamiento Cluster 12\\n\n",
      "[2025-07-06 21:34:20] Features OK para Cluster 12\\n\n",
      "[2025-07-06 21:34:20] Train shape: (736, 100) | Test shape: (24, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.45 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 588, Val Rows: 148\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.84s of the 299.84s of remaining time.\n",
      "\t-3.3822\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.82s of the 299.82s of remaining time.\n",
      "\t-3.3476\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.80s of the 299.80s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 2.23442\n",
      "[2000]\tvalid_set's l1: 2.19641\n",
      "[3000]\tvalid_set's l1: 2.18534\n",
      "[4000]\tvalid_set's l1: 2.1806\n",
      "[5000]\tvalid_set's l1: 2.17869\n",
      "[6000]\tvalid_set's l1: 2.17739\n",
      "[7000]\tvalid_set's l1: 2.17708\n",
      "[8000]\tvalid_set's l1: 2.17805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-2.1768\t = Validation score   (-mean_absolute_error)\n",
      "\t19.0s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 280.32s of the 280.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 2.41831\n",
      "[2000]\tvalid_set's l1: 2.40037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-2.3988\t = Validation score   (-mean_absolute_error)\n",
      "\t5.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 274.62s of the 274.62s of remaining time.\n",
      "\t-2.3866\t = Validation score   (-mean_absolute_error)\n",
      "\t0.82s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 273.72s of the 273.72s of remaining time.\n",
      "\t-2.1636\t = Validation score   (-mean_absolute_error)\n",
      "\t100.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 173.56s of the 173.55s of remaining time.\n",
      "\t-2.225\t = Validation score   (-mean_absolute_error)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 173.03s of the 173.03s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 172.81s of the 172.81s of remaining time.\n",
      "\t-2.9693\t = Validation score   (-mean_absolute_error)\n",
      "\t2.73s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 170.05s of the 170.04s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 169.80s of the 169.80s of remaining time.\n",
      "\t-2.665\t = Validation score   (-mean_absolute_error)\n",
      "\t5.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.84s of the 164.24s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE': 0.458, 'LightGBMXT': 0.375, 'CatBoost': 0.167}\n",
      "\t-1.8444\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 135.84s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2450.3 rows/s (148 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003420\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_003636\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.53 GB / 15.31 GB (16.5%)\n",
      "Disk Space Avail:   215.95 GB / 475.95 GB (45.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003636\"\n",
      "Train Data Rows:    272\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2593.43 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.18 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:36:36] ✅ Cluster 12: Predicciones guardadas | Productos: 24\\n\n",
      "[2025-07-06 21:36:36] ===> Cluster 12 completado en 2.27 min.\\n\n",
      "[2025-07-06 21:36:36] ===> Iniciando procesamiento Cluster 15\\n\n",
      "[2025-07-06 21:36:36] Features OK para Cluster 15\\n\n",
      "[2025-07-06 21:36:36] Train shape: (272, 100) | Test shape: (8, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['brand_factorized', 'mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 2 | ['brand_factorized', 'mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  7 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'descripcion_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  6 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'descripcion_factorized', 'quarter_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.2s = Fit runtime\n",
      "\t79 features in original data used to generate 79 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.26s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 217, Val Rows: 55\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.74s of the 299.74s of remaining time.\n",
      "\t-6.3753\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.71s of the 299.71s of remaining time.\n",
      "\t-6.3386\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.69s of the 299.69s of remaining time.\n",
      "\t-2.8937\t = Validation score   (-mean_absolute_error)\n",
      "\t1.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.10s of the 298.10s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 3.19635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-3.1843\t = Validation score   (-mean_absolute_error)\n",
      "\t2.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 295.41s of the 295.41s of remaining time.\n",
      "\t-3.3639\t = Validation score   (-mean_absolute_error)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 294.88s of the 294.88s of remaining time.\n",
      "\t-3.8321\t = Validation score   (-mean_absolute_error)\n",
      "\t3.82s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 291.05s of the 291.05s of remaining time.\n",
      "\t-3.1984\t = Validation score   (-mean_absolute_error)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 290.58s of the 290.58s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 290.36s of the 290.36s of remaining time.\n",
      "\t-2.7992\t = Validation score   (-mean_absolute_error)\n",
      "\t1.36s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 288.98s of the 288.97s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 288.73s of the 288.73s of remaining time.\n",
      "\t-2.9285\t = Validation score   (-mean_absolute_error)\n",
      "\t3.94s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.74s of the 284.61s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost': 0.545, 'LightGBMXT': 0.455}\n",
      "\t-2.4664\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 15.48s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6311.2 rows/s (55 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003636\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_003652\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.47 GB / 15.31 GB (16.1%)\n",
      "Disk Space Avail:   215.93 GB / 475.95 GB (45.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003652\"\n",
      "Train Data Rows:    578\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2525.10 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.38 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:36:52] ✅ Cluster 15: Predicciones guardadas | Productos: 8\\n\n",
      "[2025-07-06 21:36:52] ===> Cluster 15 completado en 0.26 min.\\n\n",
      "[2025-07-06 21:36:52] ===> Iniciando procesamiento Cluster 21\\n\n",
      "[2025-07-06 21:36:52] Features OK para Cluster 21\\n\n",
      "[2025-07-06 21:36:52] Train shape: (578, 100) | Test shape: (17, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.35 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 462, Val Rows: 116\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.86s of the 299.86s of remaining time.\n",
      "\t-8.3422\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.83s of remaining time.\n",
      "\t-7.2457\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.81s of the 299.81s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 3.44778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-3.4412\t = Validation score   (-mean_absolute_error)\n",
      "\t3.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 296.60s of the 296.60s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.3607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.344\t = Validation score   (-mean_absolute_error)\n",
      "\t2.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 293.88s of the 293.87s of remaining time.\n",
      "\t-5.6015\t = Validation score   (-mean_absolute_error)\n",
      "\t0.68s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 293.13s of the 293.13s of remaining time.\n",
      "\t-4.6897\t = Validation score   (-mean_absolute_error)\n",
      "\t18.15s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 274.97s of the 274.96s of remaining time.\n",
      "\t-4.7536\t = Validation score   (-mean_absolute_error)\n",
      "\t0.44s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 274.47s of the 274.46s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 274.25s of the 274.24s of remaining time.\n",
      "\t-4.7852\t = Validation score   (-mean_absolute_error)\n",
      "\t1.98s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 272.24s of the 272.24s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 271.99s of the 271.99s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.94804\n",
      "[2000]\tvalid_set's l1: 4.94798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.948\t = Validation score   (-mean_absolute_error)\n",
      "\t21.58s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.86s of the 249.86s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.944, 'XGBoost': 0.056}\n",
      "\t-3.4377\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 50.22s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 14529.6 rows/s (116 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003652\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_003742\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.63 GB / 15.31 GB (17.2%)\n",
      "Disk Space Avail:   215.87 GB / 475.95 GB (45.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003742\"\n",
      "Train Data Rows:    510\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2696.26 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:37:42] ✅ Cluster 21: Predicciones guardadas | Productos: 17\\n\n",
      "[2025-07-06 21:37:42] ===> Cluster 21 completado en 0.84 min.\\n\n",
      "[2025-07-06 21:37:42] ===> Iniciando procesamiento Cluster 44\\n\n",
      "[2025-07-06 21:37:42] Features OK para Cluster 44\\n\n",
      "[2025-07-06 21:37:42] Train shape: (510, 100) | Test shape: (15, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.31 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 408, Val Rows: 102\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.85s of the 299.84s of remaining time.\n",
      "\t-5.7171\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.83s of remaining time.\n",
      "\t-5.4704\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.81s of the 299.81s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 3.00314\n",
      "[2000]\tvalid_set's l1: 2.96747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-2.9661\t = Validation score   (-mean_absolute_error)\n",
      "\t4.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 295.62s of the 295.61s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 3.65594\n",
      "[2000]\tvalid_set's l1: 3.56388\n",
      "[3000]\tvalid_set's l1: 3.56274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-3.5611\t = Validation score   (-mean_absolute_error)\n",
      "\t6.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 288.47s of the 288.46s of remaining time.\n",
      "\t-4.3591\t = Validation score   (-mean_absolute_error)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 287.80s of the 287.80s of remaining time.\n",
      "\t-3.2693\t = Validation score   (-mean_absolute_error)\n",
      "\t106.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 181.70s of the 181.69s of remaining time.\n",
      "\t-3.951\t = Validation score   (-mean_absolute_error)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 181.18s of the 181.18s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 180.95s of the 180.94s of remaining time.\n",
      "\t-3.6311\t = Validation score   (-mean_absolute_error)\n",
      "\t2.36s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 178.55s of the 178.55s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 178.30s of the 178.30s of remaining time.\n",
      "\t-3.8365\t = Validation score   (-mean_absolute_error)\n",
      "\t5.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.85s of the 172.72s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.68, 'CatBoost': 0.24, 'KNeighborsDist': 0.04, 'XGBoost': 0.04}\n",
      "\t-2.8634\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 127.38s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 7477.3 rows/s (102 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003742\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_003950\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.69 GB / 15.31 GB (17.6%)\n",
      "Disk Space Avail:   215.82 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003950\"\n",
      "Train Data Rows:    374\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2755.66 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.25 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:39:50] ✅ Cluster 44: Predicciones guardadas | Productos: 15\\n\n",
      "[2025-07-06 21:39:50] ===> Cluster 44 completado en 2.13 min.\\n\n",
      "[2025-07-06 21:39:50] ===> Iniciando procesamiento Cluster 19\\n\n",
      "[2025-07-06 21:39:50] Features OK para Cluster 19\\n\n",
      "[2025-07-06 21:39:50] Train shape: (374, 100) | Test shape: (11, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.3s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.23 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.31s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 299, Val Rows: 75\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.69s of the 299.69s of remaining time.\n",
      "\t-13.1722\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.67s of the 299.67s of remaining time.\n",
      "\t-12.1527\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.66s of the 299.65s of remaining time.\n",
      "\t-5.369\t = Validation score   (-mean_absolute_error)\n",
      "\t1.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.03s of the 298.03s of remaining time.\n",
      "\t-5.9978\t = Validation score   (-mean_absolute_error)\n",
      "\t1.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 296.78s of the 296.78s of remaining time.\n",
      "\t-8.0245\t = Validation score   (-mean_absolute_error)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 296.17s of the 296.17s of remaining time.\n",
      "\t-6.561\t = Validation score   (-mean_absolute_error)\n",
      "\t101.93s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 194.21s of the 194.21s of remaining time.\n",
      "\t-8.8175\t = Validation score   (-mean_absolute_error)\n",
      "\t0.44s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 193.70s of the 193.70s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 193.47s of the 193.47s of remaining time.\n",
      "\t-9.2552\t = Validation score   (-mean_absolute_error)\n",
      "\t1.62s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 191.83s of the 191.83s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 191.59s of the 191.58s of remaining time.\n",
      "\t-8.4925\t = Validation score   (-mean_absolute_error)\n",
      "\t2.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.69s of the 188.48s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.7, 'CatBoost': 0.25, 'XGBoost': 0.05}\n",
      "\t-5.1666\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 111.61s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6147.4 rows/s (75 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_003950\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_004142\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.68 GB / 15.31 GB (17.5%)\n",
      "Disk Space Avail:   215.79 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004142\"\n",
      "Train Data Rows:    374\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2738.75 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.25 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:41:42] ✅ Cluster 19: Predicciones guardadas | Productos: 11\\n\n",
      "[2025-07-06 21:41:42] ===> Cluster 19 completado en 1.86 min.\\n\n",
      "[2025-07-06 21:41:42] ===> Iniciando procesamiento Cluster 30\\n\n",
      "[2025-07-06 21:41:42] Features OK para Cluster 30\\n\n",
      "[2025-07-06 21:41:42] Train shape: (374, 100) | Test shape: (11, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.23 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.31s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 299, Val Rows: 75\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.69s of the 299.69s of remaining time.\n",
      "\t-5.9665\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.67s of the 299.67s of remaining time.\n",
      "\t-5.6218\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.65s of the 299.65s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.1352\n",
      "[2000]\tvalid_set's l1: 4.10212\n",
      "[3000]\tvalid_set's l1: 4.09295\n",
      "[4000]\tvalid_set's l1: 4.0901\n",
      "[5000]\tvalid_set's l1: 4.08996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.0893\t = Validation score   (-mean_absolute_error)\n",
      "\t9.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 289.56s of the 289.55s of remaining time.\n",
      "\t-4.3449\t = Validation score   (-mean_absolute_error)\n",
      "\t1.38s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 288.15s of the 288.15s of remaining time.\n",
      "\t-4.1441\t = Validation score   (-mean_absolute_error)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 287.55s of the 287.55s of remaining time.\n",
      "\t-3.9725\t = Validation score   (-mean_absolute_error)\n",
      "\t3.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 284.28s of the 284.28s of remaining time.\n",
      "\t-3.9303\t = Validation score   (-mean_absolute_error)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 283.75s of the 283.74s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 283.51s of the 283.51s of remaining time.\n",
      "\t-3.5973\t = Validation score   (-mean_absolute_error)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 282.25s of the 282.25s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 282.01s of the 282.00s of remaining time.\n",
      "\t-4.1772\t = Validation score   (-mean_absolute_error)\n",
      "\t2.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.69s of the 279.63s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost': 0.4, 'ExtraTreesMSE': 0.333, 'LightGBMXT': 0.267}\n",
      "\t-3.4086\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 20.45s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1310.4 rows/s (75 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004142\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_004202\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.65 GB / 15.31 GB (17.3%)\n",
      "Disk Space Avail:   215.77 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004202\"\n",
      "Train Data Rows:    374\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2711.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.25 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:42:02] ✅ Cluster 30: Predicciones guardadas | Productos: 11\\n\n",
      "[2025-07-06 21:42:02] ===> Cluster 30 completado en 0.35 min.\\n\n",
      "[2025-07-06 21:42:02] ===> Iniciando procesamiento Cluster 47\\n\n",
      "[2025-07-06 21:42:02] Features OK para Cluster 47\\n\n",
      "[2025-07-06 21:42:02] Train shape: (374, 100) | Test shape: (11, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.23 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.31s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 299, Val Rows: 75\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.69s of the 299.69s of remaining time.\n",
      "\t-8.9796\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.67s of the 299.67s of remaining time.\n",
      "\t-8.7214\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.65s of the 299.65s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.815\n",
      "[2000]\tvalid_set's l1: 4.72807\n",
      "[3000]\tvalid_set's l1: 4.70139\n",
      "[4000]\tvalid_set's l1: 4.69071\n",
      "[5000]\tvalid_set's l1: 4.68457\n",
      "[6000]\tvalid_set's l1: 4.67898\n",
      "[7000]\tvalid_set's l1: 4.67567\n",
      "[8000]\tvalid_set's l1: 4.6764\n",
      "[9000]\tvalid_set's l1: 4.67752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.6749\t = Validation score   (-mean_absolute_error)\n",
      "\t14.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 284.25s of the 284.25s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.27497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.2685\t = Validation score   (-mean_absolute_error)\n",
      "\t2.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 281.60s of the 281.60s of remaining time.\n",
      "\t-5.9337\t = Validation score   (-mean_absolute_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 280.97s of the 280.97s of remaining time.\n",
      "\t-5.3765\t = Validation score   (-mean_absolute_error)\n",
      "\t24.88s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 256.08s of the 256.08s of remaining time.\n",
      "\t-5.5199\t = Validation score   (-mean_absolute_error)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 255.56s of the 255.56s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 255.33s of the 255.33s of remaining time.\n",
      "\t-5.4314\t = Validation score   (-mean_absolute_error)\n",
      "\t1.54s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 253.77s of the 253.77s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 253.49s of the 253.48s of remaining time.\n",
      "\t-5.562\t = Validation score   (-mean_absolute_error)\n",
      "\t2.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.69s of the 250.36s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.545, 'XGBoost': 0.318, 'CatBoost': 0.091, 'ExtraTreesMSE': 0.045}\n",
      "\t-4.4145\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 49.72s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1464.0 rows/s (75 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004202\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_004252\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.64 GB / 15.31 GB (17.2%)\n",
      "Disk Space Avail:   215.73 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004252\"\n",
      "Train Data Rows:    306\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2701.09 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.20 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:42:52] ✅ Cluster 47: Predicciones guardadas | Productos: 11\\n\n",
      "[2025-07-06 21:42:52] ===> Cluster 47 completado en 0.83 min.\\n\n",
      "[2025-07-06 21:42:52] ===> Iniciando procesamiento Cluster 46\\n\n",
      "[2025-07-06 21:42:52] Features OK para Cluster 46\\n\n",
      "[2025-07-06 21:42:52] Train shape: (306, 100) | Test shape: (9, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.2s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.18 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.28s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 244, Val Rows: 62\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.72s of the 299.72s of remaining time.\n",
      "\t-8.009\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.70s of the 299.70s of remaining time.\n",
      "\t-7.9207\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.68s of the 299.68s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.2944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.2642\t = Validation score   (-mean_absolute_error)\n",
      "\t2.36s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.28s of the 297.28s of remaining time.\n",
      "\t-4.3294\t = Validation score   (-mean_absolute_error)\n",
      "\t1.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 295.99s of the 295.98s of remaining time.\n",
      "\t-4.4058\t = Validation score   (-mean_absolute_error)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 295.41s of the 295.40s of remaining time.\n",
      "\t-4.6566\t = Validation score   (-mean_absolute_error)\n",
      "\t6.82s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 288.58s of the 288.58s of remaining time.\n",
      "\t-4.5656\t = Validation score   (-mean_absolute_error)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 288.06s of the 288.06s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 287.85s of the 287.85s of remaining time.\n",
      "\t-5.1004\t = Validation score   (-mean_absolute_error)\n",
      "\t1.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 286.83s of the 286.82s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 286.57s of the 286.57s of remaining time.\n",
      "\t-5.2922\t = Validation score   (-mean_absolute_error)\n",
      "\t2.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.72s of the 284.31s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.364, 'RandomForestMSE': 0.273, 'ExtraTreesMSE': 0.227, 'CatBoost': 0.091, 'XGBoost': 0.045}\n",
      "\t-3.5176\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 15.78s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 706.3 rows/s (62 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004252\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_004309\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.56 GB / 15.31 GB (16.7%)\n",
      "Disk Space Avail:   215.72 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004309\"\n",
      "Train Data Rows:    306\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2620.54 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.20 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:43:08] ✅ Cluster 46: Predicciones guardadas | Productos: 9\\n\n",
      "[2025-07-06 21:43:08] ===> Cluster 46 completado en 0.27 min.\\n\n",
      "[2025-07-06 21:43:08] ===> Iniciando procesamiento Cluster 37\\n\n",
      "[2025-07-06 21:43:08] Features OK para Cluster 37\\n\n",
      "[2025-07-06 21:43:09] Train shape: (306, 100) | Test shape: (9, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.2s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.18 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.28s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 244, Val Rows: 62\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.72s of the 299.72s of remaining time.\n",
      "\t-5.7033\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.70s of the 299.69s of remaining time.\n",
      "\t-5.5352\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.68s of the 299.68s of remaining time.\n",
      "\t-3.8545\t = Validation score   (-mean_absolute_error)\n",
      "\t0.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.67s of the 298.67s of remaining time.\n",
      "\t-4.4472\t = Validation score   (-mean_absolute_error)\n",
      "\t0.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 297.78s of the 297.77s of remaining time.\n",
      "\t-3.7332\t = Validation score   (-mean_absolute_error)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 297.24s of the 297.24s of remaining time.\n",
      "\t-2.7766\t = Validation score   (-mean_absolute_error)\n",
      "\t26.4s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 270.81s of the 270.81s of remaining time.\n",
      "\t-3.7726\t = Validation score   (-mean_absolute_error)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 270.32s of the 270.32s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 270.08s of the 270.07s of remaining time.\n",
      "\t-4.3091\t = Validation score   (-mean_absolute_error)\n",
      "\t0.98s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 269.08s of the 269.08s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 268.84s of the 268.84s of remaining time.\n",
      "\t-4.3248\t = Validation score   (-mean_absolute_error)\n",
      "\t2.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.72s of the 266.67s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.333, 'CatBoost': 0.292, 'KNeighborsDist': 0.25, 'RandomForestMSE': 0.125}\n",
      "\t-2.2125\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 33.41s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1564.9 rows/s (62 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004309\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_004342\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       3.02 GB / 15.31 GB (19.7%)\n",
      "Disk Space Avail:   215.70 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004342\"\n",
      "Train Data Rows:    231\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3096.60 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.15 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 34): ['tn_20', 'tn_21', 'tn_22', 'tn_23', 'tn_24', 'tn_25', 'tn_26', 'tn_27', 'tn_28', 'tn_29', 'tn_30', 'tn_31', 'tn_32', 'tn_33', 'tn_34', 'tn_35', 'tn_36', 'diff_tn_20', 'diff_tn_21', 'diff_tn_22', 'diff_tn_23', 'diff_tn_24', 'diff_tn_25', 'diff_tn_26', 'diff_tn_27', 'diff_tn_28', 'diff_tn_29', 'diff_tn_30', 'diff_tn_31', 'diff_tn_32', 'diff_tn_33', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 44 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 44 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t52 features in original data used to generate 52 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.09 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 184, Val Rows: 47\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.89s of the 299.89s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:43:42] ✅ Cluster 37: Predicciones guardadas | Productos: 9\\n\n",
      "[2025-07-06 21:43:42] ===> Cluster 37 completado en 0.56 min.\\n\n",
      "[2025-07-06 21:43:42] ===> Iniciando procesamiento Cluster 17\\n\n",
      "[2025-07-06 21:43:42] Features OK para Cluster 17\\n\n",
      "[2025-07-06 21:43:42] Train shape: (231, 100) | Test shape: (15, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.1757\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.88s of the 299.87s of remaining time.\n",
      "\t-4.094\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.85s of the 299.85s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.53846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.3781\t = Validation score   (-mean_absolute_error)\n",
      "\t2.55s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.25s of the 297.25s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.87552\n",
      "[2000]\tvalid_set's l1: 4.72576\n",
      "[3000]\tvalid_set's l1: 4.69886\n",
      "[4000]\tvalid_set's l1: 4.69498\n",
      "[5000]\tvalid_set's l1: 4.68943\n",
      "[6000]\tvalid_set's l1: 4.68783\n",
      "[7000]\tvalid_set's l1: 4.68649\n",
      "[8000]\tvalid_set's l1: 4.68636\n",
      "[9000]\tvalid_set's l1: 4.6862\n",
      "[10000]\tvalid_set's l1: 4.68609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.6861\t = Validation score   (-mean_absolute_error)\n",
      "\t14.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 282.65s of the 282.65s of remaining time.\n",
      "\t-2.8481\t = Validation score   (-mean_absolute_error)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 282.12s of the 282.12s of remaining time.\n",
      "\t-6.3739\t = Validation score   (-mean_absolute_error)\n",
      "\t3.92s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 278.18s of the 278.18s of remaining time.\n",
      "\t-2.7228\t = Validation score   (-mean_absolute_error)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 277.70s of the 277.70s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 277.48s of the 277.48s of remaining time.\n",
      "\t-3.8502\t = Validation score   (-mean_absolute_error)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 276.84s of the 276.84s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 276.59s of the 276.58s of remaining time.\n",
      "\t-2.2766\t = Validation score   (-mean_absolute_error)\n",
      "\t2.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.89s of the 274.39s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE': 0.643, 'LightGBMLarge': 0.357}\n",
      "\t-2.1148\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 25.7s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1286.4 rows/s (47 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004342\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_004408\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       3.02 GB / 15.31 GB (19.7%)\n",
      "Disk Space Avail:   215.68 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004408\"\n",
      "Train Data Rows:    272\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3092.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.18 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:44:08] ✅ Cluster 17: Predicciones guardadas | Productos: 15\\n\n",
      "[2025-07-06 21:44:08] ===> Cluster 17 completado en 0.43 min.\\n\n",
      "[2025-07-06 21:44:08] ===> Iniciando procesamiento Cluster 25\\n\n",
      "[2025-07-06 21:44:08] Features OK para Cluster 25\\n\n",
      "[2025-07-06 21:44:08] Train shape: (272, 100) | Test shape: (8, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['brand_factorized', 'descripcion_factorized', 'mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 3 | ['brand_factorized', 'descripcion_factorized', 'mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  6 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'quarter_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  6 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'quarter_factorized', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t78 features in original data used to generate 78 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.26s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 217, Val Rows: 55\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.74s of the 299.74s of remaining time.\n",
      "\t-8.5475\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.73s of the 299.73s of remaining time.\n",
      "\t-8.2581\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.71s of the 299.71s of remaining time.\n",
      "\t-3.2511\t = Validation score   (-mean_absolute_error)\n",
      "\t1.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.24s of the 298.24s of remaining time.\n",
      "\t-4.6102\t = Validation score   (-mean_absolute_error)\n",
      "\t1.69s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 296.52s of the 296.52s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.70734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.6635\t = Validation score   (-mean_absolute_error)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 295.94s of the 295.94s of remaining time.\n",
      "\t-4.93\t = Validation score   (-mean_absolute_error)\n",
      "\t90.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 205.35s of the 205.35s of remaining time.\n",
      "\t-5.3194\t = Validation score   (-mean_absolute_error)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 204.88s of the 204.88s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 204.64s of the 204.64s of remaining time.\n",
      "\t-4.6579\t = Validation score   (-mean_absolute_error)\n",
      "\t0.99s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 203.63s of the 203.63s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 203.39s of the 203.39s of remaining time.\n",
      "\t-4.921\t = Validation score   (-mean_absolute_error)\n",
      "\t2.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.74s of the 201.24s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.826, 'XGBoost': 0.174}\n",
      "\t-3.1879\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 98.84s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6805.3 rows/s (55 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004408\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_004547\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       3.40 GB / 15.31 GB (22.2%)\n",
      "Disk Space Avail:   215.65 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004547\"\n",
      "Train Data Rows:    464\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3473.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:45:47] ✅ Cluster 25: Predicciones guardadas | Productos: 8\\n\n",
      "[2025-07-06 21:45:47] ===> Cluster 25 completado en 1.65 min.\\n\n",
      "[2025-07-06 21:45:47] ===> Iniciando procesamiento Cluster 3\\n\n",
      "[2025-07-06 21:45:47] Features OK para Cluster 3\\n\n",
      "[2025-07-06 21:45:47] Train shape: (464, 100) | Test shape: (38, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 42): ['tn_16', 'tn_17', 'tn_18', 'tn_19', 'tn_20', 'tn_21', 'tn_22', 'tn_23', 'tn_24', 'tn_25', 'tn_26', 'tn_27', 'tn_28', 'tn_29', 'tn_30', 'tn_31', 'tn_32', 'tn_33', 'tn_34', 'tn_35', 'tn_36', 'diff_tn_16', 'diff_tn_17', 'diff_tn_18', 'diff_tn_19', 'diff_tn_20', 'diff_tn_21', 'diff_tn_22', 'diff_tn_23', 'diff_tn_24', 'diff_tn_25', 'diff_tn_26', 'diff_tn_27', 'diff_tn_28', 'diff_tn_29', 'diff_tn_30', 'diff_tn_31', 'diff_tn_32', 'diff_tn_33', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 36 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 36 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t44 features in original data used to generate 44 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 371, Val Rows: 93\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.89s of the 299.88s of remaining time.\n",
      "\t-5.9328\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.86s of the 299.85s of remaining time.\n",
      "\t-5.7221\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.84s of the 299.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.2387\n",
      "[2000]\tvalid_set's l1: 6.17559\n",
      "[3000]\tvalid_set's l1: 6.136\n",
      "[4000]\tvalid_set's l1: 6.11552\n",
      "[5000]\tvalid_set's l1: 6.09872\n",
      "[6000]\tvalid_set's l1: 6.09694\n",
      "[7000]\tvalid_set's l1: 6.09881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-6.0953\t = Validation score   (-mean_absolute_error)\n",
      "\t12.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 286.76s of the 286.76s of remaining time.\n",
      "\t-6.9466\t = Validation score   (-mean_absolute_error)\n",
      "\t1.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 285.75s of the 285.74s of remaining time.\n",
      "\t-5.8402\t = Validation score   (-mean_absolute_error)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 285.19s of the 285.19s of remaining time.\n",
      "\t-3.767\t = Validation score   (-mean_absolute_error)\n",
      "\t81.73s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 203.42s of the 203.41s of remaining time.\n",
      "\t-5.8353\t = Validation score   (-mean_absolute_error)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 202.94s of the 202.94s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 202.72s of the 202.71s of remaining time.\n",
      "\t-5.2778\t = Validation score   (-mean_absolute_error)\n",
      "\t0.86s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 201.84s of the 201.84s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 201.61s of the 201.60s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.31845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.318\t = Validation score   (-mean_absolute_error)\n",
      "\t9.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.89s of the 191.55s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 0.96, 'XGBoost': 0.04}\n",
      "\t-3.6772\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 108.54s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 13335.7 rows/s (93 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004547\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_004736\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       3.18 GB / 15.31 GB (20.8%)\n",
      "Disk Space Avail:   215.60 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004736\"\n",
      "Train Data Rows:    544\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3254.38 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:47:36] ✅ Cluster 3: Predicciones guardadas | Productos: 38\\n\n",
      "[2025-07-06 21:47:36] ===> Cluster 3 completado en 1.81 min.\\n\n",
      "[2025-07-06 21:47:36] ===> Iniciando procesamiento Cluster 26\\n\n",
      "[2025-07-06 21:47:36] Features OK para Cluster 26\\n\n",
      "[2025-07-06 21:47:36] Train shape: (544, 100) | Test shape: (16, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 435, Val Rows: 109\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.83s of the 299.82s of remaining time.\n",
      "\t-3.8984\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.81s of the 299.80s of remaining time.\n",
      "\t-3.8909\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.79s of the 299.78s of remaining time.\n",
      "\t-2.3753\t = Validation score   (-mean_absolute_error)\n",
      "\t1.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.16s of the 298.16s of remaining time.\n",
      "\t-2.575\t = Validation score   (-mean_absolute_error)\n",
      "\t2.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 296.04s of the 296.03s of remaining time.\n",
      "\t-2.8771\t = Validation score   (-mean_absolute_error)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 295.33s of the 295.33s of remaining time.\n",
      "\t-2.8963\t = Validation score   (-mean_absolute_error)\n",
      "\t108.41s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 186.89s of the 186.89s of remaining time.\n",
      "\t-3.2088\t = Validation score   (-mean_absolute_error)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 186.35s of the 186.34s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 186.11s of the 186.10s of remaining time.\n",
      "\t-2.8959\t = Validation score   (-mean_absolute_error)\n",
      "\t2.17s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 183.92s of the 183.91s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 183.65s of the 183.65s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 2.82297\n",
      "[2000]\tvalid_set's l1: 2.82196\n",
      "[3000]\tvalid_set's l1: 2.82196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-2.822\t = Validation score   (-mean_absolute_error)\n",
      "\t24.15s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.83s of the 159.00s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.85, 'RandomForestMSE': 0.1, 'XGBoost': 0.05}\n",
      "\t-2.3393\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 141.07s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1976.9 rows/s (109 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004736\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_004957\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       3.22 GB / 15.31 GB (21.0%)\n",
      "Disk Space Avail:   215.54 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004957\"\n",
      "Train Data Rows:    143\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3295.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.10 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:49:57] ✅ Cluster 26: Predicciones guardadas | Productos: 16\\n\n",
      "[2025-07-06 21:49:57] ===> Cluster 26 completado en 2.36 min.\\n\n",
      "[2025-07-06 21:49:57] ===> Iniciando procesamiento Cluster 38\\n\n",
      "[2025-07-06 21:49:57] Features OK para Cluster 38\\n\n",
      "[2025-07-06 21:49:57] Train shape: (143, 100) | Test shape: (11, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['diff_tn_33', 'brand_factorized', 'mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['diff_tn_33']\n",
      "\t\t('int', [])   : 2 | ['brand_factorized', 'mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 71 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  7 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'descripcion_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 70 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['tn_33']\n",
      "\t0.1s = Fit runtime\n",
      "\t78 features in original data used to generate 78 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.08 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 114, Val Rows: 29\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.86s of the 299.85s of remaining time.\n",
      "\t-7.7458\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.82s of remaining time.\n",
      "\t-7.4604\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.81s of the 299.80s of remaining time.\n",
      "\t-5.548\t = Validation score   (-mean_absolute_error)\n",
      "\t1.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.51s of the 298.50s of remaining time.\n",
      "\t-8.0564\t = Validation score   (-mean_absolute_error)\n",
      "\t1.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 297.45s of the 297.44s of remaining time.\n",
      "\t-6.4362\t = Validation score   (-mean_absolute_error)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 296.80s of the 296.79s of remaining time.\n",
      "\t-5.8827\t = Validation score   (-mean_absolute_error)\n",
      "\t17.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 279.58s of the 279.57s of remaining time.\n",
      "\t-5.3675\t = Validation score   (-mean_absolute_error)\n",
      "\t0.41s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 279.12s of the 279.11s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 278.90s of the 278.90s of remaining time.\n",
      "\t-8.7154\t = Validation score   (-mean_absolute_error)\n",
      "\t0.82s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 278.07s of the 278.06s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 277.81s of the 277.81s of remaining time.\n",
      "\t-6.8457\t = Validation score   (-mean_absolute_error)\n",
      "\t1.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.86s of the 276.69s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.56, 'CatBoost': 0.2, 'RandomForestMSE': 0.12, 'ExtraTreesMSE': 0.12}\n",
      "\t-4.006\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 23.39s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 329.1 rows/s (29 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_004957\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_005021\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       3.15 GB / 15.31 GB (20.6%)\n",
      "Disk Space Avail:   215.53 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005021\"\n",
      "Train Data Rows:    505\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3227.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:50:21] ✅ Cluster 38: Predicciones guardadas | Productos: 11\\n\n",
      "[2025-07-06 21:50:21] ===> Cluster 38 completado en 0.39 min.\\n\n",
      "[2025-07-06 21:50:21] ===> Iniciando procesamiento Cluster 22\\n\n",
      "[2025-07-06 21:50:21] Features OK para Cluster 22\\n\n",
      "[2025-07-06 21:50:21] Train shape: (505, 100) | Test shape: (24, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['diff_tn_33', 'mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['diff_tn_33']\n",
      "\t\t('int', [])   : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 71 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 70 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['tn_33']\n",
      "\t0.2s = Fit runtime\n",
      "\t79 features in original data used to generate 79 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.30 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.19s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 404, Val Rows: 101\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.81s of the 299.81s of remaining time.\n",
      "\t-5.6202\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.79s of the 299.79s of remaining time.\n",
      "\t-5.4685\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.76s of the 299.76s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 3.39415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-3.3679\t = Validation score   (-mean_absolute_error)\n",
      "\t3.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 296.37s of the 296.37s of remaining time.\n",
      "\t-4.0723\t = Validation score   (-mean_absolute_error)\n",
      "\t1.82s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 294.52s of the 294.52s of remaining time.\n",
      "\t-5.1555\t = Validation score   (-mean_absolute_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 293.87s of the 293.87s of remaining time.\n",
      "\t-3.6309\t = Validation score   (-mean_absolute_error)\n",
      "\t32.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 261.61s of the 261.61s of remaining time.\n",
      "\t-4.5541\t = Validation score   (-mean_absolute_error)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 261.05s of the 261.05s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 260.82s of the 260.82s of remaining time.\n",
      "\t-4.8477\t = Validation score   (-mean_absolute_error)\n",
      "\t2.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 258.77s of the 258.77s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 258.52s of the 258.51s of remaining time.\n",
      "\t-5.2974\t = Validation score   (-mean_absolute_error)\n",
      "\t4.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.81s of the 254.34s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'CatBoost': 0.36, 'KNeighborsDist': 0.04}\n",
      "\t-3.2362\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 45.73s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6750.6 rows/s (101 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005021\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_005107\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       3.17 GB / 15.31 GB (20.7%)\n",
      "Disk Space Avail:   215.49 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005107\"\n",
      "Train Data Rows:    153\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3244.57 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.10 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 61): ['tn_8', 'tn_9', 'tn_10', 'tn_11', 'tn_12', 'tn_13', 'tn_14', 'tn_15', 'tn_16', 'tn_17', 'tn_18', 'tn_19', 'tn_20', 'tn_21', 'tn_22', 'tn_23', 'tn_24', 'tn_25', 'tn_26', 'tn_27', 'tn_28', 'tn_29', 'tn_30', 'tn_31', 'tn_32', 'tn_33', 'tn_34', 'tn_35', 'tn_36', 'diff_tn_8', 'diff_tn_9', 'diff_tn_10', 'diff_tn_11', 'diff_tn_12', 'diff_tn_13', 'diff_tn_14', 'diff_tn_15', 'diff_tn_16', 'diff_tn_17', 'diff_tn_18', 'diff_tn_19', 'diff_tn_20', 'diff_tn_21', 'diff_tn_22', 'diff_tn_23', 'diff_tn_24', 'diff_tn_25', 'diff_tn_26', 'diff_tn_27', 'diff_tn_28', 'diff_tn_29', 'diff_tn_30', 'diff_tn_31', 'diff_tn_32', 'diff_tn_33', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36', 'rollmean_9', 'rollmean_12', 'diff_rollmean_12']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['diff_tn_7', 'mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['diff_tn_7']\n",
      "\t\t('int', [])   : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 16 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 15 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['tn_7']\n",
      "\t0.1s = Fit runtime\n",
      "\t24 features in original data used to generate 24 features in processed data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:51:07] ✅ Cluster 22: Predicciones guardadas | Productos: 24\\n\n",
      "[2025-07-06 21:51:07] ===> Cluster 22 completado en 0.77 min.\\n\n",
      "[2025-07-06 21:51:07] ===> Iniciando procesamiento Cluster 5\\n\n",
      "[2025-07-06 21:51:07] Features OK para Cluster 5\\n\n",
      "[2025-07-06 21:51:07] Train shape: (153, 100) | Test shape: (35, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 122, Val Rows: 31\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.90s of the 299.89s of remaining time.\n",
      "\t-3.1193\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.88s of the 299.88s of remaining time.\n",
      "\t-3.2314\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.86s of the 299.86s of remaining time.\n",
      "\t-6.0287\t = Validation score   (-mean_absolute_error)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 299.25s of the 299.24s of remaining time.\n",
      "\t-5.3057\t = Validation score   (-mean_absolute_error)\n",
      "\t0.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 298.47s of the 298.47s of remaining time.\n",
      "\t-2.3057\t = Validation score   (-mean_absolute_error)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 297.99s of the 297.99s of remaining time.\n",
      "\t-1.9179\t = Validation score   (-mean_absolute_error)\n",
      "\t2.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 295.87s of the 295.86s of remaining time.\n",
      "\t-2.7731\t = Validation score   (-mean_absolute_error)\n",
      "\t0.38s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 295.44s of the 295.44s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 295.23s of the 295.22s of remaining time.\n",
      "\t-2.4595\t = Validation score   (-mean_absolute_error)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 294.58s of the 294.57s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 294.33s of the 294.32s of remaining time.\n",
      "\t-2.8441\t = Validation score   (-mean_absolute_error)\n",
      "\t1.1s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.90s of the 293.09s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 0.609, 'XGBoost': 0.391}\n",
      "\t-1.6843\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 6.98s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 7775.1 rows/s (31 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005107\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_005114\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       3.14 GB / 15.31 GB (20.5%)\n",
      "Disk Space Avail:   215.49 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005114\"\n",
      "Train Data Rows:    319\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3218.20 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.21 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:51:14] ✅ Cluster 5: Predicciones guardadas | Productos: 35\\n\n",
      "[2025-07-06 21:51:14] ===> Cluster 5 completado en 0.12 min.\\n\n",
      "[2025-07-06 21:51:14] ===> Iniciando procesamiento Cluster 2\\n\n",
      "[2025-07-06 21:51:14] Features OK para Cluster 2\\n\n",
      "[2025-07-06 21:51:14] Train shape: (319, 100) | Test shape: (15, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 16): ['tn_29', 'tn_30', 'tn_31', 'tn_32', 'tn_33', 'tn_34', 'tn_35', 'tn_36', 'diff_tn_29', 'diff_tn_30', 'diff_tn_31', 'diff_tn_32', 'diff_tn_33', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['diff_tn_28', 'mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['diff_tn_28']\n",
      "\t\t('int', [])   : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 61 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 60 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['tn_28']\n",
      "\t0.1s = Fit runtime\n",
      "\t69 features in original data used to generate 69 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.17 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 255, Val Rows: 64\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.88s of the 299.88s of remaining time.\n",
      "\t-6.1591\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.87s of the 299.87s of remaining time.\n",
      "\t-5.578\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.85s of the 299.85s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.46807\n",
      "[2000]\tvalid_set's l1: 4.15901\n",
      "[3000]\tvalid_set's l1: 4.08097\n",
      "[4000]\tvalid_set's l1: 4.04761\n",
      "[5000]\tvalid_set's l1: 4.0276\n",
      "[6000]\tvalid_set's l1: 4.01207\n",
      "[7000]\tvalid_set's l1: 4.00313\n",
      "[8000]\tvalid_set's l1: 3.99508\n",
      "[9000]\tvalid_set's l1: 3.99087\n",
      "[10000]\tvalid_set's l1: 3.98716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-3.9871\t = Validation score   (-mean_absolute_error)\n",
      "\t14.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 284.45s of the 284.45s of remaining time.\n",
      "\t-5.0757\t = Validation score   (-mean_absolute_error)\n",
      "\t1.55s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 282.87s of the 282.87s of remaining time.\n",
      "\t-4.1769\t = Validation score   (-mean_absolute_error)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 282.27s of the 282.27s of remaining time.\n",
      "\t-4.4226\t = Validation score   (-mean_absolute_error)\n",
      "\t4.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 278.06s of the 278.06s of remaining time.\n",
      "\t-4.6919\t = Validation score   (-mean_absolute_error)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 277.55s of the 277.55s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 277.34s of the 277.33s of remaining time.\n",
      "\t-3.4977\t = Validation score   (-mean_absolute_error)\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 276.45s of the 276.45s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 276.18s of the 276.18s of remaining time.\n",
      "\t-3.5506\t = Validation score   (-mean_absolute_error)\n",
      "\t3.83s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.89s of the 272.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMLarge': 0.565, 'XGBoost': 0.304, 'CatBoost': 0.13}\n",
      "\t-3.1786\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 27.94s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6443.8 rows/s (64 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005114\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_005142\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       3.16 GB / 15.31 GB (20.6%)\n",
      "Disk Space Avail:   215.46 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005142\"\n",
      "Train Data Rows:    238\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3238.13 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:51:42] ✅ Cluster 2: Predicciones guardadas | Productos: 15\\n\n",
      "[2025-07-06 21:51:42] ===> Cluster 2 completado en 0.47 min.\\n\n",
      "[2025-07-06 21:51:42] ===> Iniciando procesamiento Cluster 42\\n\n",
      "[2025-07-06 21:51:42] Features OK para Cluster 42\\n\n",
      "[2025-07-06 21:51:42] Train shape: (238, 100) | Test shape: (7, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['brand_factorized', 'mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 2 | ['brand_factorized', 'mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  7 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'descripcion_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  6 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'descripcion_factorized', 'quarter_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.2s = Fit runtime\n",
      "\t79 features in original data used to generate 79 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.14 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.27s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 190, Val Rows: 48\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.73s of the 299.73s of remaining time.\n",
      "\t-5.8657\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.71s of the 299.70s of remaining time.\n",
      "\t-5.6584\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.69s of the 299.69s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 3.76862\n",
      "[2000]\tvalid_set's l1: 3.66249\n",
      "[3000]\tvalid_set's l1: 3.63074\n",
      "[4000]\tvalid_set's l1: 3.62791\n",
      "[5000]\tvalid_set's l1: 3.62639\n",
      "[6000]\tvalid_set's l1: 3.62741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-3.6257\t = Validation score   (-mean_absolute_error)\n",
      "\t8.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 290.85s of the 290.85s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.37289\n",
      "[2000]\tvalid_set's l1: 4.30698\n",
      "[3000]\tvalid_set's l1: 4.30558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.3033\t = Validation score   (-mean_absolute_error)\n",
      "\t4.7s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 286.00s of the 286.00s of remaining time.\n",
      "\t-4.7435\t = Validation score   (-mean_absolute_error)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 285.40s of the 285.39s of remaining time.\n",
      "\t-5.0763\t = Validation score   (-mean_absolute_error)\n",
      "\t89.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 195.90s of the 195.90s of remaining time.\n",
      "\t-4.6223\t = Validation score   (-mean_absolute_error)\n",
      "\t0.44s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 195.40s of the 195.39s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 195.16s of the 195.16s of remaining time.\n",
      "\t-4.4259\t = Validation score   (-mean_absolute_error)\n",
      "\t0.94s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 194.20s of the 194.20s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 193.96s of the 193.96s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 3.96204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-3.962\t = Validation score   (-mean_absolute_error)\n",
      "\t4.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.73s of the 189.30s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-3.6257\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 110.78s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 5691.4 rows/s (48 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005142\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_005333\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       3.27 GB / 15.31 GB (21.3%)\n",
      "Disk Space Avail:   215.42 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005333\"\n",
      "Train Data Rows:    204\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3346.79 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.14 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:53:33] ✅ Cluster 42: Predicciones guardadas | Productos: 7\\n\n",
      "[2025-07-06 21:53:33] ===> Cluster 42 completado en 1.85 min.\\n\n",
      "[2025-07-06 21:53:33] ===> Iniciando procesamiento Cluster 14\\n\n",
      "[2025-07-06 21:53:33] Features OK para Cluster 14\\n\n",
      "[2025-07-06 21:53:33] Train shape: (204, 100) | Test shape: (6, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['cat2_factorized', 'mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 2 | ['cat2_factorized', 'mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  7 | ['periodo_factorized', 'cat1_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  7 | ['periodo_factorized', 'cat1_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t79 features in original data used to generate 79 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.12 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.27s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 163, Val Rows: 41\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.73s of the 299.72s of remaining time.\n",
      "\t-2.812\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.71s of the 299.71s of remaining time.\n",
      "\t-2.7471\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.69s of the 299.69s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 1.84439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.8298\t = Validation score   (-mean_absolute_error)\n",
      "\t1.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.67s of the 297.66s of remaining time.\n",
      "\t-1.8029\t = Validation score   (-mean_absolute_error)\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 296.77s of the 296.77s of remaining time.\n",
      "\t-2.8299\t = Validation score   (-mean_absolute_error)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 296.21s of the 296.21s of remaining time.\n",
      "\t-2.1601\t = Validation score   (-mean_absolute_error)\n",
      "\t18.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 277.97s of the 277.97s of remaining time.\n",
      "\t-2.1054\t = Validation score   (-mean_absolute_error)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 277.50s of the 277.49s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 277.24s of the 277.24s of remaining time.\n",
      "\t-2.7655\t = Validation score   (-mean_absolute_error)\n",
      "\t0.69s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 276.53s of the 276.53s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 276.27s of the 276.27s of remaining time.\n",
      "\t-2.8639\t = Validation score   (-mean_absolute_error)\n",
      "\t1.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.73s of the 274.89s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.533, 'CatBoost': 0.267, 'ExtraTreesMSE': 0.133, 'KNeighborsDist': 0.067}\n",
      "\t-1.7805\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 25.19s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 999.6 rows/s (41 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005333\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_005359\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       3.28 GB / 15.31 GB (21.4%)\n",
      "Disk Space Avail:   215.41 GB / 475.95 GB (45.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005359\"\n",
      "Train Data Rows:    510\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3361.11 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 7): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36', 'cat1_factorized']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:53:59] ✅ Cluster 14: Predicciones guardadas | Productos: 6\\n\n",
      "[2025-07-06 21:53:59] ===> Cluster 14 completado en 0.42 min.\\n\n",
      "[2025-07-06 21:53:59] ===> Iniciando procesamiento Cluster 34\\n\n",
      "[2025-07-06 21:53:59] Features OK para Cluster 34\\n\n",
      "[2025-07-06 21:53:59] Train shape: (510, 100) | Test shape: (15, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  6 | ['periodo_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', 'quarter_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat2_factorized']\n",
      "\t0.1s = Fit runtime\n",
      "\t79 features in original data used to generate 79 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.30 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 408, Val Rows: 102\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.85s of the 299.85s of remaining time.\n",
      "\t-2.5223\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.84s of the 299.83s of remaining time.\n",
      "\t-2.3632\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.81s of the 299.81s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 1.50359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.503\t = Validation score   (-mean_absolute_error)\n",
      "\t2.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.19s of the 297.19s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 1.64294\n",
      "[2000]\tvalid_set's l1: 1.61955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.6191\t = Validation score   (-mean_absolute_error)\n",
      "\t5.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 291.43s of the 291.43s of remaining time.\n",
      "\t-1.9581\t = Validation score   (-mean_absolute_error)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 290.78s of the 290.77s of remaining time.\n",
      "\t-1.6811\t = Validation score   (-mean_absolute_error)\n",
      "\t31.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 259.72s of the 259.72s of remaining time.\n",
      "\t-1.7859\t = Validation score   (-mean_absolute_error)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 259.15s of the 259.15s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 258.81s of the 258.81s of remaining time.\n",
      "\t-1.7362\t = Validation score   (-mean_absolute_error)\n",
      "\t2.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 256.77s of the 256.77s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 256.51s of the 256.51s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 2.28004\n",
      "[2000]\tvalid_set's l1: 2.27922\n",
      "[3000]\tvalid_set's l1: 2.2792\n",
      "[4000]\tvalid_set's l1: 2.2792\n",
      "[5000]\tvalid_set's l1: 2.2792\n",
      "[6000]\tvalid_set's l1: 2.2792\n",
      "[7000]\tvalid_set's l1: 2.2792\n",
      "[8000]\tvalid_set's l1: 2.2792\n",
      "[9000]\tvalid_set's l1: 2.2792\n",
      "[10000]\tvalid_set's l1: 2.2792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-2.2792\t = Validation score   (-mean_absolute_error)\n",
      "\t72.55s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.85s of the 182.51s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.684, 'XGBoost': 0.211, 'ExtraTreesMSE': 0.105}\n",
      "\t-1.4281\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 117.57s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2525.0 rows/s (102 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005359\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_005556\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.21 GB / 15.31 GB (14.5%)\n",
      "Disk Space Avail:   215.27 GB / 475.95 GB (45.2%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005556\"\n",
      "Train Data Rows:    544\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2267.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:55:56] ✅ Cluster 34: Predicciones guardadas | Productos: 15\\n\n",
      "[2025-07-06 21:55:56] ===> Cluster 34 completado en 1.96 min.\\n\n",
      "[2025-07-06 21:55:56] ===> Iniciando procesamiento Cluster 8\\n\n",
      "[2025-07-06 21:55:56] Features OK para Cluster 8\\n\n",
      "[2025-07-06 21:55:56] Train shape: (544, 100) | Test shape: (16, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 435, Val Rows: 109\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.86s of the 299.85s of remaining time.\n",
      "\t-2.4761\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.83s of remaining time.\n",
      "\t-2.4284\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.82s of the 299.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 1.35435\n",
      "[2000]\tvalid_set's l1: 1.31903\n",
      "[3000]\tvalid_set's l1: 1.29938\n",
      "[4000]\tvalid_set's l1: 1.29089\n",
      "[5000]\tvalid_set's l1: 1.28892\n",
      "[6000]\tvalid_set's l1: 1.28741\n",
      "[7000]\tvalid_set's l1: 1.28764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.2869\t = Validation score   (-mean_absolute_error)\n",
      "\t14.43s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 285.01s of the 285.01s of remaining time.\n",
      "\t-1.671\t = Validation score   (-mean_absolute_error)\n",
      "\t1.89s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 283.09s of the 283.09s of remaining time.\n",
      "\t-2.0552\t = Validation score   (-mean_absolute_error)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 282.42s of the 282.42s of remaining time.\n",
      "\t-1.7454\t = Validation score   (-mean_absolute_error)\n",
      "\t79.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 203.21s of the 203.21s of remaining time.\n",
      "\t-1.9525\t = Validation score   (-mean_absolute_error)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 202.70s of the 202.70s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 202.48s of the 202.48s of remaining time.\n",
      "\t-1.8112\t = Validation score   (-mean_absolute_error)\n",
      "\t1.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 200.49s of the 200.49s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 200.25s of the 200.25s of remaining time.\n",
      "\t-1.8737\t = Validation score   (-mean_absolute_error)\n",
      "\t3.69s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.86s of the 196.43s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.929, 'XGBoost': 0.071}\n",
      "\t-1.2799\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 103.66s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 5084.3 rows/s (109 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005556\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_005740\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.14 GB / 15.31 GB (13.9%)\n",
      "Disk Space Avail:   215.22 GB / 475.95 GB (45.2%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005740\"\n",
      "Train Data Rows:    544\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2188.94 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:57:40] ✅ Cluster 8: Predicciones guardadas | Productos: 16\\n\n",
      "[2025-07-06 21:57:40] ===> Cluster 8 completado en 1.73 min.\\n\n",
      "[2025-07-06 21:57:40] ===> Iniciando procesamiento Cluster 27\\n\n",
      "[2025-07-06 21:57:40] Features OK para Cluster 27\\n\n",
      "[2025-07-06 21:57:40] Train shape: (544, 100) | Test shape: (16, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 435, Val Rows: 109\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.84s of the 299.84s of remaining time.\n",
      "\t-2.6994\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.82s of the 299.82s of remaining time.\n",
      "\t-2.584\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.81s of the 299.80s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 1.18961\n",
      "[2000]\tvalid_set's l1: 1.18231\n",
      "[3000]\tvalid_set's l1: 1.17647\n",
      "[4000]\tvalid_set's l1: 1.17223\n",
      "[5000]\tvalid_set's l1: 1.17294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.172\t = Validation score   (-mean_absolute_error)\n",
      "\t9.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 290.18s of the 290.18s of remaining time.\n",
      "\t-1.3467\t = Validation score   (-mean_absolute_error)\n",
      "\t1.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 288.36s of the 288.35s of remaining time.\n",
      "\t-1.8268\t = Validation score   (-mean_absolute_error)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 287.70s of the 287.69s of remaining time.\n",
      "\t-1.4385\t = Validation score   (-mean_absolute_error)\n",
      "\t82.3s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 205.38s of the 205.38s of remaining time.\n",
      "\t-1.7998\t = Validation score   (-mean_absolute_error)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 204.89s of the 204.88s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 204.68s of the 204.68s of remaining time.\n",
      "\t-1.6547\t = Validation score   (-mean_absolute_error)\n",
      "\t1.84s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 202.82s of the 202.81s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 202.57s of the 202.57s of remaining time.\n",
      "\t-1.9575\t = Validation score   (-mean_absolute_error)\n",
      "\t4.77s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.84s of the 197.62s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.857, 'CatBoost': 0.143}\n",
      "\t-1.1456\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 102.46s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 8270.8 rows/s (109 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005740\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_005923\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.32 GB / 15.31 GB (15.1%)\n",
      "Disk Space Avail:   215.18 GB / 475.95 GB (45.2%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005923\"\n",
      "Train Data Rows:    408\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2373.26 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.27 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 21:59:23] ✅ Cluster 27: Predicciones guardadas | Productos: 16\\n\n",
      "[2025-07-06 21:59:23] ===> Cluster 27 completado en 1.71 min.\\n\n",
      "[2025-07-06 21:59:23] ===> Iniciando procesamiento Cluster 13\\n\n",
      "[2025-07-06 21:59:23] Features OK para Cluster 13\\n\n",
      "[2025-07-06 21:59:23] Train shape: (408, 100) | Test shape: (12, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.3s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.25 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.29s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 326, Val Rows: 82\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.71s of the 299.71s of remaining time.\n",
      "\t-2.1821\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.69s of the 299.69s of remaining time.\n",
      "\t-2.1978\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.68s of the 299.67s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 1.7342\n",
      "[2000]\tvalid_set's l1: 1.70114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.6985\t = Validation score   (-mean_absolute_error)\n",
      "\t4.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 294.88s of the 294.88s of remaining time.\n",
      "\t-1.6633\t = Validation score   (-mean_absolute_error)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 293.43s of the 293.42s of remaining time.\n",
      "\t-1.9508\t = Validation score   (-mean_absolute_error)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 292.85s of the 292.85s of remaining time.\n",
      "\t-1.2609\t = Validation score   (-mean_absolute_error)\n",
      "\t105.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 187.01s of the 187.01s of remaining time.\n",
      "\t-1.932\t = Validation score   (-mean_absolute_error)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 186.41s of the 186.41s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 186.12s of the 186.12s of remaining time.\n",
      "\t-2.1614\t = Validation score   (-mean_absolute_error)\n",
      "\t1.75s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 184.35s of the 184.35s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 184.07s of the 184.06s of remaining time.\n",
      "\t-2.4396\t = Validation score   (-mean_absolute_error)\n",
      "\t3.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.71s of the 180.35s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 0.81, 'LightGBM': 0.143, 'KNeighborsUnif': 0.048}\n",
      "\t-1.2154\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 119.75s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 7918.2 rows/s (82 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_005923\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_010123\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.16 GB / 15.31 GB (14.1%)\n",
      "Disk Space Avail:   204.27 GB / 475.95 GB (42.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010123\"\n",
      "Train Data Rows:    560\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2214.30 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.37 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 22:01:23] ✅ Cluster 13: Predicciones guardadas | Productos: 12\\n\n",
      "[2025-07-06 22:01:23] ===> Cluster 13 completado en 2.00 min.\\n\n",
      "[2025-07-06 22:01:23] ===> Iniciando procesamiento Cluster 23\\n\n",
      "[2025-07-06 22:01:23] Features OK para Cluster 23\\n\n",
      "[2025-07-06 22:01:23] Train shape: (560, 100) | Test shape: (21, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.2s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.34 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.21s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 448, Val Rows: 112\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.79s of the 299.79s of remaining time.\n",
      "\t-2.1882\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.76s of the 299.76s of remaining time.\n",
      "\t-2.0888\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.74s of the 299.74s of remaining time.\n",
      "\t-1.2299\t = Validation score   (-mean_absolute_error)\n",
      "\t2.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.35s of the 297.35s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 1.23609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.4511\t = Validation score   (-mean_absolute_error)\n",
      "\t1.63s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 295.69s of the 295.69s of remaining time.\n",
      "\t-1.4648\t = Validation score   (-mean_absolute_error)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 294.97s of the 294.97s of remaining time.\n",
      "\t-1.3863\t = Validation score   (-mean_absolute_error)\n",
      "\t24.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 270.43s of the 270.43s of remaining time.\n",
      "\t-1.3784\t = Validation score   (-mean_absolute_error)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 269.86s of the 269.86s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 269.63s of the 269.62s of remaining time.\n",
      "\t-1.609\t = Validation score   (-mean_absolute_error)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 267.87s of the 267.87s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 267.60s of the 267.60s of remaining time.\n",
      "\t-1.4403\t = Validation score   (-mean_absolute_error)\n",
      "\t3.94s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.79s of the 263.51s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.571, 'CatBoost': 0.286, 'ExtraTreesMSE': 0.143}\n",
      "\t-1.177\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 36.58s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2836.9 rows/s (112 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010123\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_010200\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.23 GB / 15.31 GB (14.5%)\n",
      "Disk Space Avail:   215.11 GB / 475.95 GB (45.2%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010200\"\n",
      "Train Data Rows:    90\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2279.69 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 68): ['tn_5', 'tn_6', 'tn_7', 'tn_8', 'tn_9', 'tn_10', 'tn_11', 'tn_12', 'tn_13', 'tn_14', 'tn_15', 'tn_16', 'tn_17', 'tn_18', 'tn_19', 'tn_20', 'tn_21', 'tn_22', 'tn_23', 'tn_24', 'tn_25', 'tn_26', 'tn_27', 'tn_28', 'tn_29', 'tn_30', 'tn_31', 'tn_32', 'tn_33', 'tn_34', 'tn_35', 'tn_36', 'diff_tn_5', 'diff_tn_6', 'diff_tn_7', 'diff_tn_8', 'diff_tn_9', 'diff_tn_10', 'diff_tn_11', 'diff_tn_12', 'diff_tn_13', 'diff_tn_14', 'diff_tn_15', 'diff_tn_16', 'diff_tn_17', 'diff_tn_18', 'diff_tn_19', 'diff_tn_20', 'diff_tn_21', 'diff_tn_22', 'diff_tn_23', 'diff_tn_24', 'diff_tn_25', 'diff_tn_26', 'diff_tn_27', 'diff_tn_28', 'diff_tn_29', 'diff_tn_30', 'diff_tn_31', 'diff_tn_32', 'diff_tn_33', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36', 'rollmean_6', 'rollmean_9', 'rollmean_12', 'diff_rollmean_12']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 10 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 22:02:00] ✅ Cluster 23: Predicciones guardadas | Productos: 21\\n\n",
      "[2025-07-06 22:02:00] ===> Cluster 23 completado en 0.61 min.\\n\n",
      "[2025-07-06 22:02:00] ===> Iniciando procesamiento Cluster 36\\n\n",
      "[2025-07-06 22:02:00] Features OK para Cluster 36\\n\n",
      "[2025-07-06 22:02:00] Train shape: (90, 100) | Test shape: (25, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t('float', [])     : 10 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.1s = Fit runtime\n",
      "\t18 features in original data used to generate 18 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 72, Val Rows: 18\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.89s of the 299.89s of remaining time.\n",
      "\t-5.3083\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.87s of the 299.87s of remaining time.\n",
      "\t-4.7405\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.85s of the 299.85s of remaining time.\n",
      "\t-7.8306\t = Validation score   (-mean_absolute_error)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 299.22s of the 299.22s of remaining time.\n",
      "\t-6.2257\t = Validation score   (-mean_absolute_error)\n",
      "\t1.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 298.02s of the 298.01s of remaining time.\n",
      "\t-4.0907\t = Validation score   (-mean_absolute_error)\n",
      "\t0.44s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 297.53s of the 297.53s of remaining time.\n",
      "\t-3.5425\t = Validation score   (-mean_absolute_error)\n",
      "\t1.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 296.27s of the 296.26s of remaining time.\n",
      "\t-3.3362\t = Validation score   (-mean_absolute_error)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 295.84s of the 295.84s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 295.60s of the 295.60s of remaining time.\n",
      "\t-4.0476\t = Validation score   (-mean_absolute_error)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 295.05s of the 295.05s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 294.80s of the 294.80s of remaining time.\n",
      "\t-5.23\t = Validation score   (-mean_absolute_error)\n",
      "\t0.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.89s of the 293.71s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 0.696, 'ExtraTreesMSE': 0.261, 'XGBoost': 0.043}\n",
      "\t-2.9609\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 6.36s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 495.1 rows/s (18 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010200\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_010207\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.28 GB / 15.31 GB (14.9%)\n",
      "Disk Space Avail:   215.11 GB / 475.95 GB (45.2%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010207\"\n",
      "Train Data Rows:    306\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2335.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.20 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 22:02:07] ✅ Cluster 36: Predicciones guardadas | Productos: 25\\n\n",
      "[2025-07-06 22:02:07] ===> Cluster 36 completado en 0.11 min.\\n\n",
      "[2025-07-06 22:02:07] ===> Iniciando procesamiento Cluster 49\\n\n",
      "[2025-07-06 22:02:07] Features OK para Cluster 49\\n\n",
      "[2025-07-06 22:02:07] Train shape: (306, 100) | Test shape: (9, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 7): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36', 'cat1_factorized']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  6 | ['periodo_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', 'quarter_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat2_factorized']\n",
      "\t0.3s = Fit runtime\n",
      "\t79 features in original data used to generate 79 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.18 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.3s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 244, Val Rows: 62\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.70s of the 299.70s of remaining time.\n",
      "\t-2.6087\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.67s of the 299.67s of remaining time.\n",
      "\t-2.5093\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.65s of the 299.65s of remaining time.\n",
      "\t-1.3813\t = Validation score   (-mean_absolute_error)\n",
      "\t1.6s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.02s of the 298.02s of remaining time.\n",
      "\t-1.6136\t = Validation score   (-mean_absolute_error)\n",
      "\t1.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 296.01s of the 296.01s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 1.68499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.9442\t = Validation score   (-mean_absolute_error)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 295.42s of the 295.42s of remaining time.\n",
      "\t-1.6413\t = Validation score   (-mean_absolute_error)\n",
      "\t5.78s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 289.64s of the 289.63s of remaining time.\n",
      "\t-1.7629\t = Validation score   (-mean_absolute_error)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 289.12s of the 289.11s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 288.82s of the 288.82s of remaining time.\n",
      "\t-1.642\t = Validation score   (-mean_absolute_error)\n",
      "\t1.61s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 287.18s of the 287.18s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 286.93s of the 286.93s of remaining time.\n",
      "\t-1.8631\t = Validation score   (-mean_absolute_error)\n",
      "\t3.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.70s of the 283.74s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.5, 'XGBoost': 0.3, 'CatBoost': 0.2}\n",
      "\t-1.201\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 16.35s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 7687.8 rows/s (62 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010207\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_010223\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.20 GB / 15.31 GB (14.4%)\n",
      "Disk Space Avail:   215.09 GB / 475.95 GB (45.2%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010223\"\n",
      "Train Data Rows:    231\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2252.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.15 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 22:02:23] ✅ Cluster 49: Predicciones guardadas | Productos: 9\\n\n",
      "[2025-07-06 22:02:23] ===> Cluster 49 completado en 0.28 min.\\n\n",
      "[2025-07-06 22:02:23] ===> Iniciando procesamiento Cluster 29\\n\n",
      "[2025-07-06 22:02:23] Features OK para Cluster 29\\n\n",
      "[2025-07-06 22:02:23] Train shape: (231, 100) | Test shape: (24, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 20): ['tn_27', 'tn_28', 'tn_29', 'tn_30', 'tn_31', 'tn_32', 'tn_33', 'tn_34', 'tn_35', 'tn_36', 'diff_tn_27', 'diff_tn_28', 'diff_tn_29', 'diff_tn_30', 'diff_tn_31', 'diff_tn_32', 'diff_tn_33', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 58 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 57 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['tn_26']\n",
      "\t0.1s = Fit runtime\n",
      "\t66 features in original data used to generate 66 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.11 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 184, Val Rows: 47\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.85s of the 299.85s of remaining time.\n",
      "\t-2.1196\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.83s of remaining time.\n",
      "\t-2.0189\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.81s of the 299.81s of remaining time.\n",
      "\t-2.3548\t = Validation score   (-mean_absolute_error)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 299.09s of the 299.08s of remaining time.\n",
      "\t-2.092\t = Validation score   (-mean_absolute_error)\n",
      "\t0.77s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 298.31s of the 298.30s of remaining time.\n",
      "\t-1.9302\t = Validation score   (-mean_absolute_error)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 297.76s of the 297.76s of remaining time.\n",
      "\t-1.526\t = Validation score   (-mean_absolute_error)\n",
      "\t3.17s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 294.58s of the 294.58s of remaining time.\n",
      "\t-1.6685\t = Validation score   (-mean_absolute_error)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 294.08s of the 294.08s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 293.85s of the 293.85s of remaining time.\n",
      "\t-1.8672\t = Validation score   (-mean_absolute_error)\n",
      "\t0.78s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 293.05s of the 293.05s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 292.80s of the 292.80s of remaining time.\n",
      "\t-2.0065\t = Validation score   (-mean_absolute_error)\n",
      "\t1.72s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.85s of the 290.96s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost': 0.417, 'ExtraTreesMSE': 0.333, 'CatBoost': 0.25}\n",
      "\t-1.4534\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 9.12s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1273.7 rows/s (47 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010223\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_010233\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.30 GB / 15.31 GB (15.0%)\n",
      "Disk Space Avail:   215.07 GB / 475.95 GB (45.2%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010233\"\n",
      "Train Data Rows:    306\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2353.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.20 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 22:02:33] ✅ Cluster 29: Predicciones guardadas | Productos: 24\\n\n",
      "[2025-07-06 22:02:33] ===> Cluster 29 completado en 0.16 min.\\n\n",
      "[2025-07-06 22:02:33] ===> Iniciando procesamiento Cluster 33\\n\n",
      "[2025-07-06 22:02:33] Features OK para Cluster 33\\n\n",
      "[2025-07-06 22:02:33] Train shape: (306, 100) | Test shape: (9, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.3s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.18 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.31s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 244, Val Rows: 62\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.69s of the 299.69s of remaining time.\n",
      "\t-2.3502\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.67s of the 299.67s of remaining time.\n",
      "\t-2.3037\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.64s of the 299.64s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 1.30351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.2984\t = Validation score   (-mean_absolute_error)\n",
      "\t2.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.01s of the 297.01s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 1.41383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.4078\t = Validation score   (-mean_absolute_error)\n",
      "\t2.65s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 294.28s of the 294.28s of remaining time.\n",
      "\t-1.6694\t = Validation score   (-mean_absolute_error)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 293.71s of the 293.71s of remaining time.\n",
      "\t-1.5642\t = Validation score   (-mean_absolute_error)\n",
      "\t9.94s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 283.76s of the 283.75s of remaining time.\n",
      "\t-1.687\t = Validation score   (-mean_absolute_error)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 283.26s of the 283.25s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 283.03s of the 283.03s of remaining time.\n",
      "\t-1.4718\t = Validation score   (-mean_absolute_error)\n",
      "\t1.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 281.90s of the 281.90s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 281.65s of the 281.65s of remaining time.\n",
      "\t-1.577\t = Validation score   (-mean_absolute_error)\n",
      "\t2.66s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.69s of the 278.86s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.708, 'XGBoost': 0.292}\n",
      "\t-1.254\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 21.22s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6635.9 rows/s (62 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010233\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_010254\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.41 GB / 15.31 GB (15.7%)\n",
      "Disk Space Avail:   215.05 GB / 475.95 GB (45.2%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010254\"\n",
      "Train Data Rows:    306\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2469.40 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.20 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 22:02:54] ✅ Cluster 33: Predicciones guardadas | Productos: 9\\n\n",
      "[2025-07-06 22:02:54] ===> Cluster 33 completado en 0.36 min.\\n\n",
      "[2025-07-06 22:02:54] ===> Iniciando procesamiento Cluster 28\\n\n",
      "[2025-07-06 22:02:54] Features OK para Cluster 28\\n\n",
      "[2025-07-06 22:02:54] Train shape: (306, 100) | Test shape: (9, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['brand_factorized', 'mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 2 | ['brand_factorized', 'mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  7 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'descripcion_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  7 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'descripcion_factorized', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t79 features in original data used to generate 79 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.18 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.32s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 244, Val Rows: 62\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.68s of the 299.68s of remaining time.\n",
      "\t-1.1778\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.66s of the 299.66s of remaining time.\n",
      "\t-1.1481\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.64s of the 299.64s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.744121\n",
      "[2000]\tvalid_set's l1: 0.711003\n",
      "[3000]\tvalid_set's l1: 0.709399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.708\t = Validation score   (-mean_absolute_error)\n",
      "\t5.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 294.01s of the 294.01s of remaining time.\n",
      "\t-0.8339\t = Validation score   (-mean_absolute_error)\n",
      "\t1.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 292.66s of the 292.66s of remaining time.\n",
      "\t-0.9623\t = Validation score   (-mean_absolute_error)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 292.11s of the 292.11s of remaining time.\n",
      "\t-0.862\t = Validation score   (-mean_absolute_error)\n",
      "\t20.56s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 271.53s of the 271.53s of remaining time.\n",
      "\t-0.9579\t = Validation score   (-mean_absolute_error)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 270.99s of the 270.99s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 270.75s of the 270.75s of remaining time.\n",
      "\t-0.6788\t = Validation score   (-mean_absolute_error)\n",
      "\t1.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 269.51s of the 269.50s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 269.25s of the 269.24s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.979691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9797\t = Validation score   (-mean_absolute_error)\n",
      "\t8.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.68s of the 260.46s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost': 0.714, 'KNeighborsDist': 0.143, 'LightGBMXT': 0.143}\n",
      "\t-0.5779\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 39.61s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6086.0 rows/s (62 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010254\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_010334\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.19 GB / 15.31 GB (14.3%)\n",
      "Disk Space Avail:   215.03 GB / 475.95 GB (45.2%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010334\"\n",
      "Train Data Rows:    654\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2241.16 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.43 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 22:03:34] ✅ Cluster 28: Predicciones guardadas | Productos: 9\\n\n",
      "[2025-07-06 22:03:34] ===> Cluster 28 completado en 0.66 min.\\n\n",
      "[2025-07-06 22:03:34] ===> Iniciando procesamiento Cluster 16\\n\n",
      "[2025-07-06 22:03:34] Features OK para Cluster 16\\n\n",
      "[2025-07-06 22:03:34] Train shape: (654, 100) | Test shape: (24, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  7 | ['periodo_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.1s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.39 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 523, Val Rows: 131\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.85s of the 299.85s of remaining time.\n",
      "\t-1.0042\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.82s of remaining time.\n",
      "\t-0.9311\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.81s of the 299.81s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.550744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5499\t = Validation score   (-mean_absolute_error)\n",
      "\t2.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 296.86s of the 296.85s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.545444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.543\t = Validation score   (-mean_absolute_error)\n",
      "\t3.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 293.36s of the 293.36s of remaining time.\n",
      "\t-0.7812\t = Validation score   (-mean_absolute_error)\n",
      "\t0.72s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 292.55s of the 292.55s of remaining time.\n",
      "\t-0.6258\t = Validation score   (-mean_absolute_error)\n",
      "\t113.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 179.49s of the 179.49s of remaining time.\n",
      "\t-0.7096\t = Validation score   (-mean_absolute_error)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 178.92s of the 178.92s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 178.67s of the 178.67s of remaining time.\n",
      "\t-0.6743\t = Validation score   (-mean_absolute_error)\n",
      "\t1.9s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 176.76s of the 176.75s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 176.49s of the 176.49s of remaining time.\n",
      "\t-0.7995\t = Validation score   (-mean_absolute_error)\n",
      "\t4.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.85s of the 171.89s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.923, 'CatBoost': 0.077}\n",
      "\t-0.5312\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 128.2s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 10836.9 rows/s (131 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010334\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_010542\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.23 GB / 15.31 GB (14.5%)\n",
      "Disk Space Avail:   214.98 GB / 475.95 GB (45.2%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010542\"\n",
      "Train Data Rows:    136\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2280.69 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.09 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 22:05:42] ✅ Cluster 16: Predicciones guardadas | Productos: 24\\n\n",
      "[2025-07-06 22:05:42] ===> Cluster 16 completado en 2.14 min.\\n\n",
      "[2025-07-06 22:05:42] ===> Iniciando procesamiento Cluster 40\\n\n",
      "[2025-07-06 22:05:42] Features OK para Cluster 40\\n\n",
      "[2025-07-06 22:05:42] Train shape: (136, 100) | Test shape: (4, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['cat2_factorized', 'mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 2 | ['cat2_factorized', 'mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  7 | ['periodo_factorized', 'cat1_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])       :  6 | ['periodo_factorized', 'cat3_factorized', 'brand_factorized', 'descripcion_factorized', 'quarter_factorized', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['cat1_factorized']\n",
      "\t0.2s = Fit runtime\n",
      "\t79 features in original data used to generate 79 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.08 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.2s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 108, Val Rows: 28\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.80s of the 299.79s of remaining time.\n",
      "\t-2.3046\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.77s of the 299.77s of remaining time.\n",
      "\t-2.2969\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.75s of the 299.75s of remaining time.\n",
      "\t-1.0306\t = Validation score   (-mean_absolute_error)\n",
      "\t0.84s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.89s of the 298.89s of remaining time.\n",
      "\t-1.1556\t = Validation score   (-mean_absolute_error)\n",
      "\t0.82s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 298.05s of the 298.05s of remaining time.\n",
      "\t-1.5643\t = Validation score   (-mean_absolute_error)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 297.52s of the 297.52s of remaining time.\n",
      "\t-1.567\t = Validation score   (-mean_absolute_error)\n",
      "\t85.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 212.25s of the 212.25s of remaining time.\n",
      "\t-1.105\t = Validation score   (-mean_absolute_error)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 211.78s of the 211.78s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 211.57s of the 211.57s of remaining time.\n",
      "\t-2.331\t = Validation score   (-mean_absolute_error)\n",
      "\t0.72s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 210.83s of the 210.83s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 210.59s of the 210.59s of remaining time.\n",
      "\t-1.687\t = Validation score   (-mean_absolute_error)\n",
      "\t1.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.80s of the 208.65s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.462, 'ExtraTreesMSE': 0.462, 'XGBoost': 0.077}\n",
      "\t-0.8069\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 91.44s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 740.9 rows/s (28 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010542\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_010714\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.16 GB / 15.31 GB (14.1%)\n",
      "Disk Space Avail:   214.96 GB / 475.95 GB (45.2%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010714\"\n",
      "Train Data Rows:    203\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2215.23 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.13 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 7): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36', 'cat1_factorized']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['cat3_factorized', 'brand_factorized', 'mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 3 | ['cat3_factorized', 'brand_factorized', 'mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  5 | ['periodo_factorized', 'cat2_factorized', 'descripcion_factorized', 'quarter_factorized', 'product_id']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 22:07:14] ✅ Cluster 40: Predicciones guardadas | Productos: 4\\n\n",
      "[2025-07-06 22:07:14] ===> Cluster 40 completado en 1.53 min.\\n\n",
      "[2025-07-06 22:07:14] ===> Iniciando procesamiento Cluster 11\\n\n",
      "[2025-07-06 22:07:14] Features OK para Cluster 11\\n\n",
      "[2025-07-06 22:07:14] Train shape: (203, 100) | Test shape: (6, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 72 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  5 | ['periodo_factorized', 'cat2_factorized', 'descripcion_factorized', 'quarter_factorized', 'product_id']\n",
      "\t0.1s = Fit runtime\n",
      "\t77 features in original data used to generate 77 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.12 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 162, Val Rows: 41\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.86s of the 299.86s of remaining time.\n",
      "\t-0.3705\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.84s of the 299.83s of remaining time.\n",
      "\t-0.3956\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.82s of the 299.82s of remaining time.\n",
      "\t-0.2011\t = Validation score   (-mean_absolute_error)\n",
      "\t0.95s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.85s of the 298.85s of remaining time.\n",
      "\t-0.2291\t = Validation score   (-mean_absolute_error)\n",
      "\t0.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 298.05s of the 298.04s of remaining time.\n",
      "\t-0.2459\t = Validation score   (-mean_absolute_error)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 297.48s of the 297.48s of remaining time.\n",
      "\t-0.1732\t = Validation score   (-mean_absolute_error)\n",
      "\t2.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 295.37s of the 295.37s of remaining time.\n",
      "\t-0.2011\t = Validation score   (-mean_absolute_error)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 294.86s of the 294.86s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 294.63s of the 294.63s of remaining time.\n",
      "\t-0.2782\t = Validation score   (-mean_absolute_error)\n",
      "\t0.84s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 293.77s of the 293.77s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 293.52s of the 293.52s of remaining time.\n",
      "\t-0.2718\t = Validation score   (-mean_absolute_error)\n",
      "\t1.68s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.86s of the 291.72s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 0.64, 'KNeighborsUnif': 0.12, 'LightGBMXT': 0.12, 'XGBoost': 0.08, 'ExtraTreesMSE': 0.04}\n",
      "\t-0.146\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 8.38s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 977.4 rows/s (41 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010714\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_010723\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       2.17 GB / 15.31 GB (14.2%)\n",
      "Disk Space Avail:   214.95 GB / 475.95 GB (45.2%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010723\"\n",
      "Train Data Rows:    106\n",
      "Train Data Columns: 87\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2224.55 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.07 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 46): ['tn_14', 'tn_15', 'tn_16', 'tn_17', 'tn_18', 'tn_19', 'tn_20', 'tn_21', 'tn_22', 'tn_23', 'tn_24', 'tn_25', 'tn_26', 'tn_27', 'tn_28', 'tn_29', 'tn_30', 'tn_31', 'tn_32', 'tn_33', 'tn_34', 'tn_35', 'tn_36', 'diff_tn_14', 'diff_tn_15', 'diff_tn_16', 'diff_tn_17', 'diff_tn_18', 'diff_tn_19', 'diff_tn_20', 'diff_tn_21', 'diff_tn_22', 'diff_tn_23', 'diff_tn_24', 'diff_tn_25', 'diff_tn_26', 'diff_tn_27', 'diff_tn_28', 'diff_tn_29', 'diff_tn_30', 'diff_tn_31', 'diff_tn_32', 'diff_tn_33', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['mm-yyyy_factorized']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['mm-yyyy_factorized']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 32 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 32 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  8 | ['periodo_factorized', 'cat1_factorized', 'cat2_factorized', 'cat3_factorized', 'brand_factorized', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t40 features in original data used to generate 40 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 84, Val Rows: 22\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 22:07:23] ✅ Cluster 11: Predicciones guardadas | Productos: 6\\n\n",
      "[2025-07-06 22:07:23] ===> Cluster 11 completado en 0.14 min.\\n\n",
      "[2025-07-06 22:07:23] ===> Iniciando procesamiento Cluster 6\\n\n",
      "[2025-07-06 22:07:23] Features OK para Cluster 6\\n\n",
      "[2025-07-06 22:07:23] Train shape: (106, 100) | Test shape: (9, 100)\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting model: KNeighborsUnif ... Training model for up to 299.90s of the 299.90s of remaining time.\n",
      "\t-0.3686\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.89s of the 299.88s of remaining time.\n",
      "\t-0.3223\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.87s of the 299.87s of remaining time.\n",
      "\t-0.1931\t = Validation score   (-mean_absolute_error)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 299.23s of the 299.22s of remaining time.\n",
      "\t-0.2223\t = Validation score   (-mean_absolute_error)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 298.60s of the 298.60s of remaining time.\n",
      "\t-0.3375\t = Validation score   (-mean_absolute_error)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 298.08s of the 298.08s of remaining time.\n",
      "\t-0.1641\t = Validation score   (-mean_absolute_error)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 297.71s of the 297.70s of remaining time.\n",
      "\t-0.2258\t = Validation score   (-mean_absolute_error)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 297.24s of the 297.23s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ... Training model for up to 296.95s of the 296.94s of remaining time.\n",
      "\t-0.1994\t = Validation score   (-mean_absolute_error)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 296.37s of the 296.36s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\FSONZOGNI\\AppData\\Local\\anaconda3\\envs\\labo_III\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ... Training model for up to 296.08s of the 296.08s of remaining time.\n",
      "\t-0.2258\t = Validation score   (-mean_absolute_error)\n",
      "\t1.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.90s of the 294.90s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 0.929, 'XGBoost': 0.071}\n",
      "\t-0.1637\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 7317.0 rows/s (22 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_010723\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 22:07:28] ✅ Cluster 6: Predicciones guardadas | Productos: 9\\n\n",
      "[2025-07-06 22:07:28] ===> Cluster 6 completado en 0.09 min.\\n\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Iterar cluster por cluster\n",
    "for cluster_id in clusters_unicos:\n",
    "    log(f\"===> Iniciando procesamiento Cluster {cluster_id}\")\n",
    "    cluster_start = datetime.now()\n",
    "    \n",
    "    # 1. Filtrar cluster\n",
    "    df_cluster = df[df['cluster_dtw'] == cluster_id].copy()\n",
    "    if df_cluster.empty:\n",
    "        log(f\"Cluster {cluster_id} está vacío. Saltando...\")\n",
    "        continue\n",
    "    \n",
    "    # 2. Ingeniería de features (misma lógica)\n",
    "    for lag in range(1, 37):\n",
    "        df_cluster[f'tn_{lag}'] = df_cluster.groupby('product_id')['tn_total'].shift(lag)\n",
    "        df_cluster[f'diff_tn_{lag}'] = df_cluster['clase'] - df_cluster[f'tn_{lag}']\n",
    "    \n",
    "    df_cluster['rollmean_3'] = df_cluster.groupby('product_id')['tn_total'].transform(lambda x: x.shift(1).rolling(3).mean())\n",
    "    df_cluster['rollmean_6'] = df_cluster.groupby('product_id')['tn_total'].transform(lambda x: x.shift(1).rolling(6).mean())\n",
    "    df_cluster['rollmean_9'] = df_cluster.groupby('product_id')['tn_total'].transform(lambda x: x.shift(1).rolling(9).mean())\n",
    "    df_cluster['rollmean_12'] = df_cluster.groupby('product_id')['tn_total'].transform(lambda x: x.shift(1).rolling(12).mean())\n",
    "    df_cluster['diff_rollmean_12'] = df_cluster['clase'] - df_cluster['rollmean_12']\n",
    "\n",
    "    # Factorizar no numéricas\n",
    "    for col in df_cluster.select_dtypes(include='object').columns:\n",
    "        if col != 'cluster_dtw':\n",
    "            df_cluster[col + '_factorized'], _ = pd.factorize(df_cluster[col])\n",
    "\n",
    "    log(f\"Features OK para Cluster {cluster_id}\")\n",
    "    \n",
    "    # 3. Train/Test split (sin validación ni escalado)\n",
    "    df_cluster['fecha'] = pd.to_datetime(df_cluster['fecha']).dt.normalize()\n",
    "    \n",
    "    train_set = df_cluster[(df_cluster['fecha'] <= '2019-10-01') & df_cluster['clase'].notnull()].copy()\n",
    "    test_set  = df_cluster[df_cluster['fecha'] == '2019-12-01'].copy()\n",
    "    \n",
    "    log(f\"Train shape: {train_set.shape} | Test shape: {test_set.shape}\")\n",
    "    \n",
    "    if train_set.empty or test_set.empty:\n",
    "        log(f\"Cluster {cluster_id} sin datos suficientes. Saltando...\")\n",
    "        continue\n",
    "    \n",
    "    # 4. Train modelo tabular (sin escalado)\n",
    "    features = ['tn_total'] + [f'tn_{i}' for i in range(1, 37)] + \\\n",
    "               [f'diff_tn_{i}' for i in range(1, 37)] + \\\n",
    "               ['rollmean_3', 'rollmean_6', 'rollmean_9', 'rollmean_12', 'diff_rollmean_12'] + \\\n",
    "               [c for c in df_cluster.columns if c.endswith('_factorized')] + ['product_id']\n",
    "\n",
    "    predictor = TabularPredictor(label='clase', problem_type='regression', eval_metric='mae')\n",
    "    predictor.fit(train_data=train_set[features + ['clase']], time_limit=1200, presets='high_quality')\n",
    "\n",
    "    # 5. Predicción final sin inversa\n",
    "    test_set['tn'] = predictor.predict(test_set[features])\n",
    "    test_set['tn'] = test_set['tn'].clip(lower=0)\n",
    "\n",
    "    # 6. Guardar CSV por cluster\n",
    "    test_set[['product_id', 'tn']].to_csv(f\"{output_dir}/forecast_cluster_{cluster_id}.csv\", index=False)\n",
    "    log(f\"✅ Cluster {cluster_id}: Predicciones guardadas | Productos: {test_set['product_id'].nunique()}\")\n",
    "    \n",
    "    cluster_end = datetime.now()\n",
    "    duration = (cluster_end - cluster_start).total_seconds() / 60\n",
    "    log(f\"===> Cluster {cluster_id} completado en {duration:.2f} min.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d10d870",
   "metadata": {},
   "source": [
    "# 🗃️ 4. Combinar todos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9530185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 22:10:00] Archivo final forecast_tabular_clusters_202002.csv generado.\\n\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Combinar todos los CSV\n",
    "csv_files = glob.glob(f\"{output_dir}/forecast_cluster_*.csv\")\n",
    "dfs = [pd.read_csv(f) for f in csv_files]\n",
    "df_final = pd.concat(dfs, axis=0)\n",
    "\n",
    "df_final.to_csv(\"forecast_tabular_clusters_202002.csv\", index=False)\n",
    "log(f\"Archivo final forecast_tabular_clusters_202002.csv generado.\")\n",
    "\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469014ad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labo_III",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
