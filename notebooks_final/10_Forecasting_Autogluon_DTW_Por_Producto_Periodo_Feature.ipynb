{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e28ee4e",
   "metadata": {},
   "source": [
    "# 🧩 1. Setup inicial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69c28b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Cargar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# 1.2 Paths de entrada y salida\n",
    "ruta_parquet = \"C:/Developer/Laboratorio_III/data/dataset_product_periodo_con_clusters.parquet\"\n",
    "output_dir = \"output_forecasts_by_cluster_tabular_full\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1.3 Archivo de log\n",
    "log_file = open(\"log_forecast_clusters_3.txt\", \"w\")\n",
    "def log(msg):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    line = f\"[{timestamp}] {msg}\\\\n\"\n",
    "    log_file.write(line)\n",
    "    log_file.flush()\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a1239",
   "metadata": {},
   "source": [
    "# 📥 2. Cargar dataset y clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6f42358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 11:38:08] Clusters únicos: [ 1  0 31  4 39  7 43 48  9 41 24 45 35 10 20 18 32 12 15 21 44 19 30 47\n",
      " 46 37 17 25  3 26 38 22  5  2 42 14 34  8 27 13 23 36 49 29 33 28 16 40\n",
      " 11  6]\\n\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Cargar dataset con clusters\n",
    "df = pd.read_parquet(ruta_parquet)\n",
    "# Normalizar fecha\n",
    "df['fecha'] = pd.to_datetime(df['fecha']).dt.normalize()\n",
    "\n",
    "# 2.2 Crear campo clase (tn_total desplazado +2)\n",
    "df['clase'] = df.groupby('product_id')['tn_total'].shift(-2)\n",
    "\n",
    "# 2.3 Revisar clusters únicos\n",
    "clusters_unicos = df['cluster_dtw'].dropna().unique()\n",
    "log(f\"Clusters únicos: {clusters_unicos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58f9a9b",
   "metadata": {},
   "source": [
    "# ✅ 2.1 Bloque hyperparameters sugerido (árboles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ff53475",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'GBM': {\n",
    "        'extra_trees': True,     # Usar LightGBM con Extra Trees (más robustez)\n",
    "        'num_boost_round': 300,  # N° de iteraciones boosting\n",
    "        'early_stopping_rounds': 20,\n",
    "    },\n",
    "    'CAT': {\n",
    "        'iterations': 300,\n",
    "        'learning_rate': 0.05,\n",
    "        'depth': 16,\n",
    "    },\n",
    "    'XGB': {\n",
    "        'num_boost_round': 600,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 16,\n",
    "    },\n",
    "    'RF': {\n",
    "        'n_estimators': 300,\n",
    "    },\n",
    "    'XT': {\n",
    "        'n_estimators': 300,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d65e04",
   "metadata": {},
   "source": [
    "# 🔄 3. Loop por cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b63b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250707_143810\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          14\n",
      "Memory Avail:       1.69 GB / 15.31 GB (11.0%)\n",
      "Disk Space Avail:   199.09 GB / 475.95 GB (41.8%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=5, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1800s of the 7200s of remaining time (25%).\n",
      "\t\tContext path: \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_143810\\ds_sub_fit\\sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 11:38:10] ✅ Ingeniería de features completada para todo el dataset\\n\n",
      "[2025-07-07 11:38:10] ✅ Shapes => Train: (20815, 105) | Test: (780, 104)\\n\n",
      "[2025-07-07 11:38:10] ✅ Peso promedio: 7.0110\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val          eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3      -2.086603  -2.556011  mean_absolute_error        9.755493       9.303828  1545.832778                 0.026768                0.000997           0.136171            3       True         10\n",
      "1    ExtraTrees_BAG_L2      -2.093862  -2.615540  mean_absolute_error        7.560785       6.789911  1222.671886                 0.817483                2.116324          22.205643            2       True          8\n",
      "2  RandomForest_BAG_L2      -2.172816  -2.710361  mean_absolute_error        7.586477       6.141037  1284.137637                 0.843175                1.467450          83.671394            2       True          7\n",
      "3       XGBoost_BAG_L2      -2.245620  -3.041034  mean_absolute_error        7.733441       5.294732  1432.943295                 0.990139                0.621145         232.477053            2       True          9\n",
      "4  WeightedEnsemble_L2      -2.810576  -3.123999  mean_absolute_error        6.773597       4.675485  1200.570259                 0.030295                0.001898           0.104016            2       True          5\n",
      "5       XGBoost_BAG_L1      -3.026129  -3.948698  mean_absolute_error        2.325953       0.591126  1000.648099                 2.325953                0.591126        1000.648099            1       True          4\n",
      "6      LightGBM_BAG_L2      -3.084910  -4.068277  mean_absolute_error        7.077927       5.097911  1207.342516                 0.334625                0.424325           6.876273            2       True          6\n",
      "7    ExtraTrees_BAG_L1      -3.123548  -3.446170  mean_absolute_error        0.730641       1.980521    15.949030                 0.730641                1.980521          15.949030            1       True          3\n",
      "8  RandomForest_BAG_L1      -3.669191  -3.785281  mean_absolute_error        0.735914       1.544229    85.484064                 0.735914                1.544229          85.484064            1       True          2\n",
      "9      LightGBM_BAG_L1      -3.707548  -4.759673  mean_absolute_error        2.950794       0.557711    98.385049                 2.950794                0.557711          98.385049            1       True          1\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1758s\t = DyStack   runtime |\t5442s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 5442s\n",
      "AutoGluon will save models to \"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_143810\"\n",
      "Train Data Rows:    20815\n",
      "Train Data Columns: 82\n",
      "Label Column:       clase\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2983.75 MB\n",
      "\tTrain Data (Original)  Memory Usage: 13.02 MB (0.4% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 6): ['tn_34', 'tn_35', 'tn_36', 'diff_tn_34', 'diff_tn_35', 'diff_tn_36']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 75 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  1 | ['product_id']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 75 | ['tn_total', 'tn_1', 'tn_2', 'tn_3', 'tn_4', ...]\n",
      "\t\t('int', [])   :  1 | ['product_id']\n",
      "\t0.5s = Fit runtime\n",
      "\t76 features in original data used to generate 76 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 12.07 MB (0.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.61s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{'extra_trees': True, 'num_boost_round': 300, 'early_stopping_rounds': 20}],\n",
      "\t'CAT': [{'iterations': 300, 'learning_rate': 0.05, 'depth': 16}],\n",
      "\t'XGB': [{'num_boost_round': 600, 'learning_rate': 0.05, 'max_depth': 16}],\n",
      "\t'RF': [{'n_estimators': 300}],\n",
      "\t'XT': [{'n_estimators': 300}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 5 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 3626.80s of the 5441.54s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=3.10%)\n",
      "\t-4.6302\t = Validation score   (-mean_absolute_error)\n",
      "\t9.7s\t = Training   runtime\n",
      "\t0.65s\t = Validation runtime\n",
      "Fitting model: RandomForest_BAG_L1 ... Training model for up to 3610.23s of the 5424.98s of remaining time.\n",
      "\t-3.6034\t = Validation score   (-mean_absolute_error)\n",
      "\t68.13s\t = Training   runtime\n",
      "\t2.26s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 3539.01s of the 5353.75s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 36.916 GB out of 3.583 GB available memory (1030.215%)... (100.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=10.35 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train CatBoost_BAG_L1... Skipping this model.\n",
      "Fitting model: ExtraTrees_BAG_L1 ... Training model for up to 3537.62s of the 5352.36s of remaining time.\n",
      "\t-3.3172\t = Validation score   (-mean_absolute_error)\n",
      "\t16.91s\t = Training   runtime\n",
      "\t2.08s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 3517.81s of the 5332.55s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 28.61% memory usage per fold, 57.22%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=7, gpus=0, memory=28.61%)\n",
      "\t-3.9923\t = Validation score   (-mean_absolute_error)\n",
      "\t273.02s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 362.68s of the 5056.11s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTrees_BAG_L1': 0.56, 'LightGBM_BAG_L1': 0.2, 'RandomForest_BAG_L1': 0.12, 'XGBoost_BAG_L1': 0.12}\n",
      "\t-3.0512\t = Validation score   (-mean_absolute_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 5 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 5055.98s of the 5055.97s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=2.34%)\n",
      "\t-3.8109\t = Validation score   (-mean_absolute_error)\n",
      "\t7.41s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: RandomForest_BAG_L2 ... Training model for up to 5043.35s of the 5043.33s of remaining time.\n",
      "\t-2.5826\t = Validation score   (-mean_absolute_error)\n",
      "\t87.29s\t = Training   runtime\n",
      "\t2.2s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 4953.10s of the 4953.09s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 38.835 GB out of 4.042 GB available memory (960.749%)... (100.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=9.66 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train CatBoost_BAG_L2... Skipping this model.\n",
      "Fitting model: ExtraTrees_BAG_L2 ... Training model for up to 4952.62s of the 4952.61s of remaining time.\n",
      "\t-2.4814\t = Validation score   (-mean_absolute_error)\n",
      "\t20.38s\t = Training   runtime\n",
      "\t2.27s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 4929.27s of the 4929.25s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 26.23% memory usage per fold, 52.46%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=7, gpus=0, memory=26.23%)\n",
      "\t-2.9104\t = Validation score   (-mean_absolute_error)\n",
      "\t456.92s\t = Training   runtime\n",
      "\t1.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 505.60s of the 4468.74s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTrees_BAG_L2': 0.56, 'RandomForest_BAG_L2': 0.16, 'XGBoost_BAG_L2': 0.16, 'LightGBM_BAG_L2': 0.08, 'ExtraTrees_BAG_L1': 0.04}\n",
      "\t-2.4183\t = Validation score   (-mean_absolute_error)\n",
      "\t0.15s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 973.65s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 917.4 rows/s (4163 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Developer\\Laboratorio_III\\notebooks\\AutogluonModels\\ag-20250707_143810\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 12:23:47] ✅ Predicciones guardadas: output_forecasts_by_cluster_tabular_full/forecast_modelo_global_v4.csv | Productos: 780\\n\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "#  Normalizar fecha\n",
    "# ====================================\n",
    "df['fecha'] = pd.to_datetime(df['fecha']).dt.normalize()\n",
    "\n",
    "# ====================================\n",
    "#  Crear clase = tn_total + 2 meses\n",
    "# ====================================\n",
    "df = df.sort_values(['product_id', 'fecha'])\n",
    "df['clase'] = df.groupby('product_id')['tn_total'].shift(-2)\n",
    "\n",
    "# ====================================\n",
    "#  Ingeniería de features\n",
    "# ====================================\n",
    "for lag in range(1, 37):\n",
    "    df[f'tn_{lag}'] = df.groupby('product_id')['tn_total'].shift(lag)\n",
    "    df[f'diff_tn_{lag}'] = df['clase'] - df[f'tn_{lag}']\n",
    "\n",
    "df['rollmean_3'] = df.groupby('product_id')['tn_total'].transform(lambda x: x.shift(1).rolling(3).mean())\n",
    "df['rollmean_6'] = df.groupby('product_id')['tn_total'].transform(lambda x: x.shift(1).rolling(6).mean())\n",
    "df['rollmean_9'] = df.groupby('product_id')['tn_total'].transform(lambda x: x.shift(1).rolling(9).mean())\n",
    "df['rollmean_12'] = df.groupby('product_id')['tn_total'].transform(lambda x: x.shift(1).rolling(12).mean())\n",
    "df['diff_rollmean_12'] = df['tn_total'] - df['rollmean_12']\n",
    "df['diff_rollmean_9'] = df['tn_total'] - df['rollmean_9']\n",
    "df['diff_rollmean_6'] = df['tn_total'] - df['rollmean_6']\n",
    "df['diff_rollmean_3'] = df['tn_total'] - df['rollmean_3']\n",
    "\n",
    "# ====================================\n",
    "#  Factorizar cluster_dtw y otras categóricas\n",
    "# ====================================\n",
    "df['cluster_dtw_factorized'], _ = pd.factorize(df['cluster_dtw'].fillna(-1))\n",
    "\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    if col not in ['cluster_dtw']:\n",
    "        df[col + '_factorized'], _ = pd.factorize(df[col])\n",
    "\n",
    "log(\"✅ Ingeniería de features completada para todo el dataset\")\n",
    "\n",
    "# ====================================\n",
    "#  Definir lista de features\n",
    "# ====================================\n",
    "numeric_cols = ['tn_total'] + [f'tn_{i}' for i in range(1, 37)] + \\\n",
    "               [f'diff_tn_{i}' for i in range(1, 37)] + \\\n",
    "               ['rollmean_3', 'rollmean_6', 'rollmean_9', 'rollmean_12', 'diff_rollmean_3', 'diff_rollmean_6', 'diff_rollmean_9', 'diff_rollmean_12']\n",
    "\n",
    "#factor_cols = ['cluster_dtw_factorized'] + [c for c in df.columns if c.endswith('_factorized') and c != 'cluster_dtw_factorized']\n",
    "\n",
    "features = numeric_cols + ['product_id']\n",
    "\n",
    "# ====================================\n",
    "#  Split train / test\n",
    "# ====================================\n",
    "train_set = df[(df['fecha'] <= '2019-10-01') & df['clase'].notnull()].copy()\n",
    "test_set = df[df['fecha'] == '2019-12-01'].copy()\n",
    "\n",
    "# ====================================\n",
    "#  Validación de shape y pesos\n",
    "# ====================================\n",
    "if train_set.empty or test_set.empty:\n",
    "    log(f\"Train o Test vacío. Verifica tus datos.\")\n",
    "else:\n",
    "    train_mean_tn = train_set['tn_total'].mean()\n",
    "    train_set['weight'] = train_set['tn_total']\n",
    "    train_set['weight'] = train_set['weight'].fillna(1).clip(lower=0.1, upper=10)\n",
    "\n",
    "    log(f\"✅ Shapes => Train: {train_set.shape} | Test: {test_set.shape}\")\n",
    "    log(f\"✅ Peso promedio: {train_set['weight'].mean():.4f}\")\n",
    "\n",
    "    # ====================================\n",
    "    #  Entrenar modelo global\n",
    "    # ====================================\n",
    "    predictor = TabularPredictor(label='clase', problem_type='regression', eval_metric='mae')\n",
    "    predictor.fit(\n",
    "        train_data=train_set[features + ['clase']],\n",
    "        ag_args_fit={'sample_weight': 'weight'},\n",
    "        hyperparameters=hyperparameters,\n",
    "        presets='best_quality',\n",
    "        time_limit=7200,\n",
    "        num_bag_folds=5,\n",
    "        num_stack_levels=1\n",
    "    )\n",
    "\n",
    "    # ====================================\n",
    "    #  Predicción final\n",
    "    # ====================================\n",
    "    test_set['tn'] = predictor.predict(test_set[features])\n",
    "    test_set['tn'] = test_set['tn'].clip(lower=0)\n",
    "\n",
    "    # ====================================\n",
    "    #  Guardar salida final\n",
    "    # ====================================\n",
    "    output_file = f\"{output_dir}/forecast_modelo_global_v4.csv\"\n",
    "    test_set[['product_id', 'tn']].to_csv(output_file, index=False)\n",
    "    log(f\"✅ Predicciones guardadas: {output_file} | Productos: {test_set['product_id'].nunique()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d10d870",
   "metadata": {},
   "source": [
    "# 🗃️ 4. Combinar todos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9530185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 12:23:47] Archivo final forecast_tabular_clusters_202002.csv generado.\\n\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Combinar todos los CSV\n",
    "csv_files = glob.glob(f\"{output_dir}/forecast_modelo_global_v4.csv\")\n",
    "dfs = [pd.read_csv(f) for f in csv_files]\n",
    "df_final = pd.concat(dfs, axis=0)\n",
    "\n",
    "df_final.to_csv(\"forecast_tabular_clusters_202002_full.csv\", index=False)\n",
    "log(f\"Archivo final forecast_tabular_clusters_202002.csv generado.\")\n",
    "\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469014ad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labo_III",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
